{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Bridging the Human-AI Knowledge Gap - Exercises\n",
    "\n",
    "This notebook provides hands-on exercises for extracting, filtering, and validating concepts that bridge the human-AI knowledge gap.\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Implement concept discovery using Concept Activation Vectors (CAVs)\n",
    "2. Test teachability of concepts with student models\n",
    "3. Assess novelty by comparing with training data\n",
    "4. Design and run mini human studies\n",
    "5. Build a concept atlas for your project\n",
    "6. Validate interpretability claims rigorously\n",
    "\n",
    "**Structure:**\n",
    "- Parts 1-3: Concept discovery and extraction (Schut method)\n",
    "- Parts 4-5: Teachability and novelty testing\n",
    "- Parts 6-7: Human study design and analysis\n",
    "- Parts 8-9: Building concept atlases\n",
    "- Part 10: Final project validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Concept Activation Vectors (CAVs)\n",
    "\n",
    "Following Schut et al.'s approach, we'll extract concept vectors from a language model's latent space.\n",
    "\n",
    "**Exercise 1.1:** Implement CAV extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def extract_activations(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    texts: List[str],\n",
    "    layer: int = 6,\n",
    "    position: int = -1  # Last token position\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract activations from a specific layer for given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input strings\n",
    "        layer: Which transformer layer to extract from\n",
    "        position: Token position (-1 for last token)\n",
    "    \n",
    "    Returns:\n",
    "        Activations tensor of shape (n_texts, hidden_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Implement activation extraction\n",
    "    # 1. Tokenize all texts\n",
    "    # 2. Forward pass with output_hidden_states=True\n",
    "    # 3. Extract hidden states from specified layer and position\n",
    "    # 4. Return stacked activations\n",
    "    \n",
    "    raise NotImplementedError(\"Implement activation extraction\")\n",
    "\n",
    "def train_cav(\n",
    "    concept_examples: List[str],\n",
    "    random_examples: List[str],\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    layer: int = 6\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Train a Concept Activation Vector (CAV) to distinguish concept examples from random examples.\n",
    "    \n",
    "    Args:\n",
    "        concept_examples: Texts where concept is present\n",
    "        random_examples: Random contrast texts\n",
    "        layer: Which layer to extract CAV from\n",
    "    \n",
    "    Returns:\n",
    "        cav: Concept direction vector (normalized)\n",
    "        accuracy: Classification accuracy on training data\n",
    "    \"\"\"\n",
    "    # TODO: Implement CAV training\n",
    "    # 1. Extract activations for both concept and random examples\n",
    "    # 2. Create labels (1 for concept, 0 for random)\n",
    "    # 3. Train linear classifier (LogisticRegression)\n",
    "    # 4. Extract weights as CAV (normalize to unit vector)\n",
    "    # 5. Return CAV and classification accuracy\n",
    "    \n",
    "    raise NotImplementedError(\"Implement CAV training\")\n",
    "\n",
    "# Test CAV extraction\n",
    "# Example: Extract a \"politeness\" concept\n",
    "polite_examples = [\n",
    "    \"Could you please help me with this?\",\n",
    "    \"I would be grateful if you could assist.\",\n",
    "    \"Would you mind explaining this to me?\",\n",
    "    \"Thank you so much for your help.\",\n",
    "    \"I appreciate your time and effort.\"\n",
    "]\n",
    "\n",
    "neutral_examples = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"It is raining today.\",\n",
    "    \"The book is on the table.\",\n",
    "    \"They walked to the store.\",\n",
    "    \"The sun rises in the east.\"\n",
    "]\n",
    "\n",
    "# cav, accuracy = train_cav(polite_examples, neutral_examples, model, tokenizer, layer=6)\n",
    "# print(f\"CAV training accuracy: {accuracy:.2%}\")\n",
    "# print(f\"CAV shape: {cav.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** Test CAV sensitivity - how much does the concept influence predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cav_sensitivity(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    cav: np.ndarray,\n",
    "    test_texts: List[str],\n",
    "    layer: int = 6\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute how much the CAV direction influences model predictions.\n",
    "    \n",
    "    Uses directional derivatives: how much does moving in the CAV direction\n",
    "    change the output?\n",
    "    \n",
    "    Returns:\n",
    "        Sensitivity scores for each test text\n",
    "    \"\"\"\n",
    "    # TODO: Implement CAV sensitivity (similar to TCAV)\n",
    "    # 1. Extract activations for test texts\n",
    "    # 2. Project activations onto CAV direction\n",
    "    # 3. Compute gradient of output w.r.t. CAV direction\n",
    "    # 4. Return sensitivity scores\n",
    "    \n",
    "    raise NotImplementedError(\"Implement CAV sensitivity\")\n",
    "\n",
    "# Test on new examples\n",
    "# test_polite = [\"Please could you assist me?\", \"Would you kindly help?\"]\n",
    "# test_neutral = [\"The sky is blue.\", \"Water is wet.\"]\n",
    "# \n",
    "# polite_sensitivity = compute_cav_sensitivity(model, tokenizer, cav, test_polite, layer=6)\n",
    "# neutral_sensitivity = compute_cav_sensitivity(model, tokenizer, cav, test_neutral, layer=6)\n",
    "# \n",
    "# print(f\"Polite examples sensitivity: {polite_sensitivity.mean():.3f}\")\n",
    "# print(f\"Neutral examples sensitivity: {neutral_sensitivity.mean():.3f}\")\n",
    "# print(\"Expected: Polite examples should have higher sensitivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generating Concept Prototypes\n",
    "\n",
    "Following Schut's method, generate positions that strongly activate the concept.\n",
    "\n",
    "**Exercise 2.1:** Find concept prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concept_prototypes(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    cav: np.ndarray,\n",
    "    candidate_texts: List[str],\n",
    "    layer: int = 6,\n",
    "    top_k: int = 10\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Find texts that most strongly activate the concept vector.\n",
    "    \n",
    "    Args:\n",
    "        candidate_texts: Large pool of potential examples\n",
    "        top_k: Return top K prototypes\n",
    "    \n",
    "    Returns:\n",
    "        List of (text, activation_score) tuples, sorted by score\n",
    "    \"\"\"\n",
    "    # TODO: Implement prototype finding\n",
    "    # 1. Extract activations for all candidate texts\n",
    "    # 2. Project onto CAV direction: score = activation · cav\n",
    "    # 3. Sort by score, return top K\n",
    "    \n",
    "    raise NotImplementedError(\"Implement prototype finding\")\n",
    "\n",
    "# Generate large candidate pool\n",
    "# candidate_pool = [\n",
    "#     # Mix of polite, neutral, rude, etc.\n",
    "#     \"Could you possibly help me?\",\n",
    "#     \"Help me now.\",\n",
    "#     \"The weather is nice today.\",\n",
    "#     # ... (add 50+ examples)\n",
    "# ]\n",
    "# \n",
    "# prototypes = find_concept_prototypes(model, tokenizer, cav, candidate_pool, top_k=10)\n",
    "# print(\"Top 10 concept prototypes:\")\n",
    "# for text, score in prototypes:\n",
    "#     print(f\"  {score:.3f}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Multiple Layers\n",
    "\n",
    "Concepts may be represented at different abstraction levels in different layers.\n",
    "\n",
    "**Exercise 3.1:** Extract CAVs across all layers and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cavs_all_layers(\n",
    "    concept_examples: List[str],\n",
    "    random_examples: List[str],\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer\n",
    ") -> Dict[int, Tuple[np.ndarray, float]]:\n",
    "    \"\"\"\n",
    "    Extract CAVs from all transformer layers.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping layer_idx -> (cav, accuracy)\n",
    "    \"\"\"\n",
    "    # TODO: Extract CAVs from all 12 layers\n",
    "    # TODO: Return dictionary with results\n",
    "    \n",
    "    raise NotImplementedError(\"Implement multi-layer CAV extraction\")\n",
    "\n",
    "def visualize_layer_representations(\n",
    "    layer_cavs: Dict[int, Tuple[np.ndarray, float]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize how concept representation changes across layers.\n",
    "    \"\"\"\n",
    "    layers = sorted(layer_cavs.keys())\n",
    "    accuracies = [layer_cavs[l][1] for l in layers]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(layers, accuracies, marker='o')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('CAV Classification Accuracy')\n",
    "    plt.title('Concept Representation Across Layers')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Extract and visualize\n",
    "# layer_cavs = extract_cavs_all_layers(polite_examples, neutral_examples, model, tokenizer)\n",
    "# visualize_layer_representations(layer_cavs)\n",
    "# \n",
    "# # Find best layer\n",
    "# best_layer = max(layer_cavs.keys(), key=lambda l: layer_cavs[l][1])\n",
    "# print(f\"Best layer for this concept: {best_layer}\")\n",
    "# print(f\"Accuracy: {layer_cavs[best_layer][1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Teachability Testing\n",
    "\n",
    "Following Schut's teachability criterion: Can a student model learn the concept?\n",
    "\n",
    "**Exercise 4.1:** Train a student model on concept prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleStudentModel(nn.Module):\n",
    "    \"\"\"Simple student model that learns to recognize concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # Binary: concept present/absent\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def test_teachability(\n",
    "    concept_prototypes: List[str],\n",
    "    negative_examples: List[str],\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    layer: int = 6,\n",
    "    n_epochs: int = 50\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test if a concept is teachable by training a student model.\n",
    "    \n",
    "    Returns:\n",
    "        - 'train_accuracy': Accuracy on training prototypes\n",
    "        - 'test_accuracy': Accuracy on held-out test set\n",
    "        - 'improvement': Test accuracy - random baseline (50%)\n",
    "    \"\"\"\n",
    "    # TODO: Implement teachability test\n",
    "    # 1. Split prototypes into train/test (80/20)\n",
    "    # 2. Extract activations for all examples\n",
    "    # 3. Train student model to classify concept vs non-concept\n",
    "    # 4. Evaluate on test set\n",
    "    # 5. Return metrics\n",
    "    \n",
    "    raise NotImplementedError(\"Implement teachability test\")\n",
    "\n",
    "# Test teachability\n",
    "# teachability_results = test_teachability(\n",
    "#     polite_examples + [p[0] for p in prototypes],  # Prototypes + original examples\n",
    "#     neutral_examples,\n",
    "#     model, tokenizer, layer=6\n",
    "# )\n",
    "# \n",
    "# print(f\"Teachability Results:\")\n",
    "# print(f\"  Train accuracy: {teachability_results['train_accuracy']:.2%}\")\n",
    "# print(f\"  Test accuracy: {teachability_results['test_accuracy']:.2%}\")\n",
    "# print(f\"  Improvement over random: {teachability_results['improvement']:.2%}\")\n",
    "# \n",
    "# # Decision threshold (from Schut et al.)\n",
    "# if teachability_results['test_accuracy'] > 0.70:\n",
    "#     print(\"✓ Concept is TEACHABLE\")\n",
    "# else:\n",
    "#     print(\"✗ Concept may not be teachable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Novelty Assessment\n",
    "\n",
    "Following Schut's novelty criterion: Is the concept present in human data?\n",
    "\n",
    "**Exercise 5.1:** Compare concept activation on model-generated vs human data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_novelty(\n",
    "    cav: np.ndarray,\n",
    "    human_texts: List[str],\n",
    "    model_texts: List[str],\n",
    "    teacher_model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    layer: int = 6\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Assess if concept is novel by comparing activation on human vs model data.\n",
    "    \n",
    "    Args:\n",
    "        human_texts: Examples from human-generated text (e.g., training data)\n",
    "        model_texts: Examples from AI-generated text\n",
    "    \n",
    "    Returns:\n",
    "        - 'human_activation_mean': Average CAV activation on human texts\n",
    "        - 'model_activation_mean': Average CAV activation on model texts\n",
    "        - 'novelty_score': Difference (model - human)\n",
    "        - 'p_value': Statistical significance of difference\n",
    "    \"\"\"\n",
    "    # TODO: Implement novelty assessment\n",
    "    # 1. Extract activations for human and model texts\n",
    "    # 2. Project onto CAV direction\n",
    "    # 3. Compute mean activation for each group\n",
    "    # 4. Run t-test to check if significantly different\n",
    "    # 5. Return novelty score and statistics\n",
    "    \n",
    "    raise NotImplementedError(\"Implement novelty assessment\")\n",
    "\n",
    "# Collect human examples (from training data or public datasets)\n",
    "# human_examples = [\n",
    "#     # Examples from books, Wikipedia, etc.\n",
    "# ]\n",
    "# \n",
    "# # Collect model-generated examples\n",
    "# model_examples = [\n",
    "#     # Generate with GPT-2\n",
    "# ]\n",
    "# \n",
    "# novelty_results = assess_novelty(cav, human_examples, model_examples, model, tokenizer, layer=6)\n",
    "# \n",
    "# print(f\"Novelty Assessment:\")\n",
    "# print(f\"  Human activation: {novelty_results['human_activation_mean']:.3f}\")\n",
    "# print(f\"  Model activation: {novelty_results['model_activation_mean']:.3f}\")\n",
    "# print(f\"  Novelty score: {novelty_results['novelty_score']:.3f}\")\n",
    "# print(f\"  p-value: {novelty_results['p_value']:.4f}\")\n",
    "# \n",
    "# # Interpretation\n",
    "# if novelty_results['novelty_score'] > 0.1 and novelty_results['p_value'] < 0.05:\n",
    "#     print(\"✓ Concept is NOVEL (more present in model than human data)\")\n",
    "# else:\n",
    "#     print(\"✗ Concept is NOT novel (already present in human data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Designing a Mini Human Study\n",
    "\n",
    "Design a small-scale human study to validate your concept.\n",
    "\n",
    "**Exercise 6.1:** Create study materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study_materials(\n",
    "    concept_name: str,\n",
    "    prototypes: List[Tuple[str, float]],\n",
    "    n_pretest: int = 10,\n",
    "    n_posttest: int = 10\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Generate materials for a pre-test / learning / post-test study design.\n",
    "    \n",
    "    Returns:\n",
    "        - 'pretest': List of test items (before learning)\n",
    "        - 'learning_materials': Concept prototypes with explanations\n",
    "        - 'posttest': List of test items (after learning)\n",
    "    \"\"\"\n",
    "    # TODO: Generate study materials\n",
    "    # 1. Select diverse pre-test items (some with concept, some without)\n",
    "    # 2. Create learning materials from prototypes\n",
    "    # 3. Generate post-test items (different from pre-test but same difficulty)\n",
    "    \n",
    "    raise NotImplementedError(\"Implement study material generation\")\n",
    "\n",
    "def create_questionnaire(\n",
    "    test_items: List[str],\n",
    "    concept_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a questionnaire for human participants.\n",
    "    \n",
    "    Returns:\n",
    "        Formatted questionnaire text\n",
    "    \"\"\"\n",
    "    questionnaire = f\"\"\"
=== Human Study: {concept_name} Concept ===\n\nFor each example below, rate how strongly it exhibits the '{concept_name}' concept.\n\nScale: 1 (not at all) - 5 (very strongly)\n\n\"\"\"\n",
    "    \n",
    "    for i, item in enumerate(test_items, 1):\n",
    "        questionnaire += f\"\\n{i}. \\\"{item}\\\"\\n   Rating: [ ]\"\n",
    "    \n",
    "    return questionnaire\n",
    "\n",
    "# Generate materials\n",
    "# study_materials = generate_study_materials(\n",
    "#     \"Politeness\",\n",
    "#     prototypes,\n",
    "#     n_pretest=10,\n",
    "#     n_posttest=10\n",
    "# )\n",
    "# \n",
    "# # Create pre-test questionnaire\n",
    "# pretest_questionnaire = create_questionnaire(\n",
    "#     study_materials['pretest'],\n",
    "#     \"Politeness\"\n",
    "# )\n",
    "# \n",
    "# print(pretest_questionnaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.2:** Analyze human study results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_human_study(\n",
    "    pretest_scores: Dict[int, List[int]],  # participant_id -> list of ratings\n",
    "    posttest_scores: Dict[int, List[int]],\n",
    "    ground_truth: List[int]  # True concept strength for each item\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze results from pre-test / post-test human study.\n",
    "    \n",
    "    Returns:\n",
    "        - 'pretest_accuracy': Correlation with ground truth\n",
    "        - 'posttest_accuracy': Correlation with ground truth\n",
    "        - 'improvement': Post - Pre\n",
    "        - 'p_value': Significance of improvement (paired t-test)\n",
    "    \"\"\"\n",
    "    # TODO: Analyze human study results\n",
    "    # 1. Compute correlation with ground truth for pre-test\n",
    "    # 2. Compute correlation with ground truth for post-test\n",
    "    # 3. Test if improvement is significant (paired t-test)\n",
    "    # 4. Return summary statistics\n",
    "    \n",
    "    raise NotImplementedError(\"Implement human study analysis\")\n",
    "\n",
    "# Simulated example data\n",
    "# pretest_scores = {\n",
    "#     1: [3, 2, 4, 1, 5, 2, 3, 4, 2, 1],  # Participant 1's ratings\n",
    "#     2: [2, 3, 3, 2, 4, 1, 2, 5, 3, 2],\n",
    "#     3: [4, 2, 5, 1, 5, 3, 4, 4, 2, 1],\n",
    "# }\n",
    "# \n",
    "# posttest_scores = {\n",
    "#     1: [4, 3, 5, 1, 5, 2, 4, 5, 2, 1],  # After learning\n",
    "#     2: [3, 3, 4, 1, 5, 1, 3, 5, 2, 1],\n",
    "#     3: [5, 3, 5, 1, 5, 2, 5, 5, 2, 1],\n",
    "# }\n",
    "# \n",
    "# ground_truth = [5, 3, 5, 1, 5, 2, 4, 5, 2, 1]  # True concept strength\n",
    "# \n",
    "# results = analyze_human_study(pretest_scores, posttest_scores, ground_truth)\n",
    "# \n",
    "# print(f\"Human Study Results:\")\n",
    "# print(f\"  Pre-test accuracy: {results['pretest_accuracy']:.3f}\")\n",
    "# print(f\"  Post-test accuracy: {results['posttest_accuracy']:.3f}\")\n",
    "# print(f\"  Improvement: {results['improvement']:.3f}\")\n",
    "# print(f\"  p-value: {results['p_value']:.4f}\")\n",
    "# \n",
    "# if results['improvement'] > 0 and results['p_value'] < 0.05:\n",
    "#     print(\"✓ Humans significantly improved after learning the concept\")\n",
    "# else:\n",
    "#     print(\"✗ No significant improvement observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Population-Level Analysis (Shin et al. Style)\n",
    "\n",
    "Analyze if model performance or human usage has changed over time.\n",
    "\n",
    "**Exercise 7.1:** Simulate Shin et al.'s analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_improvement(\n",
    "    historical_data: List[Tuple[str, str, int]],  # (date, text, quality_score)\n",
    "    intervention_date: str,  # When AI was released\n",
    "    cav: np.ndarray,\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze if concept usage improved after AI intervention (à la Shin et al.).\n",
    "    \n",
    "    Args:\n",
    "        historical_data: Time series of human-generated text\n",
    "        intervention_date: When superhuman AI was released\n",
    "    \n",
    "    Returns:\n",
    "        - 'before_mean': Mean quality before intervention\n",
    "        - 'after_mean': Mean quality after intervention\n",
    "        - 'improvement': After - Before\n",
    "        - 'p_value': Statistical significance\n",
    "    \"\"\"\n",
    "    # TODO: Implement temporal analysis\n",
    "    # 1. Split data into before/after intervention\n",
    "    # 2. Compute quality metrics for each period\n",
    "    # 3. Test if improvement is significant\n",
    "    # 4. Analyze role of novelty (CAV activation)\n",
    "    \n",
    "    raise NotImplementedError(\"Implement temporal analysis\")\n",
    "\n",
    "# Simulated historical data\n",
    "# historical_data = [\n",
    "#     (\"2020-01-01\", \"Some text\", 3),\n",
    "#     (\"2020-06-01\", \"Some text\", 3),\n",
    "#     (\"2021-01-01\", \"Some text\", 4),  # After AI release\n",
    "#     (\"2021-06-01\", \"Some text\", 4),\n",
    "#     # ... (would need hundreds of examples)\n",
    "# ]\n",
    "# \n",
    "# results = analyze_temporal_improvement(\n",
    "#     historical_data,\n",
    "#     intervention_date=\"2020-07-01\",\n",
    "#     cav=cav,\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "# \n",
    "# print(f\"Temporal Analysis (Shin et al. Style):\")\n",
    "# print(f\"  Quality before AI: {results['before_mean']:.2f}\")\n",
    "# print(f\"  Quality after AI: {results['after_mean']:.2f}\")\n",
    "# print(f\"  Improvement: {results['improvement']:.2f}\")\n",
    "# print(f\"  p-value: {results['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Building a Concept Atlas\n",
    "\n",
    "Organize all concepts discovered in your project.\n",
    "\n",
    "**Exercise 8.1:** Create a concept taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptAtlas:\n",
    "    \"\"\"Organize and visualize discovered concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.concepts = {}  # concept_name -> metadata\n",
    "    \n",
    "    def add_concept(\n",
    "        self,\n",
    "        name: str,\n",
    "        cav: np.ndarray,\n",
    "        layer: int,\n",
    "        teachability_score: float,\n",
    "        novelty_score: float,\n",
    "        abstraction_level: str = 'mid',  # 'low', 'mid', 'high'\n",
    "        domain_specific: bool = True,\n",
    "        description: str = \"\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a concept to the atlas with metadata.\n",
    "        \"\"\"\n",
    "        self.concepts[name] = {\n",
    "            'cav': cav,\n",
    "            'layer': layer,\n",
    "            'teachability': teachability_score,\n",
    "            'novelty': novelty_score,\n",
    "            'abstraction': abstraction_level,\n",
    "            'domain_specific': domain_specific,\n",
    "            'description': description\n",
    "        }\n",
    "    \n",
    "    def compute_concept_similarity(\n",
    "        self,\n",
    "        concept1: str,\n",
    "        concept2: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two concept vectors.\n",
    "        \"\"\"\n",
    "        # TODO: Implement concept similarity\n",
    "        raise NotImplementedError(\"Implement concept similarity\")\n",
    "    \n",
    "    def visualize_atlas(self):\n",
    "        \"\"\"\n",
    "        Visualize concept atlas with 2D projection.\n",
    "        \"\"\"\n",
    "        # TODO: Use UMAP or t-SNE to project CAVs to 2D\n",
    "        # TODO: Plot concepts colored by teachability, sized by novelty\n",
    "        raise NotImplementedError(\"Implement atlas visualization\")\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a summary report of the atlas.\n",
    "        \"\"\"\n",
    "        report = f\"=== Concept Atlas Summary ===\\n\\n\"\n",
    "        report += f\"Total concepts: {len(self.concepts)}\\n\\n\"\n",
    "        \n",
    "        # Group by characteristics\n",
    "        novel = [c for c, m in self.concepts.items() if m['novelty'] > 0.1]\n",
    "        teachable = [c for c, m in self.concepts.items() if m['teachability'] > 0.7]\n",
    "        \n",
    "        report += f\"Novel concepts: {len(novel)}\\n\"\n",
    "        for c in novel:\n",
    "            report += f\"  - {c}: novelty={self.concepts[c]['novelty']:.2f}\\n\"\n",
    "        \n",
    "        report += f\"\\nTeachable concepts: {len(teachable)}\\n\"\n",
    "        for c in teachable:\n",
    "            report += f\"  - {c}: teachability={self.concepts[c]['teachability']:.2f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Build atlas for your project\n",
    "# atlas = ConceptAtlas()\n",
    "# \n",
    "# # Add concepts discovered in previous weeks\n",
    "# atlas.add_concept(\n",
    "#     name=\"Politeness\",\n",
    "#     cav=cav,\n",
    "#     layer=6,\n",
    "#     teachability_score=0.85,\n",
    "#     novelty_score=0.05,  # Not novel (present in human data)\n",
    "#     abstraction_level='high',\n",
    "#     domain_specific=False,\n",
    "#     description=\"Linguistic markers of polite communication\"\n",
    "# )\n",
    "# \n",
    "# # Add more concepts...\n",
    "# # atlas.add_concept(...)\n",
    "# \n",
    "# # Generate report\n",
    "# print(atlas.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Concept Relationships\n",
    "\n",
    "Analyze how concepts relate to each other.\n",
    "\n",
    "**Exercise 9.1:** Test for compositional and prerequisite relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_composition(\n",
    "    cav_a: np.ndarray,\n",
    "    cav_b: np.ndarray,\n",
    "    cav_c: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test if concept C is a composition of concepts A and B.\n",
    "    \n",
    "    Method: Check if C ≈ α·A + β·B for some weights α, β.\n",
    "    \n",
    "    Returns:\n",
    "        - 'alpha': Weight for A\n",
    "        - 'beta': Weight for B\n",
    "        - 'reconstruction_error': ||C - (α·A + β·B)||\n",
    "    \"\"\"\n",
    "    # TODO: Implement composition test\n",
    "    # 1. Solve for optimal α, β via least squares\n",
    "    # 2. Compute reconstruction error\n",
    "    # 3. Return results\n",
    "    \n",
    "    raise NotImplementedError(\"Implement composition test\")\n",
    "\n",
    "def test_prerequisite(\n",
    "    concept_a_examples: List[str],\n",
    "    concept_b_examples: List[str],\n",
    "    cav_a: np.ndarray,\n",
    "    cav_b: np.ndarray,\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test if concept A is a prerequisite for concept B.\n",
    "    \n",
    "    Method: Check if all B examples activate A, but not vice versa.\n",
    "    \n",
    "    Returns:\n",
    "        - 'b_implies_a': Fraction of B examples that activate A\n",
    "        - 'a_implies_b': Fraction of A examples that activate B\n",
    "    \"\"\"\n",
    "    # TODO: Implement prerequisite test\n",
    "    # 1. Check A activation on B examples\n",
    "    # 2. Check B activation on A examples\n",
    "    # 3. If b_implies_a >> a_implies_b, then A is prerequisite for B\n",
    "    \n",
    "    raise NotImplementedError(\"Implement prerequisite test\")\n",
    "\n",
    "# Example: Test if \"Formality\" = \"Politeness\" + \"Professional Vocabulary\"\n",
    "# composition_results = test_composition(politeness_cav, vocabulary_cav, formality_cav)\n",
    "# print(f\"Composition test: Formality = {composition_results['alpha']:.2f}·Politeness + {composition_results['beta']:.2f}·Vocabulary\")\n",
    "# print(f\"Reconstruction error: {composition_results['reconstruction_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Final Project Validation\n",
    "\n",
    "Apply all Week 11 methods to validate your research project.\n",
    "\n",
    "**Exercise 10.1:** Complete validation checklist for your project concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PROJECT VALIDATION TEMPLATE ==========\n",
    "\n",
    "MY_CONCEPT = \"[Your concept here]\"\n",
    "\n",
    "def validate_project_concept(\n",
    "    concept_name: str,\n",
    "    concept_examples: List[str],\n",
    "    random_examples: List[str],\n",
    "    human_examples: List[str],\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Complete validation pipeline for your project concept.\n",
    "    \n",
    "    Runs all tests from Week 11:\n",
    "    1. Extract CAV\n",
    "    2. Test teachability\n",
    "    3. Assess novelty\n",
    "    4. Generate human study materials\n",
    "    5. Build concept atlas entry\n",
    "    \n",
    "    Returns:\n",
    "        Complete validation report\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Step 1: Extract CAV\n",
    "    print(f\"Validating concept: {concept_name}\")\n",
    "    print(\"Step 1: Extracting CAV...\")\n",
    "    # TODO: Extract CAV across all layers, find best layer\n",
    "    \n",
    "    # Step 2: Teachability\n",
    "    print(\"Step 2: Testing teachability...\")\n",
    "    # TODO: Train student model, compute teachability score\n",
    "    \n",
    "    # Step 3: Novelty\n",
    "    print(\"Step 3: Assessing novelty...\")\n",
    "    # TODO: Compare with human data, compute novelty score\n",
    "    \n",
    "    # Step 4: Human study materials\n",
    "    print(\"Step 4: Generating study materials...\")\n",
    "    # TODO: Create pre/post test items\n",
    "    \n",
    "    # Step 5: Atlas entry\n",
    "    print(\"Step 5: Adding to concept atlas...\")\n",
    "    # TODO: Add to atlas with metadata\n",
    "    \n",
    "    return report\n",
    "\n",
    "def generate_final_report(\n",
    "    validation_results: Dict[str, any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a publication-ready summary of validation results.\n",
    "    \"\"\"\n",
    "    report = f\"\"\"=== Final Validation Report ===\n\nConcept: {MY_CONCEPT}\n\n## 1. Concept Extraction\n- Best layer: Layer X\n- CAV accuracy: XX%\n\n## 2. Teachability Assessment\n- Student model test accuracy: XX%\n- Improvement over baseline: XX%\n- Conclusion: [TEACHABLE / NOT TEACHABLE]\n\n## 3. Novelty Assessment\n- Human data activation: X.XX\n- Model data activation: X.XX\n- Novelty score: X.XX (p < X.XX)\n- Conclusion: [NOVEL / NOT NOVEL]\n\n## 4. Human Study Design\n- Participants needed: N=X\n- Study type: [Pre/Post / Comparison]\n- Materials: [Link to questionnaire]\n\n## 5. Integration with Previous Weeks\n- Week 5 (Circuits): [Summary of circuit findings]\n- Week 6 (Probes): [Summary of probe findings]\n- Week 7 (SAEs): [Summary of SAE findings]\n- Week 8 (IIA): [Summary of causal validation]\n- Week 9 (Attribution): [Summary of attribution findings]\n\n## 6. Final Claim\nBased on rigorous validation using multiple methods, we find that:\n[Your validated claim here]\n\n## 7. Limitations\n[List limitations of your approach]\n\n## 8. Future Work\n[Suggestions for further validation or extension]\n\"\"\"\n",
    "    return report\n",
    "\n",
    "# Run complete validation\n",
    "# validation_results = validate_project_concept(\n",
    "#     MY_CONCEPT,\n",
    "#     concept_examples=[...],\n",
    "#     random_examples=[...],\n",
    "#     human_examples=[...],\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "# \n",
    "# # Generate final report\n",
    "# final_report = generate_final_report(validation_results)\n",
    "# print(final_report)\n",
    "# \n",
    "# # Save to file for your paper\n",
    "# with open(f\"{MY_CONCEPT}_validation_report.txt\", \"w\") as f:\n",
    "#     f.write(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Research Guidelines\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Two modes of knowledge transfer:**\n",
    "   - Passive (Shin): Humans improve from observing AI\n",
    "   - Active (Schut): Explicitly extract and teach concepts\n",
    "\n",
    "2. **Validation criteria:**\n",
    "   - **Teachability:** Can students/humans learn it?\n",
    "   - **Novelty:** Is it absent from human data?\n",
    "   - **Utility:** Does it improve human performance?\n",
    "\n",
    "3. **Human studies are crucial:**\n",
    "   - Even small studies (n=3-5) provide valuable evidence\n",
    "   - Measure performance, not just trust\n",
    "   - Use pre-post designs or comparisons\n",
    "\n",
    "4. **Concept atlases organize findings:**\n",
    "   - Map relationships (composition, prerequisite)\n",
    "   - Assess coverage (what fraction explained?)\n",
    "   - Locate your concept in the landscape\n",
    "\n",
    "**For Your Research Paper:**\n",
    "\n",
    "✅ **DO:**\n",
    "- Extract CAVs and test across multiple layers\n",
    "- Validate teachability with student models\n",
    "- Assess novelty by comparing with training data\n",
    "- Design human studies (even small ones)\n",
    "- Build a concept atlas for your domain\n",
    "- Integrate with findings from Weeks 1-10\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Claim \"superhuman\" knowledge without novelty testing\n",
    "- Skip teachability assessment\n",
    "- Rely only on computational metrics (validate with humans)\n",
    "- Treat concepts in isolation (analyze relationships)\n",
    "- Over-generalize from one model or dataset\n",
    "\n",
    "**Next Steps:**\n",
    "1. Complete the validation pipeline for your project\n",
    "2. Generate final validation report\n",
    "3. Prepare for Week 12 presentations\n",
    "4. Write up findings for NeurIPS submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
