<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 3: Evaluation Methodology - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 3: Evaluation Methodology</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">← Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      Before we can understand what's happening inside a large language model, we need to know how to measure what it
      does. This week introduces the fundamental methods for evaluating LLM behavior, from basic probability outputs to
      sophisticated benchmarking strategies. You'll learn how to design effective prompts, interpret model outputs, and
      assess whether a model has truly learned a concept or simply memorized training data.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Describe what an autoregressive language modeling neural network calculates, including its inputs and how it
        outputs probabilities for a single token</li>
      <li>Explain how to sample probabilistic output autoregressively to generate long outputs</li>
      <li>Apply three prompting strategies for measuring model capabilities: instruction-following, cloze prompts, and
        in-context learning</li>
      <li>Use methods for conforming to easy-to-evaluate output formats through ICL, custom decoding, or MCQ prompting
      </li>
      <li>Evaluate long-form output using an LLM-as-judge approach</li>
      <li>Understand and compute evaluation metrics: precision, recall, F1, and perplexity</li>
      <li>Apply statistical significance testing and interpret error bars in evaluation results</li>
      <li>Distinguish between memorization and generalization, and identify the risk of training data overlap and
        contamination</li>
      <li>Describe the approaches used by HELM to create fair and accurate benchmarks</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2212.09251" target="_blank">Discovering Language Model Behaviors with Model-Written Evaluations</a>
        </div>
        <div class="reading-description">Perez et al. (2022). Using LLMs to generate evaluation datasets at scale.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2306.05685" target="_blank">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>
        </div>
        <div class="reading-description">Zheng et al. (2023). When and how to use LLMs as evaluators for open-ended outputs.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1909.01066" target="_blank">Language Models as Knowledge Bases?</a>
        </div>
        <div class="reading-description">Petroni et al. (2019). The LAMA benchmark: probing factual knowledge via cloze-style tasks.</div>
      </div>
    </div>

    <h3>Supplementary Materials</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="model-written-evals.html">Model-Written Evaluations for Interpretability Research</a>
        </div>
        <div class="reading-description">Tutorial essay on using LLMs to generate evaluation datasets, with pun understanding as a running example.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot Learners (GPT-3)</a>
        </div>
        <div class="reading-description">Brown et al. (2020). Introduced the few-shot evaluation paradigm that now dominates the field.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Why Single-Token Predictions Matter for Interpretability</h2>

    <h3>1. The Autoregressive Language Model</h3>
    <p>
      At its core, a large language model is a function that takes a sequence of tokens as input and produces a
      probability distribution over <strong>a single next token</strong>. Given a sequence of tokens <code>x₁, x₂, ..., xₙ</code>, the model
      computes:
    </p>
    <p style="text-align: center; font-style: italic;">
      P(xₙ₊₁ | x₁, x₂, ..., xₙ)
    </p>
    <p>
      This is a distribution over the entire vocabulary, typically 50,000 to 100,000+ tokens. The model doesn't directly
      generate paragraphs or essays; it generates probabilities for <em>one token at a time</em>. Understanding this distinction is crucial for interpretability research.
    </p>

    <h3>2. Why Single-Token Predictions Are Easy to Study</h3>
    <p>
      The one-token-at-a-time nature of autoregressive models is a gift for interpretability. When a model makes a single prediction, we know exactly where and when the decision happens:
    </p>
    <ul>
      <li><strong>Logit lens:</strong> Decode intermediate layers at the final token position to watch the prediction evolve layer by layer</li>
      <li><strong>Activation patching:</strong> Intervene at a known position rather than searching across the sequence</li>
      <li><strong>Probing:</strong> Extract activations at exactly the decision point, avoiding noise from irrelevant positions</li>
      <li><strong>Probability comparison:</strong> Compare logits for candidate completions directly (e.g., P("Yes") vs P("No"))</li>
    </ul>
    <p>
      This is why <strong>cloze-style evaluation</strong> ("The capital of France is ___") is so powerful for mechanistic interpretability: the model's entire reasoning must culminate in a single token prediction at a known position.
    </p>

    <h3>3. The Challenge of Evaluating Longer Outputs</h3>
    <p>
      But not everything reduces to single-token predictions. Sometimes we need to evaluate:
    </p>
    <ul>
      <li>Extended reasoning chains</li>
      <li>Creative writing quality</li>
      <li>Dialogue coherence</li>
      <li>Following complex multi-step instructions</li>
    </ul>
    <p>
      When the model generates 100 tokens, the "decision" is distributed across all of them. There's no single position where we can say "this is where the model decided to be helpful/harmful/creative." This makes interpretability much harder.
    </p>

    <h3>4. Autoregressive Sampling (How Long Outputs Happen)</h3>
    <p>
      To generate longer sequences, we sample from the model autoregressively, one token at a time:
    </p>
    <ul>
      <li><strong>Greedy decoding:</strong> Always pick the highest probability token</li>
      <li><strong>Temperature sampling:</strong> Sample from the distribution, with temperature controlling randomness</li>
      <li><strong>Top-k sampling:</strong> Sample only from the k most likely tokens</li>
      <li><strong>Nucleus (top-p) sampling:</strong> Sample from the smallest set of tokens whose cumulative probability exceeds p</li>
    </ul>
    <p>
      Each strategy affects the diversity, coherence, and randomness of generated text. For benchmarking, greedy decoding or low-temperature sampling provides deterministic, reproducible results.
    </p>

    <h3>5. Prompting Strategies for Single-Token Evaluation</h3>
    <p>
      To make interpretability tractable, we want to reduce complex tasks to single-token predictions. Three strategies:
    </p>

    <h4>Cloze Prompts (Best for Interpretability)</h4>
    <p>
      Format the task as text completion, measuring the probability of specific completions:
    </p>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      "The capital of France is ___"
    </code>
    <p>
      We can evaluate whether "Paris" has higher probability than alternatives. The decision happens at a known position.
    </p>

    <h4>Multiple Choice Questions</h4>
    <p>
      Force selection from predefined options by comparing token probabilities for "A", "B", "C", "D":
    </p>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Which is the capital of France?<br>
      A) London  B) Paris  C) Berlin  D) Madrid<br>
      Answer:
    </code>
    <p>
      The model's prediction at the final position tells us everything. We can extract activations here and compare logits across options.
    </p>

    <h4>In-Context Learning (ICL)</h4>
    <p>
      Provide examples in the prompt to demonstrate the task, then use cloze format for the test case:
    </p>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Input: The movie was terrible.<br>
      Sentiment: Negative<br><br>
      Input: I loved every minute!<br>
      Sentiment: Positive<br><br>
      Input: It was okay, nothing special.<br>
      Sentiment:
    </code>
    <p>
      ICL is valuable for interpretability because you can compare activations with and without the in-context examples to see how they reshape processing.
    </p>

    <h3>6. LLM-as-Judge: When Single Tokens Are Not Enough</h3>
    <p>
      Some capabilities genuinely require evaluating longer outputs: writing quality, reasoning coherence, helpfulness. When you must evaluate runs of text rather than single tokens, <strong>LLM-as-judge</strong> provides an automated approach:
    </p>
    <ul>
      <li>Provide the judge with evaluation criteria and rubrics</li>
      <li>Ask for both a score and justification</li>
      <li>Use pairwise comparison rather than absolute scoring for better reliability</li>
      <li>Be aware of biases: judges favor longer responses, their own outputs, and certain styles</li>
    </ul>
    <p>
      MT-Bench demonstrated that LLM judges correlate well with human judgments for many tasks, but always validate on
      a subset with human evaluation.
    </p>
    <div style="background-color: #e8f4f8; border-left: 4px solid #0055a4; padding: 15px; margin: 15px 0;">
      <strong>For interpretability research:</strong> Try to convert free-form tasks to single-token formats when possible. Instead of "Explain why this is a pun" (hard to interpret), use "This is a pun because the word ___ has two meanings" (cloze format, tractable).
    </div>

    <h3>7. Creating Evaluation Datasets at Scale</h3>
    <p>
      The bottleneck in interpretability research is often data: you need hundreds or thousands of examples that cleanly test your concept. <a href="model-written-evals.html">Model-written evaluations</a> (Perez et al. 2022) offer a solution: use LLMs to generate evaluation datasets.
    </p>
    <p>The core workflow:</p>
    <ol>
      <li><strong>Specify the behavior</strong> you want to evaluate in a clear prompt</li>
      <li><strong>Generate examples</strong> by prompting a capable LLM with instructions and seed examples</li>
      <li><strong>Filter for quality</strong> using automated checks and human review</li>
      <li><strong>Run evaluations</strong> on target models</li>
    </ol>
    <p>
      For interpretability, design your generation prompts to produce <strong>cloze-format</strong> or <strong>MCQ</strong> examples. This ensures the resulting dataset has the token-localization properties you need.
    </p>
    <div style="background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 15px 0;">
      <strong>Example:</strong> To study pun understanding, prompt Claude to generate "This is a pun: Yes or No? Answer:" format examples. Generate 500+ raw examples, filter to 200+ high-quality ones, and you have a dataset suitable for probing, patching, and logit lens analysis.
    </div>
    <p>
      See the <a href="model-written-evals.html">supplementary essay</a> for detailed prompts and a complete pipeline example using puns.
    </p>

    <h3>8. Evaluation Metrics</h3>
    <p>
      Different tasks require different metrics:
    </p>
    <ul>
      <li><strong>Accuracy:</strong> Proportion of correct predictions (good for balanced classes)</li>
      <li><strong>Precision/Recall/F1:</strong> When classes are imbalanced or false positives/negatives have different costs</li>
      <li><strong>Perplexity:</strong> How "surprised" the model is by a sequence (lower = better)</li>
    </ul>

    <h3>9. Statistical Significance</h3>
    <p>
      A model scoring 85% vs 83% might not be meaningfully different. Quantify uncertainty:
    </p>
    <ul>
      <li><strong>Standard error:</strong> For accuracy on n samples, SE = √(p(1-p)/n)</li>
      <li><strong>Confidence intervals:</strong> 95% CI is approximately ± 2 × SE for large n</li>
      <li><strong>Bootstrap resampling:</strong> Repeatedly sample from your test set to estimate score distribution</li>
    </ul>
    <p>
      Rule of thumb: differences smaller than 2-3% on typical benchmark sizes often aren't statistically significant.
    </p>

    <h3>10. Memorization vs. Generalization</h3>
    <p>
      A fundamental question: Did the model learn the concept, or did it memorize training examples?
    </p>

    <h4>Training Data Contamination</h4>
    <p>
      If test examples appeared in training data, the model might simply recall them rather than demonstrate true
      understanding. This is especially problematic because:
    </p>
    <ul>
      <li>Popular benchmarks leak into training corpora (scraped web data, code repositories)</li>
      <li>Models trained on "the whole internet" likely saw many benchmark examples</li>
      <li>Paraphrased or slightly modified examples may still trigger memorization</li>
    </ul>

    <h4>Detecting and Mitigating Contamination</h4>
    <ul>
      <li><strong>N-gram overlap:</strong> Search training data for test example substrings</li>
      <li><strong>Membership inference:</strong> Compare perplexity on test vs similar synthetic examples</li>
      <li><strong>Rephrased evaluation:</strong> Create novel rewordings of test questions</li>
      <li><strong>Held-out test sets:</strong> Keep evaluation data truly private until final evaluation</li>
    </ul>

    <h4>Generalization Tests</h4>
    <p>
      Beyond contamination, test for genuine understanding:
    </p>
    <ul>
      <li>Novel compositions of known concepts</li>
      <li>Out-of-distribution examples</li>
      <li>Counterfactual variations</li>
      <li>Transfer to related but different tasks</li>
    </ul>

    <h3>11. HELM: Holistic Evaluation</h3>
    <p>
      The Holistic Evaluation of Language Models (HELM) benchmark exemplifies comprehensive evaluation:
    </p>

    <h4>Key Principles</h4>
    <ul>
      <li><strong>Broad coverage:</strong> 42+ scenarios across question answering, reasoning, generation,
        classification, etc.</li>
      <li><strong>Multi-metric:</strong> Evaluates accuracy, calibration, robustness, fairness, bias, toxicity, and
        efficiency</li>
      <li><strong>Standardized prompting:</strong> Consistent few-shot examples and formatting across models</li>
      <li><strong>Transparency:</strong> All prompts, data, and evaluation code are public</li>
    </ul>

    <h4>Metrics Beyond Accuracy</h4>
    <ul>
      <li><strong>Calibration:</strong> Do predicted probabilities match actual correctness rates?</li>
      <li><strong>Robustness:</strong> Performance under perturbations (typos, paraphrases, contrast sets)</li>
      <li><strong>Fairness:</strong> Equal performance across demographic groups</li>
      <li><strong>Efficiency:</strong> Inference cost, carbon footprint</li>
    </ul>

    <h3>Putting It All Together</h3>
    <p>
      Effective benchmarking requires:
    </p>
    <ol>
      <li>Understanding model fundamentals (token probabilities, sampling)</li>
      <li>Choosing appropriate prompting strategies for your concept</li>
      <li>Designing tasks with evaluable outputs</li>
      <li>Selecting metrics that match your research questions</li>
      <li>Quantifying uncertainty and statistical significance</li>
      <li>Ruling out memorization through contamination checks and generalization tests</li>
      <li>Following established best practices (HELM) for reproducible, fair evaluation</li>
    </ol>
  </section>

  <section id="dataset-construction">
    <h2>Dataset Construction for Concept Research</h2>

    <p>
      For your research project, the quality of your findings depends heavily on your dataset construction. This section
      covers principles and strategies for building datasets that enable rigorous concept characterization.
    </p>

    <h3>1. Defining Your Concept Operationally</h3>

    <p>
      Before constructing a dataset, you must operationalize your concept—make it measurable.
    </p>

    <h4>From Abstract to Concrete</h4>
    <ul>
      <li><strong>Abstract concept:</strong> "politeness"</li>
      <li><strong>Operational definition:</strong> Text containing please/thank you, formal pronouns, hedging language, indirect requests</li>
      <li><strong>Measurable examples:</strong> "Could you please..." (polite) vs "Do this now" (direct)</li>
    </ul>

    <p><strong>Exercise:</strong> For your chosen concept, write down:</p>
    <ol>
      <li>3-5 concrete features that characterize it</li>
      <li>10 clear positive examples</li>
      <li>10 clear negative examples (concept absent)</li>
      <li>5 edge cases (ambiguous or borderline)</li>
    </ol>

    <h3>2. Contrastive Pair Construction</h3>

    <p>
      The gold standard for concept research: pairs of examples that differ <strong>only</strong> in your target concept.
    </p>

    <h4>Minimal Pairs Strategy</h4>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Positive: "Could you please pass the salt?"<br>
      Negative: "Pass the salt."<br><br>
      What changed: politeness marker ("Could you please")<br>
      What stayed the same: meaning, length, topic
    </code>

    <h4>Controlled Variation</h4>
    <ul>
      <li><strong>Substitute only concept-relevant words:</strong> polite → rude, formal → casual</li>
      <li><strong>Keep constant:</strong> sentence structure, length (±2 tokens), topic, complexity</li>
      <li><strong>Avoid confounds:</strong> Don't change sentiment when studying formality</li>
    </ul>

    <h4>Methods for Generating Pairs</h4>
    <ol>
      <li><strong>Template-based:</strong> "[GREETING], could you [ACTION]?" vs "[ACTION] now"</li>
      <li><strong>LLM-assisted:</strong> Prompt GPT-4 to paraphrase with/without concept</li>
      <li><strong>Human-created:</strong> Writers produce matched pairs (expensive but high quality)</li>
      <li><strong>Found pairs:</strong> Mine existing data for natural contrasts (Reddit posts with different tones)</li>
    </ol>

    <h3>3. Balancing and Sampling Strategies</h3>

    <h4>Class Balance</h4>
    <p>
      For binary concepts (polite/rude, factual/fictional), aim for 50-50 split unless you have specific research reasons.
    </p>
    <ul>
      <li><strong>Imbalanced data risks:</strong> Model might learn "always predict majority class"</li>
      <li><strong>Probe issues:</strong> Linear probes are especially sensitive to imbalance</li>
      <li><strong>Evaluation fairness:</strong> Accuracy is misleading with skewed data</li>
    </ul>

    <h4>Stratified Sampling</h4>
    <p>Ensure diversity across relevant dimensions:</p>
    <table style="width: 100%; border-collapse: collapse; margin: 10px 0;">
      <tr>
        <th style="border: 1px solid #ddd; padding: 8px;">Dimension</th>
        <th style="border: 1px solid #ddd; padding: 8px;">Why It Matters</th>
        <th style="border: 1px solid #ddd; padding: 8px;">Example</th>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Topic</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Prevent concept from being confounded with domain</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Politeness in: requests, apologies, greetings, refusals</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Length</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Avoid "polite = longer" spurious correlation</td>
        <td style="border: 1px solid #ddd; padding: 8px;">5-10 tokens, 10-20 tokens, 20-40 tokens (balanced)</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Syntactic structure</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Ensure concept isn't just syntax</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Questions, statements, commands</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Formality</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Separate from related concepts</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Formal-polite, informal-polite, formal-rude, informal-rude</td>
      </tr>
    </table>

    <h3>4. Avoiding Common Pitfalls</h3>

    <h4>Confound: Concept Leakage</h4>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #fff3cd;">
      <strong>Bad dataset:</strong><br>
      Positive (polite): "Thank you so much for your help! I really appreciate it."<br>
      Negative (rude): "Whatever."<br><br>
      <strong>Problem:</strong> Length, sentiment, and informativeness all vary—not just politeness!
    </code>

    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #e8f4f8;">
      <strong>Better dataset:</strong><br>
      Positive (polite): "Thanks for your help."<br>
      Negative (direct): "Got it, bye."<br><br>
      <strong>Better:</strong> Similar length, neutral tone, only politeness marker differs
    </code>

    <h4>Pitfall: Lexical Shortcuts</h4>
    <p>
      Models might learn "if text contains 'please' → polite" without understanding context.
    </p>
    <ul>
      <li><strong>Test:</strong> Include "please" in rude examples ("Please shut up")</li>
      <li><strong>Include:</strong> Polite examples without marker words (indirect requests: "I was wondering if you might...")</li>
      <li><strong>Verify:</strong> Concept isn't just keyword matching</li>
    </ul>

    <h4>Pitfall: Training Data Overlap</h4>
    <p>
      If your test examples appeared in the model's training data, you're testing memorization, not understanding.
    </p>
    <ul>
      <li><strong>Create novel examples:</strong> Don't copy from web datasets</li>
      <li><strong>Paraphrase significantly:</strong> Change word choice, structure, context</li>
      <li><strong>Synthetic generation:</strong> Use templates or LLM generation for fresh data</li>
    </ul>

    <h3>5. Dataset Size Guidelines</h3>

    <p>
      How much data do you need? It depends on your research method:
    </p>

    <table style="width: 100%; border-collapse: collapse; margin: 10px 0;">
      <tr>
        <th style="border: 1px solid #ddd; padding: 8px;">Method (Week)</th>
        <th style="border: 1px solid #ddd; padding: 8px;">Minimum</th>
        <th style="border: 1px solid #ddd; padding: 8px;">Recommended</th>
        <th style="border: 1px solid #ddd; padding: 8px;">Purpose</th>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Benchmarking (Week 2)</td>
        <td style="border: 1px solid #ddd; padding: 8px;">100</td>
        <td style="border: 1px solid #ddd; padding: 8px;">500-1000</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Test set for measuring accuracy</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Steering (Week 1)</td>
        <td style="border: 1px solid #ddd; padding: 8px;">10-20 pairs</td>
        <td style="border: 1px solid #ddd; padding: 8px;">50-100 pairs</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Contrastive pairs for activation difference</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Probing (Week 5)</td>
        <td style="border: 1px solid #ddd; padding: 8px;">500</td>
        <td style="border: 1px solid #ddd; padding: 8px;">2000-5000</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Training set for linear classifier</td>
      </tr>
      <tr>
        <td style="border: 1px solid #ddd; padding: 8px;">Visualization (Week 3)</td>
        <td style="border: 1px solid #ddd; padding: 8px;">100</td>
        <td style="border: 1px solid #ddd; padding: 8px;">500-1000</td>
        <td style="border: 1px solid #ddd; padding: 8px;">Examples to visualize in 2D/3D</td>
      </tr>
    </table>

    <h3>6. Documentation and Reproducibility</h3>

    <p>
      Your dataset is a research artifact. Document it properly:
    </p>

    <ul>
      <li><strong>Datasheet:</strong> Purpose, creation method, annotator info, known limitations</li>
      <li><strong>Examples:</strong> Show 10-20 representative samples</li>
      <li><strong>Statistics:</strong> Size, balance, length distribution, vocabulary size</li>
      <li><strong>Annotation guidelines:</strong> How you (or annotators) decided positive vs negative</li>
      <li><strong>Version control:</strong> Track changes, note when you add/remove examples</li>
    </ul>

    <h3>7. Quick Start: Building Your First Dataset</h3>

    <p><strong>Week 2 Goal:</strong> Create a small, high-quality evaluation dataset (100-200 examples) for your concept.</p>

    <ol>
      <li><strong>Day 1-2:</strong> Write operational definition + 20 manual examples</li>
      <li><strong>Day 3:</strong> Create 50 contrastive pairs using templates or LLM assistance</li>
      <li><strong>Day 4:</strong> Balance across topics/lengths, check for confounds</li>
      <li><strong>Day 5:</strong> Run initial benchmarks, identify dataset weaknesses</li>
      <li><strong>Day 6-7:</strong> Expand to 100-200 examples, focusing on edge cases and diversity</li>
    </ol>

    <p>
      <strong>Deliverable:</strong> A JSON/CSV file with examples, labels, and metadata (topic, length, etc.) ready for benchmarking.
    </p>

    <h3>Key Takeaways</h3>
    <ul>
      <li>Operationalize concepts concretely before collecting data</li>
      <li>Contrastive pairs are the gold standard—minimize confounds</li>
      <li>Balance classes and stratify across relevant dimensions</li>
      <li>Avoid lexical shortcuts and training data overlap</li>
      <li>Size needs vary by method: 100 for benchmarking, 2000+ for probing</li>
      <li>Document your dataset as a research artifact</li>
    </ul>
  </section>

  <section id="in-class-exercise">
    <h2>In-Class Exercise: Pun Evaluation Dataset</h2>

    <p>
      Building on the pun theme from Week 1, we'll create a complete evaluation pipeline: generate data, test multiple evaluation methods, and apply logit lens to see where pun understanding emerges.
    </p>

    <h3>Part 1: Prompt Engineering for Dataset Generation (15 min)</h3>
    <p>
      Work in groups to craft a prompt for Claude that generates high-quality pun evaluation examples. Your prompt should produce examples in cloze format for interpretability tractability.
    </p>

    <div style="background-color: #f4f4f4; padding: 15px; margin: 15px 0; border-radius: 5px;">
      <strong>Starter prompt to iterate on:</strong>
      <pre style="margin: 10px 0; white-space: pre-wrap;">Generate pun recognition examples in cloze format. For each example:
1. Provide a sentence that may or may not be a pun
2. Format: "[sentence]" This is a pun. Yes or No? Answer:
3. Provide the correct label (Yes/No)
4. For puns, briefly note the wordplay mechanism

Generate 20 examples:
- 10 puns (varied mechanisms: homophones, polysemy, syntactic)
- 10 non-puns (similar structure but no wordplay)
- Vary topics: professions, food, animals, science, everyday life</pre>
    </div>

    <p><strong>Discussion questions:</strong></p>
    <ul>
      <li>How do we ensure the non-puns aren't obviously different from puns?</li>
      <li>Should we include "almost-puns" that fail to land?</li>
      <li>How do we avoid Claude generating the same classic puns everyone knows?</li>
    </ul>

    <h3>Part 2: Testing Evaluation Methods (20 min)</h3>
    <p>
      Using the <a href="https://nnsight.net/" target="_blank">NDIF workbench</a>, we'll compare different evaluation approaches on the same pun examples:
    </p>

    <h4>Method A: Probability Comparison</h4>
    <p>Compare P("Yes") vs P("No") at the answer position:</p>
    <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">prompt = '"I used to be a banker but I lost interest." This is a pun. Yes or No? Answer:'
# Extract logits at final position
# Compare logit["Yes"] vs logit["No"]
# Classify based on which is higher</pre>

    <h4>Method B: In-Context Learning</h4>
    <p>Provide 3-4 examples before the test case:</p>
    <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">"The bicycle was two-tired." Pun? Yes
"The bicycle was broken." Pun? No
"I lost interest in banking." Pun? Yes
"I lost patience with banking." Pun? No

"Time flies like an arrow." Pun?</pre>

    <h4>Method C: MCQ Format</h4>
    <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px;">Why is "I lost interest" a pun when said by a former banker?
A) It references losing money
B) "Interest" means both curiosity and financial returns
C) Banking is boring
D) It's not actually a pun
Answer:</pre>

    <p><strong>Compare:</strong> Which method gives the clearest signal? Which is easiest to interpret mechanistically?</p>

    <h3>Part 3: Logit Lens on Puns (25 min)</h3>
    <p>
      The key question: <strong>At which layer does the model "get" the pun?</strong>
    </p>

    <p>For a set of pun/non-pun minimal pairs, we'll:</p>
    <ol>
      <li>Run logit lens at each layer on the final token position</li>
      <li>Track when "Yes" probability emerges for puns vs "No" for non-puns</li>
      <li>Compare across model sizes (if time permits)</li>
    </ol>

    <div style="background-color: #e8f4f8; border-left: 4px solid #0055a4; padding: 15px; margin: 15px 0;">
      <strong>Hypothesis to test:</strong> Pun understanding requires integrating information about word meanings (likely mid-to-late layers). Early layers should show similar predictions for puns and non-puns; divergence indicates where the model processes the wordplay.
    </div>

    <h4>Analysis Template</h4>
    <pre style="background-color: #f4f4f4; padding: 10px; border-radius: 5px; font-size: 0.9em;">from nnsight import LanguageModel

model = LanguageModel("meta-llama/Llama-3.2-1B", device_map="auto")

pun_prompt = '"I used to be a banker but I lost interest." This is a pun. Yes or No? Answer:'
non_pun_prompt = '"I used to be a banker but I lost patience." This is a pun. Yes or No? Answer:'

# For each layer, decode the residual stream at the final position
# Track: P("Yes") - P("No") across layers
# Plot: layer vs. pun_score for puns and non-puns
# Look for: divergence point where model distinguishes pun from non-pun</pre>

    <h3>Discussion</h3>
    <ul>
      <li>Do smaller models (1B) show pun understanding? At what scale does it emerge?</li>
      <li>Is pun understanding "all or nothing" or does it develop gradually across layers?</li>
      <li>Do different pun types (homophones vs. polysemy) show different patterns?</li>
      <li>How does ICL context change the layer-by-layer pattern?</li>
    </ul>
  </section>

  <section id="code-exercise">
    <h2>Code Exercise</h2>
    <p>
      This week's exercise will give you hands-on experience with the core concepts:
    </p>
    <ul>
      <li>Load a small language model and examine token probability distributions</li>
      <li>Implement the pun evaluation methods from the in-class exercise</li>
      <li>Run logit lens analysis on a pun dataset</li>
      <li>Compare model sizes: at what scale does pun understanding emerge?</li>
      <li>Calculate accuracy with confidence intervals</li>
    </ul>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER" target="_blank" class="colab-button">
      Open Exercise in Google Colab
    </a>

    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be created and linked here.</em>
    </p>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 2</strong></p>
      <p>
        Now that you've identified a promising concept through steering, it's time to formalize it with a systematic benchmark
        and test it across multiple models to find the best target for deep investigation.
      </p>

      <h4>Part 1: Design Your Benchmark</h4>
      <ul>
        <li><strong>Operational definition:</strong> Translate your concept into measurable, concrete criteria (see dataset construction section above)</li>
        <li><strong>Contrastive pairs:</strong> Create 50-100 minimal pairs that differ primarily in your concept
          <ul>
            <li>Example (formality): "Could you help me?" vs "Help me"</li>
            <li>Control for confounds (length, topic, sentiment, etc.)</li>
          </ul>
        </li>
        <li><strong>Test set:</strong> Build a held-out test set of 500-1000 examples
          <ul>
            <li>Use stratified sampling (see guidelines above)</li>
            <li>Document data sources and collection methods</li>
            <li>Ensure balanced classes</li>
          </ul>
        </li>
        <li><strong>Evaluation metric:</strong> Define how you'll measure concept understanding
          <ul>
            <li>Probability differences, accuracy, linear separability, etc.</li>
            <li>Statistical significance thresholds</li>
          </ul>
        </li>
      </ul>

      <h4>Part 2: Test Across Models</h4>
      <ul>
        <li><strong>Model selection:</strong> Test at least 3 models of varying sizes
          <ul>
            <li>Include at least one small model (e.g., GPT-2, Pythia-1B, Llama-7B)</li>
            <li>Include at least one large model (e.g., GPT-3.5, Llama-70B)</li>
            <li>Consider models from different families if relevant</li>
          </ul>
        </li>
        <li><strong>Identify smallest capable model:</strong> Find the smallest model that demonstrates robust understanding of your concept
          <ul>
            <li>This will be your primary target for deep investigation in Weeks 3-11</li>
            <li>Smaller models = faster experiments, easier to interpret, more publishable insights</li>
            <li>Document performance across all tested models</li>
          </ul>
        </li>
      </ul>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Dataset:</strong> Contrastive pairs (50-100) and test set (500-1000), documented with collection methodology</li>
        <li><strong>Benchmark results:</strong> Performance table showing all tested models and metrics</li>
        <li><strong>Model selection rationale:</strong> Which model did you choose for deep study and why?</li>
        <li><strong>Initial findings:</strong> Does the concept exist across models? Any surprising patterns?</li>
        <li><strong>Code:</strong> Benchmark evaluation code (notebook or script)</li>
      </ul>

      <p><em>
        Tip: Don't overthink the "perfect" dataset at this stage. You'll refine it throughout the semester.
        Focus on getting a solid baseline that lets you move forward with confidence.
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">← Back to Course Home</a></p>
  </footer>

</body>

</html>
