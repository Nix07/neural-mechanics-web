<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 4: Representation Geometry - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }

    .math {
      font-style: italic;
      text-align: center;
      margin: 10px 0;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 4: Representation Geometry</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">← Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      This week focuses on techniques for making the invisible visible. You will learn how to visualize high-dimensional
      activation vectors, understand the geometric structure of representations, and use visualization to discover
      interpretable patterns. From PCA projections to attention heatmaps, you will develop the visual intuition
      essential for mechanistic interpretability research.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Apply fundamental linear algebra concepts: vectors, dot products, norms, matrix-vector products</li>
      <li>Understand and apply Singular Value Decomposition (SVD) and Principal Component Analysis (PCA)</li>
      <li>Use PCA to visualize high-dimensional activation vectors in 2D/3D</li>
      <li>Explain the "geometry of truth" and how concepts form linear subspaces</li>
      <li>Compute and interpret mass mean-difference vectors and Euclidean classifiers</li>
      <li>Understand the geometry of token embeddings (encoder) and unembeddings (decoder)</li>
      <li>Perform and interpret semantic-vector arithmetic (e.g., king - man + woman ≈ queen)</li>
      <li>Visualize and interpret attention patterns across heads and layers</li>
      <li>Explain how induction heads work and visualize their characteristic attention patterns</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2310.06824" target="_blank">The Geometry of Truth: Emergent Linear Structure in LLM Representations of True/False Datasets</a>
        </div>
        <div class="reading-description">Marks &amp; Tegmark (2023). Truth and falsehood correspond to a linear direction in representation space.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2310.15154" target="_blank">Linear Representations of Sentiment in Large Language Models</a>
        </div>
        <div class="reading-description">Tigges et al. (2023). Clean case study showing sentiment is encoded linearly, with practical techniques for finding concept directions.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2308.09124" target="_blank">Linearity of Relation Decoding in Transformer LMs</a>
        </div>
        <div class="reading-description">Hernandez et al. (2023). Even relational concepts like "the capital of X" are approximately linear.</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space (Word2Vec)</a>
        </div>
        <div class="reading-description">Mikolov et al. (2013). Historical origin of "linear directions encode concepts" with the famous king &minus; man + woman = queen example.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1607.06520" target="_blank">Man is to Computer Programmer as Woman is to Homemaker?</a>
        </div>
        <div class="reading-description">Bolukbasi et al. (2016). Early template for manipulating concept directions&mdash;debiasing word embeddings.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Visualizing the Geometry of Language Model Representations</h2>

    <h3>1. Linear Algebra Review</h3>
    <p>
      Before diving into visualization, let's review the mathematical tools we'll use.
    </p>

    <h4>Vectors and Operations</h4>
    <p>
      A <strong>vector</strong> is a list of numbers: <code>v = [v₁, v₂, ..., vₙ]</code>
    </p>
    <ul>
      <li><strong>Dot product:</strong> <code>v · w = v₁w₁ + v₂w₂ + ... + vₙwₙ</code> (measures similarity/projection)
      </li>
      <li><strong>Norm (length):</strong> <code>||v|| = √(v · v)</code></li>
      <li><strong>Cosine similarity:</strong> <code>cos(θ) = (v · w) / (||v|| ||w||)</code></li>
    </ul>

    <h4>Matrix-Vector Products</h4>
    <p>
      A matrix <code>M</code> transforms vectors: <code>y = Mx</code><br>
      Each row of <code>M</code> computes one dot product with <code>x</code>
    </p>

    <h4>Singular Value Decomposition (SVD)</h4>
    <p>
      Any matrix <code>M</code> can be decomposed as: <code>M = UΣVᵀ</code>
    </p>
    <ul>
      <li><code>U</code>: Left singular vectors (output space basis)</li>
      <li><code>Σ</code>: Singular values (scaling factors, diagonal matrix)</li>
      <li><code>V</code>: Right singular vectors (input space basis)</li>
    </ul>

    <h4>Principal Component Analysis (PCA)</h4>
    <p>
      PCA finds the directions of maximum variance in data:
    </p>
    <ol>
      <li>Center the data: subtract the mean</li>
      <li>Compute SVD of the centered data matrix</li>
      <li>The columns of <code>V</code> are the principal components</li>
      <li>Project data onto top k components for dimensionality reduction</li>
    </ol>

    <p>
      <strong>Why PCA matters:</strong> It lets us visualize 768-dimensional activation vectors in 2D or 3D while
      preserving the most important structure.
    </p>

    <h3>2. Visualizing Activation Vectors with PCA</h3>
    <p>
      Activation vectors live in high-dimensional space (typically 768-12,288 dimensions). PCA projects them into 2D or
      3D while preserving as much variance as possible.
    </p>

    <h4>Example: Visualizing Word Embeddings</h4>
    <ol>
      <li>Collect activation vectors for many words</li>
      <li>Apply PCA to find the top 2 principal components</li>
      <li>Plot each word at its projected coordinates</li>
      <li>Observe clusters: animals together, countries together, etc.</li>
    </ol>

    <p>
      <strong>What to look for:</strong>
    </p>
    <ul>
      <li>Semantic clusters (related concepts group together)</li>
      <li>Linear structure (arithmetic relationships visible as parallel arrows)</li>
      <li>Outliers (unusual or polysemantic words)</li>
    </ul>

    <h3>3. The Geometry of Truth</h3>
    <p>
      A remarkable finding: truth is represented as a <strong>linear direction</strong> in activation space, consistent
      across diverse contexts.
    </p>

    <h4>Key Results</h4>
    <ul>
      <li>True vs. false statements produce systematically different activations</li>
      <li>The difference vector (truth direction) generalizes across topics</li>
      <li>Truth forms a linear subspace that can be found via PCA on contrastive examples</li>
      <li>This structure emerges without explicit truth training</li>
    </ul>

    <h4>Mass Mean-Difference Vectors</h4>
    <p>
      A simple but powerful technique:
    </p>
    <ol>
      <li>Collect activations for many examples with property A</li>
      <li>Collect activations for many examples without property A</li>
      <li>Compute: <code>direction_A = mean(with_A) - mean(without_A)</code></li>
      <li>This direction captures the essence of property A</li>
    </ol>

    <p>
      <strong>Applications:</strong>
    </p>
    <ul>
      <li>Truth vs. false</li>
      <li>Positive vs. negative sentiment</li>
      <li>Formal vs. informal language</li>
      <li>Your concept vs. not-your-concept</li>
    </ul>

    <h4>Euclidean Classifiers</h4>
    <p>
      Use the mass mean-difference vector as a classifier:
    </p>
    <div class="diagram">
      score = activation · direction_vector<br>
      if score > threshold: predict property A present<br>
      else: predict property A absent
    </div>

    <p>
      This simple linear classifier often works surprisingly well, supporting the linear representation hypothesis.
    </p>

    <h3>4. The Geometry of Token Embeddings and Unembeddings</h3>

    <h4>Token Embedding Matrix (Encoder)</h4>
    <p>
      Converts token IDs to vectors: <code>E[token_id] → vector</code><br>
      Each row of <code>E</code> is a token's initial representation.
    </p>

    <h4>Unembedding Matrix (Decoder)</h4>
    <p>
      Converts final activations to vocabulary logits: <code>logits = U × activation</code><br>
      Each row of <code>U</code> is a direction in activation space that "votes" for that token.
    </p>

    <h4>Geometric Insight</h4>
    <p>
      Token embedding and unembedding matrices often share similar geometric structure:
    </p>
    <ul>
      <li>Semantically similar tokens have similar embedding vectors</li>
      <li>Similar tokens have similar unembedding directions</li>
      <li>The dot product <code>E[i] · U[j]ᵀ</code> measures how much token i "predicts" token j</li>
      <li>PCA on embedding/unembedding matrices reveals semantic clusters</li>
    </ul>

    <h3>5. Semantic-Vector Arithmetic</h3>
    <p>
      One of the most striking properties of neural language representations: you can do meaningful arithmetic with
      concept vectors.
    </p>

    <h4>Classic Examples</h4>
    <div class="diagram">
      king - man + woman ≈ queen<br>
      Paris - France + Germany ≈ Berlin<br>
      walking - walk + swim ≈ swimming
    </div>

    <h4>Why It Works</h4>
    <p>
      If concepts are linear directions, then:
    </p>
    <ul>
      <li>"Male" is a direction in embedding space</li>
      <li>"Royalty" is another direction</li>
      <li>"King" ≈ "royalty" + "male"</li>
      <li>"Queen" ≈ "royalty" + "female"</li>
      <li>Therefore: king - male + female ≈ queen</li>
    </ul>

    <h4>Limitations</h4>
    <ul>
      <li>Works best for simple, compositional relationships</li>
      <li>Quality depends on training data biases</li>
      <li>Not all concepts compose linearly</li>
    </ul>

    <h3>6. Visualizing Attention Patterns</h3>
    <p>
      Attention weights tell us which tokens each position is "looking at." Visualizing these patterns reveals
      interpretable structure.
    </p>

    <h4>Attention Heatmap</h4>
    <p>
      For each head, create a matrix where <code>A[i,j]</code> = attention from position i to position j<br>
      Plot as a heatmap with tokens on both axes.
    </p>

    <h4>Common Patterns</h4>
    <ul>
      <li><strong>Diagonal:</strong> Attending to same position (self-attention)</li>
      <li><strong>Previous token:</strong> Strong band just below diagonal</li>
      <li><strong>Beginning of sequence:</strong> Vertical stripe at position 0</li>
      <li><strong>Punctuation:</strong> Attending to periods, commas</li>
      <li><strong>Syntactic dependencies:</strong> Verbs attending to subjects</li>
    </ul>

    <h3>7. Induction Heads: Visualizing Pattern-Copying</h3>
    <p>
      Now we dive deep into <strong>induction heads</strong>, one of the first interpretable circuits discovered in
      transformers.
    </p>

    <h4>What Induction Heads Do</h4>
    <p>
      Pattern copying: if the model has seen <code>[A][B]</code> earlier, then upon seeing <code>[A]</code> again, it
      predicts <code>[B]</code>.
    </p>

    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Input: "foo bar baz foo bar baz foo"<br>
      At final "foo", model predicts: "bar" (copying the pattern)
    </code>

    <h4>How They Work: Two-Head Cooperation</h4>
    <p>
      Induction requires <strong>two attention heads working together</strong> across layers:
    </p>

    <p><strong>Step 1: Previous-Token Head (earlier layer)</strong></p>
    <ul>
      <li>Attends to position i-1 from position i</li>
      <li>Copies information about what came before</li>
      <li>Writes this to the residual stream</li>
    </ul>

    <div class="diagram">
      At "bar": attend to previous token → residual stream now encodes "previous token was foo"
    </div>

    <p><strong>Step 2: Induction Head (later layer)</strong></p>
    <ul>
      <li>Looks for positions where the previous token matches current context</li>
      <li>When it finds a match, it attends to what came after in the past</li>
      <li>Promotes that token in its output</li>
    </ul>

    <div class="diagram">
      Current: "foo"<br>
      Induction head: "Find where previous token was 'foo'"<br>
      → Finds earlier "foo bar"<br>
      → Attends to "bar" (what came after "foo")<br>
      → Predicts "bar" for current position
    </div>

    <h4>Characteristic Attention Pattern</h4>
    <p>
      Induction heads have a distinctive "stripe" pattern in attention visualization:
    </p>
    <ul>
      <li>Strong attention to positions where pattern previously occurred</li>
      <li>Offset by +1 (attending to what came after the match)</li>
      <li>Creates a characteristic diagonal stripe pattern offset from the main diagonal</li>
    </ul>

    <h3>Putting It All Together: A Visualization Toolkit</h3>
    <p>
      You now have multiple tools for understanding representations:
    </p>
    <ol>
      <li><strong>PCA:</strong> Reduce dimensionality, visualize clusters and structure</li>
      <li><strong>Mass mean-difference:</strong> Find concept directions</li>
      <li><strong>Geometric analysis:</strong> Understand truth, token encodings/decodings</li>
      <li><strong>Vector arithmetic:</strong> Test compositional structure</li>
      <li><strong>Attention visualization:</strong> Understand information flow</li>
      <li><strong>Induction head analysis:</strong> Identify pattern-copying mechanisms</li>
    </ol>

    <p>
      These techniques combine to give you a comprehensive view of how language models represent and process
      information.
    </p>
  </section>

  <section id="in-class-exercise">
    <h2>In-Class Exercise: Visualizing the Geometry of Puns</h2>
    <p>
      Building on our pun dataset from Week 3, we will visualize how the model represents puns versus non-puns
      and attempt to find a "pun direction" in activation space.
    </p>

    <h3>Part 1: Collecting Activations (15 min)</h3>
    <p>
      Using the provided notebook, load your pun dataset from Week 3 and extract activations:
    </p>
    <ol>
      <li>Load 20-30 puns and 20-30 non-pun sentences</li>
      <li>Run each through the model and extract the residual stream at multiple layers</li>
      <li>Focus on the final token position (where the punchline lands)</li>
      <li>Store activations for analysis</li>
    </ol>
    <p>
      <strong>Questions to consider:</strong> Should we extract from the punchline token specifically?
      What about the setup? Does position matter for pun representations?
    </p>

    <h3>Part 2: PCA Visualization (20 min)</h3>
    <p>
      Apply PCA to visualize whether puns and non-puns separate in activation space:
    </p>
    <ol>
      <li><strong>Layer comparison:</strong> Create PCA plots for early (layer 5), middle (layer 15), and late (layer 25) layers</li>
      <li><strong>Color by category:</strong> Puns in one color, non-puns in another</li>
      <li><strong>Examine structure:</strong> Do they cluster? Is there overlap? Linear separation?</li>
      <li><strong>Try different positions:</strong> Final token vs. middle of sentence</li>
    </ol>
    <p>
      <strong>Discussion:</strong> At which layer do puns most clearly separate from non-puns?
      Is the separation clean or fuzzy? What might this tell us about how the model processes humor?
    </p>

    <h3>Part 3: Finding the "Pun Direction" (25 min)</h3>
    <p>
      Compute a mass mean-difference vector to find the "pun direction":
    </p>
    <ol>
      <li><strong>Compute means:</strong>
        <ul>
          <li><code>mean_pun = average(activations for pun sentences)</code></li>
          <li><code>mean_nonpun = average(activations for non-pun sentences)</code></li>
        </ul>
      </li>
      <li><strong>Difference vector:</strong> <code>pun_direction = mean_pun - mean_nonpun</code></li>
      <li><strong>Test the direction:</strong>
        <ul>
          <li>Project held-out examples onto pun_direction</li>
          <li>Do puns have higher scores than non-puns?</li>
          <li>What is the classification accuracy using this simple linear classifier?</li>
        </ul>
      </li>
      <li><strong>Compare layers:</strong> Which layer gives the best pun direction?</li>
    </ol>
    <p>
      <strong>Extension:</strong> Try the same analysis for different types of puns (wordplay vs. situation comedy).
      Do they have different directions?
    </p>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER_WEEK4_INCLASS" target="_blank" class="colab-button">
      Open In-Class Notebook
    </a>
    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be linked here.</em>
    </p>
  </section>

  <section id="code-exercise">
    <h2>Code Exercise</h2>
    <p>
      This week's exercise provides hands-on practice with visualization techniques:
    </p>
    <ul>
      <li>Apply PCA to activation vectors and create 2D/3D visualizations</li>
      <li>Compute and visualize mass mean-difference vectors</li>
      <li>Explore token embedding and unembedding geometry</li>
      <li>Perform semantic-vector arithmetic</li>
      <li>Visualize attention patterns as heatmaps</li>
      <li>Find and visualize induction heads</li>
    </ul>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER" target="_blank" class="colab-button">
      Open Exercise in Google Colab
    </a>

    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be created and linked here.</em>
    </p>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 4</strong></p>
      <p>
        Now that you have selected your model and built a benchmark, dive deep into how the model represents your concept
        internally. Use visualization techniques to examine geometric structure across layers and token positions.
      </p>

      <h4>Geometric Structure Analysis</h4>
      <ul>
        <li><strong>Examine multiple layers:</strong>
          <ul>
            <li>Apply PCA or t-SNE to activations from layers 0, 25%, 50%, 75%, 100%</li>
            <li>Do concept examples cluster? Is there clear separation?</li>
            <li>Which layer has the cleanest geometric structure?</li>
          </ul>
        </li>
        <li><strong>Analyze multiple token positions:</strong> Final token, subject token, verb token, etc.
          <ul>
            <li>Does geometric structure vary by position?</li>
            <li>Where is the concept most clearly encoded?</li>
          </ul>
        </li>
        <li><strong>Test linear separability:</strong>
          <ul>
            <li>Train simple linear classifiers on different (layer, position) combinations</li>
            <li>Report accuracy to quantify geometric structure quality</li>
            <li>Create heatmap showing separability across layers x positions</li>
          </ul>
        </li>
        <li><strong>Find concept directions:</strong>
          <ul>
            <li>Compute mass mean-difference vectors for your concept</li>
            <li>Test if this direction generalizes to held-out examples</li>
            <li>Compare direction quality across layers</li>
          </ul>
        </li>
      </ul>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Geometric structure analysis:</strong>
          <ul>
            <li>PCA/t-SNE plots for multiple (layer, position) combinations</li>
            <li>Heatmap of linear separability across layers and positions</li>
            <li>Identification of optimal layer(s) and position(s) for your concept</li>
          </ul>
        </li>
        <li><strong>Concept direction analysis:</strong>
          <ul>
            <li>Mean-difference vector and its classification accuracy</li>
            <li>Visualization of concept direction in PCA space</li>
          </ul>
        </li>
        <li><strong>Written summary:</strong> Key observations about how and where your concept is represented
          <ul>
            <li>At which layer does the concept emerge?</li>
            <li>Is it localized to specific positions or distributed?</li>
            <li>Is the representation linear/geometric or more complex?</li>
          </ul>
        </li>
        <li><strong>Code:</strong> Notebook with all analysis and visualizations</li>
      </ul>

      <p><em>
        This analysis will guide your next steps: if you find a strong linear representation at specific layers/positions,
        that's where you'll focus causal interventions (Week 4) and probe training (Week 5).
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">← Back to Course Home</a></p>
  </footer>

</body>

</html>
