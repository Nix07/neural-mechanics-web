{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Skepticism and Interpretability Illusions - Exercises\n",
    "\n",
    "This notebook contains hands-on exercises for validating interpretability methods and avoiding common pitfalls. We'll implement sanity checks, robustness tests, and multi-method validation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from typing import List, Tuple, Dict\n",
    "import copy\n",
    "\n",
    "# Load model\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sanity Checks for Saliency Maps (Adebayo et al., 2018)\n",
    "\n",
    "Implement the model parameter randomization test: compare saliency maps from a trained model vs. a randomly initialized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(model, input_ids, target_position=-1):\n",
    "    \"\"\"\n",
    "    Compute simple gradient-based saliency for each input token.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: Token IDs [batch_size, seq_len]\n",
    "        target_position: Position to compute loss for (default: last token)\n",
    "    \n",
    "    Returns:\n",
    "        saliency: Gradient magnitude for each input token [seq_len]\n",
    "    \"\"\"\n",
    "    # TODO: Implement saliency computation\n",
    "    # 1. Get embeddings with requires_grad=True\n",
    "    # 2. Forward pass\n",
    "    # 3. Compute loss on target position\n",
    "    # 4. Backward to get gradients\n",
    "    # 5. Return gradient magnitudes\n",
    "    \n",
    "    input_ids = input_ids.to(model.device)\n",
    "    embeddings = model.transformer.wte(input_ids)\n",
    "    embeddings.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass using embeddings directly\n",
    "    outputs = model(inputs_embeds=embeddings)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Compute loss on target position\n",
    "    target_logits = logits[0, target_position, :]\n",
    "    target_token = input_ids[0, target_position + 1] if target_position < -1 else input_ids[0, 0]\n",
    "    loss = F.cross_entropy(target_logits.unsqueeze(0), target_token.unsqueeze(0))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient magnitudes\n",
    "    saliency = embeddings.grad.abs().sum(dim=-1).squeeze(0)\n",
    "    \n",
    "    return saliency.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def randomize_model_weights(model):\n",
    "    \"\"\"\n",
    "    Create a copy of the model with randomly initialized weights.\n",
    "    \n",
    "    Returns:\n",
    "        random_model: Model with random weights (same architecture)\n",
    "    \"\"\"\n",
    "    # TODO: Create a random model\n",
    "    # Hint: Use copy.deepcopy and reinitialize parameters\n",
    "    \n",
    "    random_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Reinitialize all parameters\n",
    "    for param in random_model.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        else:\n",
    "            nn.init.zeros_(param)\n",
    "    \n",
    "    return random_model\n",
    "\n",
    "\n",
    "def sanity_check_model_randomization(trained_model, input_text):\n",
    "    \"\"\"\n",
    "    Test: Do saliency maps change when we randomize the model?\n",
    "    \n",
    "    Returns:\n",
    "        correlation: Spearman correlation between trained and random saliency\n",
    "    \"\"\"\n",
    "    # TODO: Implement the sanity check\n",
    "    # 1. Compute saliency on trained model\n",
    "    # 2. Create random model and compute saliency\n",
    "    # 3. Compute correlation\n",
    "    # Expected: Low correlation (< 0.3) for good methods\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Trained model saliency\n",
    "    with torch.enable_grad():\n",
    "        trained_saliency = compute_saliency(trained_model, input_ids)\n",
    "    \n",
    "    # Random model saliency\n",
    "    random_model = randomize_model_weights(trained_model)\n",
    "    random_model.eval()\n",
    "    with torch.enable_grad():\n",
    "        random_saliency = compute_saliency(random_model, input_ids)\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation, p_value = spearmanr(trained_saliency, random_saliency)\n",
    "    \n",
    "    # Visualize\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    ax1.bar(range(len(tokens)), trained_saliency)\n",
    "    ax1.set_xticks(range(len(tokens)))\n",
    "    ax1.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax1.set_title('Trained Model Saliency')\n",
    "    ax1.set_ylabel('Gradient Magnitude')\n",
    "    \n",
    "    ax2.bar(range(len(tokens)), random_saliency)\n",
    "    ax2.set_xticks(range(len(tokens)))\n",
    "    ax2.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax2.set_title('Random Model Saliency')\n",
    "    ax2.set_ylabel('Gradient Magnitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Spearman correlation: {correlation:.3f} (p={p_value:.3f})\")\n",
    "    print(f\"{'PASS' if abs(correlation) < 0.3 else 'FAIL'}: Saliency should differ for random model\")\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"The capital of France is Paris, which is known for\"\n",
    "correlation = sanity_check_model_randomization(model, test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ROAR Benchmark (Hooker et al., 2019)\n",
    "\n",
    "Implement RemOve And Retrain: test if removing \"important\" features hurts performance more than removing random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_important_tokens(model, tokenizer, text, method='gradient', top_k=3):\n",
    "    \"\"\"\n",
    "    Identify the most important tokens according to a method.\n",
    "    \n",
    "    Args:\n",
    "        method: 'gradient', 'attention', or 'random'\n",
    "        top_k: Number of tokens to identify\n",
    "    \n",
    "    Returns:\n",
    "        important_positions: Indices of important tokens\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    if method == 'gradient':\n",
    "        # TODO: Use gradient-based importance\n",
    "        with torch.enable_grad():\n",
    "            saliency = compute_saliency(model, input_ids)\n",
    "        important_positions = np.argsort(saliency)[-top_k:]\n",
    "        \n",
    "    elif method == 'attention':\n",
    "        # TODO: Use attention-based importance\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, output_attentions=True)\n",
    "            # Average attention across heads and layers\n",
    "            attentions = torch.stack(outputs.attentions)  # [n_layers, 1, n_heads, seq_len, seq_len]\n",
    "            avg_attention = attentions.mean(dim=(0, 2))  # [1, seq_len, seq_len]\n",
    "            # Sum attention received by each token\n",
    "            importance = avg_attention.sum(dim=1).squeeze(0)  # [seq_len]\n",
    "        important_positions = torch.argsort(importance)[-top_k:].cpu().numpy()\n",
    "        \n",
    "    elif method == 'random':\n",
    "        # Random baseline\n",
    "        seq_len = input_ids.shape[1]\n",
    "        important_positions = np.random.choice(seq_len, size=top_k, replace=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return important_positions\n",
    "\n",
    "\n",
    "def mask_tokens(text, positions, tokenizer):\n",
    "    \"\"\"\n",
    "    Mask tokens at specified positions.\n",
    "    \n",
    "    Returns:\n",
    "        masked_text: Text with tokens masked\n",
    "    \"\"\"\n",
    "    # TODO: Replace tokens at positions with [MASK] or remove them\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    for pos in sorted(positions, reverse=True):\n",
    "        if pos < len(tokens):\n",
    "            tokens[pos] = '[MASK]'\n",
    "    \n",
    "    # Reconstruct text (approximate)\n",
    "    masked_text = ' '.join(tokens).replace(' ##', '')\n",
    "    \n",
    "    return masked_text\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Compute average perplexity on a list of texts.\n",
    "    \n",
    "    Returns:\n",
    "        perplexity: Average perplexity\n",
    "    \"\"\"\n",
    "    # TODO: Compute perplexity\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            if input_ids.shape[1] < 2:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            total_loss += outputs.loss.item() * (input_ids.shape[1] - 1)\n",
    "            total_tokens += input_ids.shape[1] - 1\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def roar_test(model, tokenizer, test_texts, methods=['gradient', 'attention', 'random'], top_k=3):\n",
    "    \"\"\"\n",
    "    ROAR test: Compare performance degradation when removing important tokens.\n",
    "    \n",
    "    Returns:\n",
    "        results: Dict mapping method to perplexity increase\n",
    "    \"\"\"\n",
    "    # TODO: Implement ROAR test\n",
    "    # 1. Compute baseline perplexity on original texts\n",
    "    # 2. For each method:\n",
    "    #    a. Identify important tokens\n",
    "    #    b. Mask those tokens\n",
    "    #    c. Compute perplexity on masked texts\n",
    "    # 3. Compare degradation\n",
    "    # Expected: Better methods should cause larger degradation\n",
    "    \n",
    "    baseline_ppl = compute_perplexity(model, tokenizer, test_texts)\n",
    "    print(f\"Baseline perplexity: {baseline_ppl:.2f}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        masked_texts = []\n",
    "        \n",
    "        for text in test_texts:\n",
    "            positions = identify_important_tokens(model, tokenizer, text, method=method, top_k=top_k)\n",
    "            masked_text = mask_tokens(text, positions, tokenizer)\n",
    "            masked_texts.append(masked_text)\n",
    "        \n",
    "        method_ppl = compute_perplexity(model, tokenizer, masked_texts)\n",
    "        degradation = method_ppl - baseline_ppl\n",
    "        \n",
    "        results[method] = {\n",
    "            'perplexity': method_ppl,\n",
    "            'degradation': degradation\n",
    "        }\n",
    "        \n",
    "        print(f\"{method}: ppl={method_ppl:.2f}, degradation={degradation:.2f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    methods_list = list(results.keys())\n",
    "    degradations = [results[m]['degradation'] for m in methods_list]\n",
    "    \n",
    "    plt.bar(methods_list, degradations)\n",
    "    plt.ylabel('Perplexity Increase')\n",
    "    plt.title('ROAR Test: Performance Degradation by Method')\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'PASS' if results['gradient']['degradation'] > results['random']['degradation'] else 'FAIL'}: \"\n",
    "          f\"Gradient method should outperform random\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning models can be difficult to interpret.\",\n",
    "    \"Paris is the capital of France and a major European city.\"\n",
    "]\n",
    "\n",
    "results = roar_test(model, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Attention vs. Gradient Importance (Jain & Wallace, 2019)\n",
    "\n",
    "Test whether attention weights correlate with gradient-based feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_vs_gradient(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Compare attention weights with gradient-based importance.\n",
    "    \n",
    "    Returns:\n",
    "        correlation: Spearman correlation between the two methods\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison\n",
    "    # 1. Compute gradient importance\n",
    "    # 2. Compute attention importance (average over heads/layers)\n",
    "    # 3. Correlate the two\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Gradient importance\n",
    "    with torch.enable_grad():\n",
    "        gradient_importance = compute_saliency(model, input_ids)\n",
    "    \n",
    "    # Attention importance\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "        attentions = torch.stack(outputs.attentions)  # [n_layers, 1, n_heads, seq_len, seq_len]\n",
    "        # Average attention to each token across all queries, heads, and layers\n",
    "        attention_importance = attentions.mean(dim=(0, 2, 3)).squeeze(0).cpu().numpy()  # [seq_len]\n",
    "    \n",
    "    # Normalize both\n",
    "    gradient_importance = gradient_importance / gradient_importance.sum()\n",
    "    attention_importance = attention_importance / attention_importance.sum()\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation, p_value = spearmanr(gradient_importance, attention_importance)\n",
    "    \n",
    "    # Visualize\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    \n",
    "    axes[0].bar(range(len(tokens)), gradient_importance)\n",
    "    axes[0].set_title('Gradient-Based Importance')\n",
    "    axes[0].set_xticks(range(len(tokens)))\n",
    "    axes[0].set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    \n",
    "    axes[1].bar(range(len(tokens)), attention_importance)\n",
    "    axes[1].set_title('Attention-Based Importance')\n",
    "    axes[1].set_xticks(range(len(tokens)))\n",
    "    axes[1].set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    \n",
    "    axes[2].scatter(gradient_importance, attention_importance, alpha=0.6)\n",
    "    axes[2].set_xlabel('Gradient Importance')\n",
    "    axes[2].set_ylabel('Attention Importance')\n",
    "    axes[2].set_title(f'Correlation: {correlation:.3f} (p={p_value:.3f})')\n",
    "    axes[2].plot([0, max(gradient_importance.max(), attention_importance.max())], \n",
    "                 [0, max(gradient_importance.max(), attention_importance.max())], \n",
    "                 'r--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Spearman correlation: {correlation:.3f} (p={p_value:.3f})\")\n",
    "    print(f\"Interpretation: {'Weak' if abs(correlation) < 0.3 else 'Moderate' if abs(correlation) < 0.6 else 'Strong'} correlation\")\n",
    "    print(\"Low correlation suggests attention may not explain feature importance.\")\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "\n",
    "# Test on multiple examples\n",
    "test_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"She didn't want to go to the party because she was tired.\"\n",
    "]\n",
    "\n",
    "correlations = []\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    corr = compare_attention_vs_gradient(model, tokenizer, text)\n",
    "    correlations.append(corr)\n",
    "\n",
    "print(f\"\\nAverage correlation: {np.mean(correlations):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adversarial Robustness for SAE Features\n",
    "\n",
    "Test whether SAE feature activations are robust to small input perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sae_feature(model, tokenizer, concept_text, layer=6):\n",
    "    \"\"\"\n",
    "    Simulate an SAE feature by computing a direction that activates on concept examples.\n",
    "    (Simplified version for demonstration)\n",
    "    \n",
    "    Returns:\n",
    "        feature_direction: Direction in activation space [hidden_dim]\n",
    "    \"\"\"\n",
    "    # TODO: Extract activations and compute concept direction\n",
    "    input_ids = tokenizer.encode(concept_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "        activations = outputs.hidden_states[layer]  # [1, seq_len, hidden_dim]\n",
    "        # Use mean activation as feature direction\n",
    "        feature_direction = activations.mean(dim=1).squeeze(0)  # [hidden_dim]\n",
    "    \n",
    "    # Normalize\n",
    "    feature_direction = feature_direction / feature_direction.norm()\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "\n",
    "def compute_feature_activation(model, tokenizer, text, feature_direction, layer=6):\n",
    "    \"\"\"\n",
    "    Compute how much a text activates a feature.\n",
    "    \n",
    "    Returns:\n",
    "        activation: Scalar activation value\n",
    "    \"\"\"\n",
    "    # TODO: Project activations onto feature direction\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "        activations = outputs.hidden_states[layer].mean(dim=1).squeeze(0)  # [hidden_dim]\n",
    "    \n",
    "    # Project onto feature direction\n",
    "    activation = torch.dot(activations, feature_direction).item()\n",
    "    \n",
    "    return activation\n",
    "\n",
    "\n",
    "def adversarial_perturbation(model, tokenizer, text, feature_direction, layer=6, epsilon=0.01, steps=10):\n",
    "    \"\"\"\n",
    "    Find small perturbation to input embeddings that maximally changes feature activation.\n",
    "    \n",
    "    Returns:\n",
    "        perturbed_activation: Feature activation after perturbation\n",
    "        original_activation: Feature activation before perturbation\n",
    "        output_change: Change in model output (perplexity)\n",
    "    \"\"\"\n",
    "    # TODO: Implement adversarial perturbation\n",
    "    # 1. Compute original feature activation\n",
    "    # 2. Use gradient ascent to perturb input embeddings to minimize activation\n",
    "    # 3. Measure how much model output changes\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Original activation\n",
    "    original_activation = compute_feature_activation(model, tokenizer, text, feature_direction, layer)\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.transformer.wte(input_ids).detach()\n",
    "    embeddings.requires_grad_(True)\n",
    "    \n",
    "    # Gradient descent to minimize feature activation\n",
    "    for step in range(steps):\n",
    "        if embeddings.grad is not None:\n",
    "            embeddings.grad.zero_()\n",
    "        \n",
    "        outputs = model(inputs_embeds=embeddings, output_hidden_states=True)\n",
    "        activations = outputs.hidden_states[layer].mean(dim=1).squeeze(0)\n",
    "        \n",
    "        # Objective: minimize feature activation\n",
    "        feature_activation = torch.dot(activations, feature_direction)\n",
    "        \n",
    "        feature_activation.backward()\n",
    "        \n",
    "        # Update embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings -= epsilon * embeddings.grad.sign()\n",
    "            # Project back to epsilon ball\n",
    "            embeddings.requires_grad_(True)\n",
    "    \n",
    "    # Final perturbed activation\n",
    "    with torch.no_grad():\n",
    "        outputs_perturbed = model(inputs_embeds=embeddings, output_hidden_states=True)\n",
    "        activations_perturbed = outputs_perturbed.hidden_states[layer].mean(dim=1).squeeze(0)\n",
    "        perturbed_activation = torch.dot(activations_perturbed, feature_direction).item()\n",
    "        \n",
    "        # Measure output change (simplified: just look at logit change)\n",
    "        outputs_original = model(input_ids)\n",
    "        logit_diff = (outputs_perturbed.logits - outputs_original.logits).abs().mean().item()\n",
    "    \n",
    "    return perturbed_activation, original_activation, logit_diff\n",
    "\n",
    "\n",
    "def test_sae_robustness(model, tokenizer, concept_texts, test_texts, layer=6):\n",
    "    \"\"\"\n",
    "    Test SAE feature robustness to adversarial perturbations.\n",
    "    \n",
    "    Returns:\n",
    "        results: Dict with activation changes and output changes\n",
    "    \"\"\"\n",
    "    # TODO: Implement robustness test\n",
    "    # Expected: Large activation change with small output change = fragile feature\n",
    "    \n",
    "    # Learn feature from concept examples\n",
    "    print(\"Learning feature direction from concept examples...\")\n",
    "    feature_direction = simulate_sae_feature(model, tokenizer, \" \".join(concept_texts), layer)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in test_texts:\n",
    "        perturbed_act, original_act, output_change = adversarial_perturbation(\n",
    "            model, tokenizer, text, feature_direction, layer\n",
    "        )\n",
    "        \n",
    "        activation_change = abs(perturbed_act - original_act)\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'original_activation': original_act,\n",
    "            'perturbed_activation': perturbed_act,\n",
    "            'activation_change': activation_change,\n",
    "            'output_change': output_change\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nText: {text[:50]}...\")\n",
    "        print(f\"  Original activation: {original_act:.3f}\")\n",
    "        print(f\"  Perturbed activation: {perturbed_act:.3f}\")\n",
    "        print(f\"  Activation change: {activation_change:.3f}\")\n",
    "        print(f\"  Output change: {output_change:.3f}\")\n",
    "        print(f\"  Robustness: {'PASS' if activation_change < 0.5 * abs(original_act) else 'FAIL'}\")\n",
    "    \n",
    "    # Visualize\n",
    "    activation_changes = [r['activation_change'] for r in results]\n",
    "    output_changes = [r['output_change'] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(output_changes, activation_changes, alpha=0.6, s=100)\n",
    "    plt.xlabel('Output Change (Logit Difference)')\n",
    "    plt.ylabel('Feature Activation Change')\n",
    "    plt.title('SAE Feature Robustness Test')\n",
    "    plt.axhline(y=np.median(activation_changes), color='r', linestyle='--', \n",
    "                label=f'Median activation change: {np.median(activation_changes):.3f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSummary: Large activation changes with small output changes suggest fragile features.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test\n",
    "concept_texts = [\n",
    "    \"The Golden Gate Bridge is in San Francisco.\",\n",
    "    \"I visited the Golden Gate Bridge last summer.\"\n",
    "]\n",
    "\n",
    "test_texts = [\n",
    "    \"San Francisco is a beautiful city on the west coast.\",\n",
    "    \"The bridge connects the city to Marin County.\",\n",
    "    \"California has many famous landmarks and attractions.\"\n",
    "]\n",
    "\n",
    "results = test_sae_robustness(model, tokenizer, concept_texts, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multi-Method Validation\n",
    "\n",
    "Validate a concept using three independent methods and check for agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_concept_multimethod(model, tokenizer, concept_name, positive_examples, negative_examples, layer=6):\n",
    "    \"\"\"\n",
    "    Validate a concept using multiple methods:\n",
    "    1. Linear probe\n",
    "    2. Causal intervention (steering)\n",
    "    3. Feature attribution\n",
    "    \n",
    "    Returns:\n",
    "        agreement_score: How much methods agree (0-1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement multi-method validation\n",
    "    \n",
    "    print(f\"Validating concept: {concept_name}\")\n",
    "    print(f\"Positive examples: {len(positive_examples)}\")\n",
    "    print(f\"Negative examples: {len(negative_examples)}\")\n",
    "    \n",
    "    # Method 1: Linear Probe\n",
    "    print(\"\\nMethod 1: Linear Probe\")\n",
    "    # TODO: Train probe and compute accuracy\n",
    "    probe_accuracy = 0.75  # Placeholder\n",
    "    print(f\"  Probe accuracy: {probe_accuracy:.2%}\")\n",
    "    \n",
    "    # Method 2: Causal Steering\n",
    "    print(\"\\nMethod 2: Causal Steering\")\n",
    "    # TODO: Extract concept direction and test steering effectiveness\n",
    "    steering_effectiveness = 0.70  # Placeholder\n",
    "    print(f\"  Steering effectiveness: {steering_effectiveness:.2%}\")\n",
    "    \n",
    "    # Method 3: Feature Attribution\n",
    "    print(\"\\nMethod 3: Feature Attribution\")\n",
    "    # TODO: Identify features important for concept\n",
    "    attribution_consistency = 0.65  # Placeholder\n",
    "    print(f\"  Attribution consistency: {attribution_consistency:.2%}\")\n",
    "    \n",
    "    # Compute agreement\n",
    "    scores = [probe_accuracy, steering_effectiveness, attribution_consistency]\n",
    "    agreement_score = 1.0 - np.std(scores)  # Lower std = higher agreement\n",
    "    \n",
    "    print(f\"\\nAgreement score: {agreement_score:.3f}\")\n",
    "    print(f\"{'PASS' if agreement_score > 0.8 else 'WARNING'}: Methods should agree for robust concept\")\n",
    "    \n",
    "    # Visualize\n",
    "    methods = ['Probe', 'Steering', 'Attribution']\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(methods, scores)\n",
    "    plt.ylabel('Validation Score')\n",
    "    plt.title(f'Multi-Method Validation: {concept_name}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.axhline(y=np.mean(scores), color='r', linestyle='--', label=f'Mean: {np.mean(scores):.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return agreement_score\n",
    "\n",
    "\n",
    "# Test\n",
    "positive_examples = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A dog chased the ball.\"\n",
    "]\n",
    "negative_examples = [\n",
    "    \"The theory of relativity explains gravity.\",\n",
    "    \"Machine learning uses neural networks.\"\n",
    "]\n",
    "\n",
    "score = validate_concept_multimethod(\n",
    "    model, tokenizer, \"animals\", positive_examples, negative_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Circuit Faithfulness with Multiple Ablation Methods\n",
    "\n",
    "Test how circuit faithfulness scores change with different ablation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_attention_head(model, layer, head, method='zero'):\n",
    "    \"\"\"\n",
    "    Ablate a specific attention head using different methods.\n",
    "    \n",
    "    Args:\n",
    "        method: 'zero', 'mean', 'random'\n",
    "    \n",
    "    Returns:\n",
    "        hook: Function to register as forward hook\n",
    "    \"\"\"\n",
    "    # TODO: Implement different ablation methods\n",
    "    \n",
    "    def zero_ablation_hook(module, input, output):\n",
    "        # output is (batch, num_heads, seq_len, head_dim)\n",
    "        output[0][:, head, :, :] = 0\n",
    "        return output\n",
    "    \n",
    "    def mean_ablation_hook(module, input, output):\n",
    "        # Replace with mean across sequence\n",
    "        output[0][:, head, :, :] = output[0][:, head, :, :].mean(dim=1, keepdim=True)\n",
    "        return output\n",
    "    \n",
    "    def random_ablation_hook(module, input, output):\n",
    "        # Replace with random values from standard normal\n",
    "        output[0][:, head, :, :] = torch.randn_like(output[0][:, head, :, :])\n",
    "        return output\n",
    "    \n",
    "    if method == 'zero':\n",
    "        return zero_ablation_hook\n",
    "    elif method == 'mean':\n",
    "        return mean_ablation_hook\n",
    "    elif method == 'random':\n",
    "        return random_ablation_hook\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ablation method: {method}\")\n",
    "\n",
    "\n",
    "def test_circuit_faithfulness(model, tokenizer, text, layer, head, ablation_methods=['zero', 'mean', 'random']):\n",
    "    \"\"\"\n",
    "    Test how faithfulness scores depend on ablation method.\n",
    "    \n",
    "    Returns:\n",
    "        results: Dict mapping ablation method to faithfulness score\n",
    "    \"\"\"\n",
    "    # TODO: Compute faithfulness with different ablation methods\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Baseline (no ablation)\n",
    "    with torch.no_grad():\n",
    "        baseline_outputs = model(input_ids)\n",
    "        baseline_logits = baseline_outputs.logits[0, -1, :]\n",
    "        baseline_probs = F.softmax(baseline_logits, dim=-1)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in ablation_methods:\n",
    "        # Register hook\n",
    "        hook = ablate_attention_head(model, layer, head, method)\n",
    "        handle = model.transformer.h[layer].attn.register_forward_hook(hook)\n",
    "        \n",
    "        # Forward pass with ablation\n",
    "        with torch.no_grad():\n",
    "            ablated_outputs = model(input_ids)\n",
    "            ablated_logits = ablated_outputs.logits[0, -1, :]\n",
    "            ablated_probs = F.softmax(ablated_logits, dim=-1)\n",
    "        \n",
    "        # Remove hook\n",
    "        handle.remove()\n",
    "        \n",
    "        # Compute faithfulness (KL divergence)\n",
    "        kl_div = F.kl_div(\n",
    "            ablated_probs.log(), baseline_probs, reduction='sum'\n",
    "        ).item()\n",
    "        \n",
    "        results[method] = kl_div\n",
    "        \n",
    "        print(f\"{method} ablation: KL divergence = {kl_div:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    methods_list = list(results.keys())\n",
    "    kl_divs = list(results.values())\n",
    "    \n",
    "    plt.bar(methods_list, kl_divs)\n",
    "    plt.ylabel('KL Divergence (Faithfulness)')\n",
    "    plt.title(f'Circuit Faithfulness: Layer {layer}, Head {head}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check sensitivity\n",
    "    max_diff = max(kl_divs) - min(kl_divs)\n",
    "    print(f\"\\nMax difference: {max_diff:.4f}\")\n",
    "    print(f\"{'WARNING' if max_diff > 0.1 else 'OK'}: Large differences suggest conclusion depends on ablation choice\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"The capital of France is Paris, which is known for\"\n",
    "results = test_circuit_faithfulness(model, tokenizer, test_text, layer=6, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Red Team Your Project Concept\n",
    "\n",
    "Apply skeptical validation to your own project concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive validation suite for interpretability claims.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, concept_name):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concept_name = concept_name\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def run_all_checks(self, positive_examples, negative_examples, layer=6):\n",
    "        \"\"\"\n",
    "        Run complete validation suite.\n",
    "        \n",
    "        Returns:\n",
    "            report: Dict with all validation results\n",
    "        \"\"\"\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"Red Team Validation: {self.concept_name}\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # 1. Sanity checks\n",
    "        print(\"\\n[1/6] Sanity Checks...\")\n",
    "        sanity_pass = self.sanity_checks(positive_examples[0])\n",
    "        self.validation_results['sanity_checks'] = sanity_pass\n",
    "        \n",
    "        # 2. Multi-method agreement\n",
    "        print(\"\\n[2/6] Multi-Method Validation...\")\n",
    "        agreement = self.multimethod_validation(positive_examples, negative_examples, layer)\n",
    "        self.validation_results['method_agreement'] = agreement\n",
    "        \n",
    "        # 3. Robustness tests\n",
    "        print(\"\\n[3/6] Robustness Tests...\")\n",
    "        robustness = self.robustness_tests(positive_examples, layer)\n",
    "        self.validation_results['robustness'] = robustness\n",
    "        \n",
    "        # 4. Causal validation\n",
    "        print(\"\\n[4/6] Causal Validation...\")\n",
    "        causal_pass = self.causal_validation(positive_examples)\n",
    "        self.validation_results['causal_validation'] = causal_pass\n",
    "        \n",
    "        # 5. Baseline comparisons\n",
    "        print(\"\\n[5/6] Baseline Comparisons...\")\n",
    "        baseline_pass = self.baseline_comparisons(positive_examples, negative_examples)\n",
    "        self.validation_results['baseline_comparison'] = baseline_pass\n",
    "        \n",
    "        # 6. Alternative explanations\n",
    "        print(\"\\n[6/6] Testing Alternative Explanations...\")\n",
    "        alternatives = self.test_alternative_explanations(positive_examples)\n",
    "        self.validation_results['alternative_explanations'] = alternatives\n",
    "        \n",
    "        # Summary\n",
    "        self.print_summary()\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    def sanity_checks(self, example_text):\n",
    "        \"\"\"Check if interpretation changes for random model.\"\"\"\n",
    "        # TODO: Implement\n",
    "        print(\"  Testing on random model...\")\n",
    "        correlation = sanity_check_model_randomization(self.model, example_text)\n",
    "        passed = abs(correlation) < 0.3\n",
    "        print(f\"  Result: {'PASS' if passed else 'FAIL'}\")\n",
    "        return passed\n",
    "    \n",
    "    def multimethod_validation(self, positive_examples, negative_examples, layer):\n",
    "        \"\"\"Test agreement across methods.\"\"\"\n",
    "        # TODO: Implement\n",
    "        print(\"  Comparing probe, steering, and attribution...\")\n",
    "        agreement = validate_concept_multimethod(\n",
    "            self.model, self.tokenizer, self.concept_name,\n",
    "            positive_examples, negative_examples, layer\n",
    "        )\n",
    "        return agreement\n",
    "    \n",
    "    def robustness_tests(self, examples, layer):\n",
    "        \"\"\"Test adversarial robustness.\"\"\"\n",
    "        # TODO: Implement\n",
    "        print(\"  Testing adversarial robustness...\")\n",
    "        # Simplified: just return a score\n",
    "        return 0.75\n",
    "    \n",
    "    def causal_validation(self, examples):\n",
    "        \"\"\"Test if interventions have predicted effects.\"\"\"\n",
    "        # TODO: Implement\n",
    "        print(\"  Testing causal interventions...\")\n",
    "        return True\n",
    "    \n",
    "    def baseline_comparisons(self, positive_examples, negative_examples):\n",
    "        \"\"\"Compare against random and simple baselines.\"\"\"\n",
    "        # TODO: Implement ROAR-style test\n",
    "        print(\"  Comparing to random baseline...\")\n",
    "        return True\n",
    "    \n",
    "    def test_alternative_explanations(self, examples):\n",
    "        \"\"\"Check if simpler explanations fit the data.\"\"\"\n",
    "        # TODO: Test word frequency, recency, etc.\n",
    "        print(\"  Testing word frequency baseline...\")\n",
    "        print(\"  Testing positional bias...\")\n",
    "        return ['word_frequency', 'positional_bias']\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print validation summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"VALIDATION SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        checks = [\n",
    "            ('Sanity Checks', self.validation_results['sanity_checks']),\n",
    "            ('Method Agreement', self.validation_results['method_agreement'] > 0.8),\n",
    "            ('Robustness', self.validation_results['robustness'] > 0.7),\n",
    "            ('Causal Validation', self.validation_results['causal_validation']),\n",
    "            ('Baseline Comparison', self.validation_results['baseline_comparison']),\n",
    "        ]\n",
    "        \n",
    "        passed = sum(1 for _, result in checks if result)\n",
    "        total = len(checks)\n",
    "        \n",
    "        for check_name, result in checks:\n",
    "            status = '‚úì PASS' if result else '‚úó FAIL'\n",
    "            print(f\"  {check_name:.<40} {status}\")\n",
    "        \n",
    "        print(f\"\\nOverall: {passed}/{total} checks passed\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"\\nüéâ Concept validation looks strong!\")\n",
    "        elif passed >= total * 0.7:\n",
    "            print(\"\\n‚ö†Ô∏è  Concept validation is promising but needs improvement.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Concept validation has significant issues. Rethink your approach.\")\n",
    "        \n",
    "        print(f\"\\nAlternative explanations to rule out:\")\n",
    "        for alt in self.validation_results['alternative_explanations']:\n",
    "            print(f\"  - {alt}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "validator = ConceptValidator(model, tokenizer, \"YOUR_CONCEPT_NAME\")\n",
    "\n",
    "positive_examples = [\n",
    "    # TODO: Add your concept's positive examples\n",
    "    \"Example text with your concept\",\n",
    "    \"Another example with your concept\",\n",
    "]\n",
    "\n",
    "negative_examples = [\n",
    "    # TODO: Add negative examples\n",
    "    \"Example without your concept\",\n",
    "    \"Another example without your concept\",\n",
    "]\n",
    "\n",
    "results = validator.run_all_checks(positive_examples, negative_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Publication-Ready Validation Checklist\n",
    "\n",
    "Create a checklist for your paper's interpretability claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Checklist for Your Paper\n",
    "\n",
    "Before submitting, ensure you can answer YES to these questions:\n",
    "\n",
    "#### 1. Sanity Checks\n",
    "- [ ] Tested interpretation method on randomly initialized model\n",
    "- [ ] Tested interpretation method on model trained with random labels\n",
    "- [ ] Results differ significantly from random model (correlation < 0.3)\n",
    "\n",
    "#### 2. Method Validation\n",
    "- [ ] Used at least 3 independent interpretability methods\n",
    "- [ ] Methods show consistent results (agreement > 70%)\n",
    "- [ ] Reported where methods disagree and investigated why\n",
    "\n",
    "#### 3. Causal Validation\n",
    "- [ ] Performed causal interventions (steering, ablation, or editing)\n",
    "- [ ] Interventions had predicted effects on model behavior\n",
    "- [ ] Tested on held-out examples not used during interpretation discovery\n",
    "\n",
    "#### 4. Robustness\n",
    "- [ ] Tested robustness to input perturbations\n",
    "- [ ] Tested robustness to prompt variations\n",
    "- [ ] Tested across multiple model checkpoints or architectures\n",
    "\n",
    "#### 5. Baseline Comparisons\n",
    "- [ ] Compared against random baselines\n",
    "- [ ] Compared against simple heuristics (word frequency, position, etc.)\n",
    "- [ ] Results significantly outperform baselines (p < 0.05)\n",
    "\n",
    "#### 6. Alternative Explanations\n",
    "- [ ] Identified at least 3 alternative explanations for observations\n",
    "- [ ] Designed tests to distinguish between alternatives\n",
    "- [ ] Provided evidence ruling out simpler explanations\n",
    "\n",
    "#### 7. Quantitative Evaluation\n",
    "- [ ] Used quantitative metrics (not just visualization)\n",
    "- [ ] Reported confidence intervals or statistical significance\n",
    "- [ ] Tested on sufficient sample size (n > 30 for statistical tests)\n",
    "\n",
    "#### 8. Methodological Transparency\n",
    "- [ ] Specified all hyperparameters (layer, alpha, threshold, etc.)\n",
    "- [ ] Specified ablation method if using circuit analysis\n",
    "- [ ] Reported how many experiments were run (avoid p-hacking)\n",
    "- [ ] Included negative results or failure cases\n",
    "\n",
    "#### 9. Human Validation (if applicable)\n",
    "- [ ] Conducted human study with appropriate controls\n",
    "- [ ] Measured whether explanations help humans (not just make sense)\n",
    "- [ ] Tested for confirmation bias in human evaluations\n",
    "\n",
    "#### 10. Limitations\n",
    "- [ ] Included explicit limitations section\n",
    "- [ ] Discussed what interpretation cannot tell us\n",
    "- [ ] Acknowledged methodological dependencies\n",
    "- [ ] Suggested future work to address limitations\n",
    "\n",
    "**Score:** ___/10 categories completed\n",
    "\n",
    "**Recommendation:**\n",
    "- 9-10: Ready to submit\n",
    "- 7-8: Address gaps before submission\n",
    "- <7: Significant validation work needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've learned to:\n",
    "\n",
    "1. **Implement sanity checks** to detect when interpretations are model-independent\n",
    "2. **Use ROAR benchmarks** to quantitatively validate feature importance\n",
    "3. **Compare attention vs. gradient importance** and understand when they disagree\n",
    "4. **Test adversarial robustness** of learned features and concepts\n",
    "5. **Validate concepts with multiple methods** and check for agreement\n",
    "6. **Measure circuit faithfulness** with different ablation methods\n",
    "7. **Red team your own work** with comprehensive validation suite\n",
    "8. **Follow best practices** for publication-ready validation\n",
    "\n",
    "Remember: **Every interpretability claim is a hypothesis that needs rigorous testing.**\n",
    "\n",
    "The bar is high, but meeting it is what separates true mechanistic understanding from interpretability theater."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
