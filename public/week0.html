<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Week 0: Introduction & Course Overview</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      border-bottom: 3px solid #3498db;
      padding-bottom: 10px;
    }

    h2 {
      color: #34495e;
      margin-top: 30px;
      border-bottom: 2px solid #bdc3c7;
      padding-bottom: 5px;
    }

    h3 {
      color: #555;
      margin-top: 20px;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-box {
      background-color: #e8f4f8;
      border-left: 4px solid #3498db;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .discussion-box {
      background-color: #fff9e6;
      border-left: 4px solid #f39c12;
      padding: 15px;
      margin: 20px 0;
    }

    .assignment-box {
      background-color: #e8f8f5;
      border-left: 4px solid #27ae60;
      padding: 15px;
      margin: 20px 0;
    }

    ul {
      margin: 10px 0;
    }

    li {
      margin: 8px 0;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
    }

    .objectives {
      background-color: #f0f7ff;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }
  </style>
</head>

<body>
  <h1>Week 0: Introduction & Course Overview</h1>

  <div class="info-box">
    <p><strong>Start here:</strong> <a href="week0_welcome.html">Prologue: At the Edge of Understanding</a></p>
    <p>An essay on why understanding AI matters and what neural mechanics can teach us about the future of human knowledge.</p>
    <p><strong>Also see:</strong> <a href="week0_notes.html">Lecture Notes</a> | <a href="week0_research.html">Finding a Good Research Question</a></p>
  </div>

  <div class="objectives">
    <h3>Learning Objectives</h3>
    <ul>
      <li>Understand course structure, expectations, and grading</li>
      <li>Grasp basic neural network concepts (accessible to non-ML students)</li>
      <li>Appreciate the historical surprise and significance of in-context learning</li>
      <li>See concrete examples of interpretability discoveries from early work</li>
      <li>Understand function vector results at a high level (methods come later)</li>
      <li>Identify characteristics of successful interdisciplinary interpretability projects</li>
      <li>Form teams and propose a concept to study</li>
    </ul>
  </div>

  <h2>1. Course Logistics</h2>

  <h3>Course Overview</h3>
  <p>
    <strong>Goal:</strong> Study how large language models encode non-CS concepts through interpretability research,
    culminating in papers suitable for submission to NeurIPS or similar venues.
  </p>

  <p>
    <strong>Team Structure:</strong> Interdisciplinary teams of ~3 students:
  </p>
  <ul>
    <li><strong>Non-CS PhD student:</strong> Domain expertise in your field (biology, linguistics, psychology, physics,
      law, etc.)</li>
    <li><strong>CS/ML PhD student:</strong> Technical ML background, PyTorch experience</li>
    <li><strong>BauLab member:</strong> Expert in interpretability methods, will guide technical implementation</li>
  </ul>

  <h3>Course Structure</h3>
  <table>
    <tr>
      <th>Weeks</th>
      <th>Focus</th>
    </tr>
    <tr>
      <td>0-1</td>
      <td>Foundations: Course setup, benchmarking concepts</td>
    </tr>
    <tr>
      <td>2-8</td>
      <td>Methods: Core interpretability techniques (steering, circuits, probes, SAEs, validation)</td>
    </tr>
    <tr>
      <td>9-11</td>
      <td>Advanced topics: Attribution, skepticism, and research best practices</td>
    </tr>
    <tr>
      <td>12</td>
      <td>Final presentations</td>
    </tr>
  </table>

  <h3>Grading & Expectations</h3>
  <div class="info-box">
    <p><strong>This is a research course, not a traditional class.</strong></p>
    <p>Grading will be based on:</p>
    <ul>
      <li><strong>Weekly exercises:</strong> Hands-on implementation of interpretability methods (30%)</li>
      <li><strong>Project milestones:</strong> Proposal, preliminary results, draft figures (30%)</li>
      <li><strong>Final deliverable:</strong> Research paper draft + presentation (40%)</li>
    </ul>
    <p><em>Success = rigorous investigation of your concept, not necessarily positive results. Negative results with
        careful validation are publishable!</em></p>
  </div>

  <h2>2. The Central Mystery: Are Concepts Different from Words?</h2>

  <p>
    Large language models process text, but do they think in words? This course begins with a provocation: the internal representations that drive model behavior may be fundamentally different from the tokens that flow in and out.
  </p>

  <p>
    Words are surface phenomena. They vary across languages, shift meaning with context, and often fail to capture the abstractions that underlie coherent reasoning. <strong>Concepts</strong>, by contrast, are the invariants&mdash;the stable structures that persist across linguistic transformations. This course asks: can we find these invariants inside neural networks?
  </p>

  <h3>The Gap Between Knowledge and Expression</h3>
  <p>
    What an AI knows is not always what it says. This gap between internal representation and external behavior has become starkly visible in recent work.
  </p>

  <p>
    <strong>Rager et al. (2025)</strong> study DeepSeek models, revealing a striking case of censorship mechanics. The models demonstrably possess knowledge about sensitive topics&mdash;their internal representations encode the relevant concepts&mdash;yet they refuse to express this knowledge in their outputs. The information exists inside the model; the suppression is a separate mechanism layered on top.
  </p>

  <p>
    This dissociation between knowing and saying points to a general phenomenon: models may have internal states that diverge systematically from their outputs. If we want to understand what models actually believe, know, or intend, we cannot rely solely on their words. <strong>We must look inside.</strong>
  </p>

  <h3>Concepts as Invariants</h3>
  <p>
    The emerging picture suggests that concepts are invariants of the system&mdash;internal structures whose functional roles remain unchanged under many transformations. Just as a physical law remains valid regardless of the coordinate system used to express it, a neural concept may persist across languages, phrasings, and contexts.
  </p>

  <p>Examples of concepts that appear to have this invariant character:</p>
  <ul>
    <li><strong>Truthfulness concepts:</strong> Directions in activation space corresponding to whether a statement is true or false, independent of topic or phrasing</li>
    <li><strong>Task concepts:</strong> The abstract notion of "translate" or "summarize" as distinct from any particular instance</li>
    <li><strong>User concepts:</strong> Representations of who the model is talking to, tracking properties across a conversation</li>
    <li><strong>Domain concepts:</strong> Abstract structures from law, medicine, physics, music&mdash;the focus of this course</li>
  </ul>

  <p>
    These are not isolated neurons but distributed patterns of activation. They are not explicitly labeled in the training data but emerge from the structure of the task. And they are not words&mdash;they are what the words point to.
  </p>

  <h2>3. Causal Mediation: Finding the Neurons That Matter</h2>

  <p>
    How do we identify which internal components are responsible for specific behaviors? One methodology is <strong>causal mediation analysis</strong>: intervening on internal components to establish their causal role in the model's outputs.
  </p>

  <h3>Early Success: The Lamp-Controlling Neuron</h3>
  <p>
    Early work on GANs (Generative Adversarial Networks) provides an instructive template. A StyleGAN trained to generate bedrooms&mdash;with no supervision beyond imitation of a few million bedroom images&mdash;revealed striking internal structure.
  </p>

  <div class="example-box">
    <p><strong>Experiment (Bau et al., 2019):</strong></p>
    <ol>
      <li>Generate one image that has a lamp, using a fixed random seed z, and manually mark the pixels illuminated by the lamp light</li>
      <li>For each of the 9,000 style neurons, regenerate the same image (same seed z) with that neuron removed (set to zero), and rank neurons by their causal effect on the marked pixels</li>
      <li>Test the highest-ranked neuron for its effect on other bedroom images</li>
    </ol>
    <p><strong>Result:</strong> A single neuron turns lights on and off across many different images.</p>
  </div>

  <p>
    This is surprising because there is nothing in the training data explicitly about switching lights on and off. The training set contains only static bedroom images&mdash;no before/after pairs, no labels about lighting. Yet "switching off the lights" emerges as a concept that the neural network learned from the training process.
  </p>

  <p>
    <strong>This leads to the central question of this course:</strong> When we train on much more complex data&mdash;text spanning all of human knowledge&mdash;what concepts do neural networks learn?
  </p>

  <h2>4. In-Context Learning and Function Vectors</h2>

  <h3>In-Context Learning: A Form of Metareasoning</h3>
  <p>
    In 2020, GPT-3 demonstrated <strong>in-context learning</strong> (ICL): the ability to learn
    new tasks from a few examples in the prompt, <em>without any gradient updates</em>.
  </p>

  <div class="example-box">
    <p><strong>Example: Translation without training</strong></p>
    <p>
      <strong>Prompt:</strong><br>
      English: Hello &rarr; French: Bonjour<br>
      English: Goodbye &rarr; French: Au revoir<br>
      English: Thank you &rarr; French: <em>[model generates: Merci]</em>
    </p>
  </div>

  <p>
    This is a form of <strong>metareasoning</strong>&mdash;the model is not just processing text but reasoning about how to reason. Where does this capability come from? It is not explicitly programmed. It emerges from training on next-token prediction, yet manifests as something that looks remarkably like flexible cognition.
  </p>

  <h3>Function Vectors: Portable Neural Representations of Tasks</h3>
  <p>
    Where is in-context learning encoded inside the model? Research by <strong>Todd et al. (2024)</strong> reveals a striking answer: when a model performs ICL, it develops compact vector representations of the task itself&mdash;<em>function vectors</em>&mdash;that can be extracted and transplanted to new contexts.
  </p>

  <div class="example-box">
    <p><strong>Experiment (Todd et al., 2024):</strong></p>
    <ol>
      <li>Give the model in-context examples demonstrating a function (e.g., antonyms: "arrive:depart, small:big, hot:cold")</li>
      <li>Extract the activation pattern from attention head outputs&mdash;this is the "function vector" for antonyms</li>
      <li>Insert this vector into a completely different context: "The word 'fast' means"</li>
    </ol>
    <p><strong>Result:</strong> The model outputs "slow"&mdash;the antonym&mdash;even though no antonym examples appeared in this new prompt.</p>
  </div>

  <p>
    This is directly analogous to the lamp-controlling neuron, but for abstract functions rather than visual features. Just as the lamp neuron's causal effect transfers across different bedroom images, function vectors transfer across completely different textual contexts. The neural representation learned in one setting causes the same function to execute in another.
  </p>

  <div class="info-box">
    <p><strong>A Key Finding:</strong> Function vectors work across languages. An antonym function vector extracted from English examples can induce antonym behavior on French or German inputs. The internal representation of "find the opposite" is not tied to any particular language&mdash;it is an abstract structure that transcends the tokens.</p>
  </div>

  <p>
    This reinforces the central thesis: concepts inside LLMs are not words. They are abstract structures&mdash;stable under linguistic transformations&mdash;that we can study, characterize, and manipulate.
  </p>

  <h3>Key Research Threads in LLMs</h3>
  <p>
    The methodology of causal mediation generalizes from GANs to language models. Three lines of work illustrate the progress that has been made in localizing and characterizing neural concepts:
  </p>

  <p>
    <strong>Locating Factual Knowledge (Meng et al., 2022):</strong> The ROME paper demonstrated that factual associations in GPT-style models are localized in specific MLP layers. By applying <em>causal tracing</em>&mdash;corrupting inputs and restoring activations at targeted locations&mdash;researchers identified where facts like "The Eiffel Tower is in Paris" are stored. This established that factual knowledge has a discernible address inside the network.
  </p>

  <p>
    <strong>Locating Functions (Todd et al., 2024):</strong> This work extended localization from facts to functions. When a model performs in-context learning of an abstract task&mdash;translating to French, answering with antonyms, continuing a pattern&mdash;where is that function encoded? <em>Function vectors</em> show that task-level abstractions are localizable and, remarkably, transferable: a vector extracted from one context can induce the same function in another.
  </p>

  <p>
    <strong>Locating Mental State Representations (Prakash et al., 2025):</strong> This work examined how models represent and track the mental states of agents described in text&mdash;theory of mind. It shows that interpretability methods can reach beyond simple factual recall into the representation of genuinely abstract, cognitively rich concepts.
  </p>

  <div class="info-box">
    <p><strong>The Pattern:</strong> From facts to functions to mental states&mdash;each study finds that abstract concepts have neural correlates that can be identified, validated, and manipulated. The methods you will learn in this course are the tools for this kind of discovery.</p>
  </div>

  <h2>5. Toward an Atlas of Neural Concepts</h2>

  <p>
    If concepts inside LLMs are real, discoverable, and causally important, then we should map them systematically. The vision motivating this course is the creation of an <strong>atlas of neural concepts</strong>: a comprehensive characterization of the internal structures that drive model behavior across domains.
  </p>

  <h3>Why Build an Atlas?</h3>
  <p>
    Such an atlas would serve multiple purposes:
  </p>
  <ul>
    <li><strong>Scientific understanding:</strong> What are the basic building blocks of machine cognition?</li>
    <li><strong>Safety and alignment:</strong> Which concepts relate to honesty, deception, harm, and helpfulness?</li>
    <li><strong>Knowledge extraction:</strong> What has the model learned that humans do not yet know?</li>
    <li><strong>Control and steering:</strong> How can we intervene on concepts to shape behavior?</li>
  </ul>

  <p>
    Building this atlas requires methods. The works surveyed above&mdash;and the techniques you will learn throughout this course&mdash;constitute the toolkit for this cartographic project.
  </p>

  <h3>What Should the Atlas Contain?</h3>
  <p>
    This is an open question. Possibilities include:
  </p>
  <ul>
    <li><strong>Individual neurons:</strong> The simplest units, but concepts are rarely confined to single neurons</li>
    <li><strong>Directions in activation space:</strong> Linear combinations of neurons that represent concepts</li>
    <li><strong>Circuits:</strong> Collections of components that work together to implement functions</li>
    <li><strong>Functional roles:</strong> Abstract descriptions of what computations are performed where</li>
  </ul>

  <p>
    The right level of description likely depends on the question being asked. Your projects will contribute to answering this.
  </p>

  <div class="discussion-box">
    <h3>Discussion Questions</h3>
    <ol>
      <li><strong>What do interpretability methods have in common?</strong> Causal tracing, function vectors, circuit analysis&mdash;each uses different techniques. What makes something a "mechanistic interpretability" approach?</li>
      <li><strong>How do we know when we have found a concept?</strong> What criteria distinguish a genuine neural concept from a spurious correlation or an artifact of our analysis methods?</li>
      <li><strong>What concepts matter most?</strong> Given limited research resources, which concepts should we prioritize? What makes a concept important from a scientific perspective? From a safety perspective?</li>
      <li><strong>Can models describe their own concepts?</strong> If concepts are not words, can language models ever articulate what they internally represent? Or is there a fundamental gap between neural representation and linguistic expression?</li>
    </ol>
  </div>

  <h2>6. What Makes a Good Research Question?</h2>

  <p>
    Before you invest months of effort into a research project, you should ask: is this the right question? The FINER framework&mdash;adapted from clinical research methodology&mdash;offers useful criteria: a good research question should be <strong>Feasible, Interesting, Novel, Ethical, and Relevant</strong>.
  </p>

  <p>For a detailed discussion, see <a href="week0_research.html">Finding a Good Research Question</a>.</p>

  <h3>The FINER Framework for Interpretability</h3>

  <table>
    <tr>
      <th>Criterion</th>
      <th>Key Questions</th>
    </tr>
    <tr>
      <td><strong>Feasible</strong></td>
      <td>Can you find a model where the phenomenon appears and where you can see inside? Are there signs of life in the internal representations?</td>
    </tr>
    <tr>
      <td><strong>Interesting</strong></td>
      <td>Does this question excite you? Would your colleagues (in ML and in your domain) consider it important?</td>
    </tr>
    <tr>
      <td><strong>Novel</strong></td>
      <td>Has this been done before? Domain-specific concepts are vastly understudied&mdash;this is your opportunity.</td>
    </tr>
    <tr>
      <td><strong>Ethical</strong></td>
      <td>Would this work primarily enable harm? Interpretability tools can be misused.</td>
    </tr>
    <tr>
      <td><strong>Relevant</strong></td>
      <td>If you answer this question, what changes for the field? For the domains where models are deployed?</td>
    </tr>
  </table>

  <h3>The Interdisciplinary Advantage</h3>

  <p>
    LLMs are being deployed in medicine, law, education, scientific research, policy analysis. The highest-stakes applications are almost entirely outside computer science. Yet interpretability research remains focused on questions that matter primarily to ML researchers.
  </p>

  <p>
    This is the gap. This course is forming interdisciplinary teams precisely to address it. The interpretability community has strong intuitions about which questions matter for AI safety and for understanding deep learning. But we have weak intuitions about which questions matter for medicine, for law, for scientific discovery. <strong>Your non-CS collaborators have those intuitions.</strong>
  </p>

  <div class="info-box">
    <p>When the legal scholar on your team gets excited about a research direction, that signal contains information you could not generate yourself. When the biologist identifies a concept that would matter for scientific discovery, that is the kind of question no one else is asking.</p>
  </div>

  <h3>Example Project Ideas</h3>

  <div class="example-box">
    <p><strong>Biology:</strong> Evolutionary relationships, protein structure encoding, ecological relationships</p>
    <p><strong>Linguistics:</strong> Politeness strategies, evidentiality, discourse structure</p>
    <p><strong>Psychology:</strong> Theory of mind, temporal reasoning, causal attribution</p>
    <p><strong>Physics/Chemistry:</strong> Conservation laws, reaction mechanisms, spatial reasoning</p>
    <p><strong>Law:</strong> Legal precedent, burden of proof, jurisdiction</p>
    <p><strong>Music:</strong> Harmonic relationships, rhythmic patterns, musical form</p>
  </div>

  <h3>Group Discussion</h3>

  <div class="discussion-box">
    <p><strong>In your teams, discuss:</strong></p>
    <ol>
      <li>What is a fundamental concept in your domain that non-experts often misunderstand?</li>
      <li>How would you test if an LLM understands this concept?</li>
      <li>What would it mean to "localize" this concept in a neural network?</li>
      <li>What failure modes might be interesting? (When does the model get it wrong?)</li>
      <li>How could this research benefit your field or AI safety?</li>
    </ol>
  </div>

  <h2>7. Assignment: Initial Pitch (Due Week 1)</h2>

  <div class="assignment-box">
    <p><strong>Part 1: Form Teams</strong></p>
    <ul>
      <li>Groups of ~3 students (ideally: 1 non-CS PhD, 1 CS/ML PhD, 1 Bau Lab member)</li>
      <li>Use class Slack to coordinate team formation</li>
      <li>Create a shared Google Drive folder for your team's project materials</li>
    </ul>

    <p><strong>Part 2: Write Your Pitch Document (1-2 pages)</strong></p>
    <p>Create a Google Doc in your team's Drive folder. Your pitch should include:</p>
    <ul>
      <li><strong>Team members:</strong> Names, departments, relevant expertise</li>
      <li><strong>Concept description:</strong> What concept will you study? Why is it important in your domain? Why is it interesting for LLM interpretability?</li>
      <li><strong>Initial questions:</strong> 2-3 specific questions you want to investigate</li>
      <li><strong>Example prompts:</strong> A few example sentences where the concept is present vs. absent</li>
    </ul>

    <p><strong>Part 3: Prepare Your Elevator Pitch</strong></p>
    <p>Prepare a 5-minute presentation of your idea. In Week 1, each team will pitch to the class, and we will discuss each proposal using the FINER framework.</p>

    <p><strong>Submission:</strong> Upload your pitch document to your team Google Drive before the Thursday class (Jan 15). We will provide feedback during your presentation.</p>
  </div>

  <h2>8. Looking Ahead</h2>

  <p><strong>Week 1:</strong> Foundations—logit lens, intermediate representations, and the vocabulary of mechanistic interpretability</p>
  <p><strong>Week 2:</strong> Steering—controlling model behavior through activation addition and representation engineering</p>
  <p><strong>Week 3:</strong> Evaluation Methodology—measuring and evaluating LLM behavior</p>
  <p><strong>Week 4:</strong> Representation Geometry—PCA visualization, linear directions, and geometric structure</p>
  <p><strong>Weeks 5-8:</strong> Advanced methods (causal localization, probes, attribution, circuits)</p>
  <p><strong>Weeks 9-10:</strong> Training dynamics, model editing, and self-description</p>
  <p><strong>Weeks 11-13:</strong> Paper writing workshops and final presentations</p>

  <p>
    <strong>By the end:</strong> You'll have a complete research pipeline for characterizing YOUR concept in LLMs, culminating in a paper suitable for submission to a top venue.
  </p>

  <div class="info-box">
    <h3>Questions?</h3>
    <p>Reach out to the teaching team:</p>
    <ul>
      <li>Prof. David Bau: <a href="mailto:davidbau@northeastern.edu">davidbau@northeastern.edu</a></li>
      <li>Nikhil Prakash (TA): <a href="mailto:prakash.nik@northeastern.edu">prakash.nik@northeastern.edu</a></li>
    </ul>
    <p>Office hours: [See course website]</p>
  </div>

  <h2>References & Further Reading</h2>

  <h3>On Finding Good Research Questions</h3>
  <ul>
    <li>Hamming, R. W. (1986). <a href="papers/hamming-1986-you-and-your-research.pdf">"You and Your Research"</a>. Bell Communications Research colloquium. <em>The classic essay on choosing important problems.</em></li>
    <li>Nielsen, M. A. (2004). <a href="papers/nielsen-2004-principles-of-effective-research.pdf">"Principles of Effective Research"</a>. University of Queensland Technical Note. <em>On problem-solvers vs. problem-creators.</em></li>
    <li>Cummings, S. R., Browner, W. S., &amp; Hulley, S. B. (2013). <a href="papers/cummings-2013-conceiving-the-research-question.pdf">"Conceiving the Research Question"</a>. In <em>Designing Clinical Research</em>. <em>The FINER framework.</em></li>
    <li>Bau, D. (2024). <a href="https://davidbau.com/archives/2024/12/05/in_defense_of_curiosity.html">"In Defense of Curiosity"</a>. <em>On Venetian glassmaking, the value of basic scientific research, and understanding for its own sake.</em></li>
  </ul>

  <h3>Interpretability Papers Mentioned</h3>
  <ul>
    <li>Meng et al. (2022). <a href="papers/meng-2022-rome-2202.05262.pdf">"Locating and Editing Factual Associations in GPT"</a> (ROME)</li>
    <li>Todd et al. (2024). <a href="papers/todd-2023-function-vectors-2310.15213.pdf">"Function Vectors in Large Language Models"</a></li>
    <li>Prakash et al. (2025). <a href="papers/prakash-2024-entity-tracking-2402.14811.pdf">"Fine-tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking"</a></li>
    <li>Olsson et al. (2022). <a href="papers/olsson-2022-induction-heads-2209.11895.pdf">"In-context Learning and Induction Heads"</a></li>
    <li>Bau et al. (2019). <a href="papers/bau-2018-gan-dissection-1811.10597.pdf">"GAN Dissection"</a> | <a href="https://gandissect.csail.mit.edu/" target="_blank">Interactive demo</a></li>
  </ul>

  <h3>Accessible Introductions</h3>
  <ul>
    <li><a href="https://transformer-circuits.pub/" target="_blank">Transformer Circuits Thread</a> (Anthropic) —
      Visual explanations of interpretability concepts</li>
    <li><a href="https://distill.pub/" target="_blank">Distill.pub</a> — Interactive ML explanations (now archived but
      excellent)</li>
    <li><a href="https://pair.withgoogle.com/explorables/" target="_blank">Google PAIR Explorables</a> — Interactive ML
      concept demos</li>
  </ul>

  <h3>Supplementary Reading</h3>
  <ul>
    <li>Olah et al. (2020). "Zoom In: An Introduction to Circuits". <a href="https://distill.pub/2020/circuits/zoom-in/"
        target="_blank">Distill</a></li>
    <li>Elhage et al. (2021). "A Mathematical Framework for Transformer Circuits". <a
        href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">Transformer Circuits</a></li>
  </ul>

  <h2>Project Milestone</h2>
  <div class="assignment-box">
    <p><strong>This Week: Form Teams and Begin Your Pitch</strong></p>
    <p>
      Form interdisciplinary teams of approximately 3 members: one non-CS PhD student (bringing domain expertise),
      one CS/ML PhD student (bringing technical ML background), and one Bau Lab member (bringing interpretability expertise).
    </p>
    <p>
      Begin brainstorming concepts from your domain that might be interesting to study in LLMs. The goal is to identify
      2-3 candidate concepts that are:
    </p>
    <ul>
      <li><strong>Specific enough</strong> to operationalize (not too abstract)</li>
      <li><strong>Interesting</strong> to your domain (relevant to real research questions)</li>
      <li><strong>Testable</strong> in language models (can be expressed through text)</li>
      <li><strong>Novel</strong> (ideally not already thoroughly studied in interpretability literature)</li>
    </ul>

    <h4>Getting Started:</h4>
    <ul>
      <li>Create a team Google Drive folder for your project</li>
      <li>Start drafting your initial pitch document (1-2 pages) as a Google Doc</li>
      <li>Prepare a 5-minute "elevator pitch" presentation for Week 1</li>
    </ul>

    <p><em>
      The pitch document and presentation are due Thursday of Week 1. Use this week to form your team,
      explore ideas, and begin writing.
    </em></p>
  </div>

</body>

</html>