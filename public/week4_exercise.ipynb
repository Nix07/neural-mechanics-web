{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Exercise: Causal Mediation Analysis\n",
    "\n",
    "In this exercise, you'll gain hands-on experience with:\n",
    "- Activation patching (noise and clean)\n",
    "- Computing causal effects\n",
    "- ROME-style causal tracing\n",
    "- Gradient-based attribution\n",
    "- Average Indirect Effect (AIE)\n",
    "- Function vector extraction\n",
    "- Counterfactual dataset design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Activation Patching\n",
    "\n",
    "Let's implement the core patching functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_cache(model, prompt, layer_idx=None):\n",
    "    \"\"\"\n",
    "    Run model and cache activations.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        prompt: Input text\n",
    "        layer_idx: If specified, only cache this layer\n",
    "    \n",
    "    Returns:\n",
    "        logits: Model output logits\n",
    "        cache: Dict of cached activations\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    cache = {}\n",
    "    \n",
    "    # Hook to save activations\n",
    "    def save_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                cache[name] = output[0].detach()\n",
    "            else:\n",
    "                cache[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    handles = []\n",
    "    if layer_idx is None:\n",
    "        # Cache all layers\n",
    "        for i, layer in enumerate(model.transformer.h):\n",
    "            handle = layer.register_forward_hook(save_activation(f\"layer_{i}\"))\n",
    "            handles.append(handle)\n",
    "    else:\n",
    "        # Cache specific layer\n",
    "        handle = model.transformer.h[layer_idx].register_forward_hook(\n",
    "            save_activation(f\"layer_{layer_idx}\")\n",
    "        )\n",
    "        handles.append(handle)\n",
    "    \n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return outputs.logits, cache\n",
    "\n",
    "# Test\n",
    "logits, cache = run_with_cache(model, \"The capital of France is\", layer_idx=6)\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Cached activations: {list(cache.keys())}\")\n",
    "print(f\"Layer 6 activation shape: {cache['layer_6'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_patch(model, prompt, patch_layer, patch_activations, patch_position=None):\n",
    "    \"\"\"\n",
    "    Run model with patched activations.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        prompt: Input text (corrupted)\n",
    "        patch_layer: Which layer to patch\n",
    "        patch_activations: Activations to patch in (from clean run)\n",
    "        patch_position: If specified, only patch this token position\n",
    "    \n",
    "    Returns:\n",
    "        logits: Model output with patching applied\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Hook to patch activations\n",
    "    def patch_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        \n",
    "        # Patch activations\n",
    "        if patch_position is None:\n",
    "            # Patch all positions\n",
    "            hidden_states[:] = patch_activations\n",
    "        else:\n",
    "            # Patch specific position\n",
    "            hidden_states[:, patch_position, :] = patch_activations[:, patch_position, :]\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "    \n",
    "    # Register hook on target layer\n",
    "    handle = model.transformer.h[patch_layer].register_forward_hook(patch_hook)\n",
    "    \n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Clean up\n",
    "    handle.remove()\n",
    "    \n",
    "    return outputs.logits\n",
    "\n",
    "# Test patching\n",
    "clean_prompt = \"The capital of France is\"\n",
    "corrupted_prompt = \"The capital of Germany is\"\n",
    "\n",
    "# Get clean activations\n",
    "_, clean_cache = run_with_cache(model, clean_prompt, layer_idx=6)\n",
    "\n",
    "# Patch them into corrupted run\n",
    "patched_logits = run_with_patch(\n",
    "    model, \n",
    "    corrupted_prompt, \n",
    "    patch_layer=6, \n",
    "    patch_activations=clean_cache['layer_6']\n",
    ")\n",
    "\n",
    "print(\"Patching test successful!\")\n",
    "print(f\"Patched output shape: {patched_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Causal Effect of Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prediction(logits, top_k=5):\n",
    "    \"\"\"Get top k predictions from logits.\"\"\"\n",
    "    probs = torch.softmax(logits[0, -1, :], dim=0)\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        results.append((token, prob.item()))\n",
    "    return results\n",
    "\n",
    "# Compare outputs\n",
    "clean_logits, _ = run_with_cache(model, clean_prompt)\n",
    "corrupted_logits, _ = run_with_cache(model, corrupted_prompt)\n",
    "patched_logits = run_with_patch(model, corrupted_prompt, 6, clean_cache['layer_6'])\n",
    "\n",
    "print(\"Clean prompt: 'The capital of France is'\")\n",
    "print(\"Top predictions:\")\n",
    "for token, prob in get_top_prediction(clean_logits):\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nCorrupted prompt: 'The capital of Germany is'\")\n",
    "print(\"Top predictions:\")\n",
    "for token, prob in get_top_prediction(corrupted_logits):\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nPatched (Germany prompt + France layer 6):\")\n",
    "print(\"Top predictions:\")\n",
    "for token, prob in get_top_prediction(patched_logits):\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does patching layer 6 transfer the clean answer? What does this tell you about layer 6's role?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Noise vs Clean Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_patch(model, prompt, patch_layer, noise_scale=1.0):\n",
    "    \"\"\"\n",
    "    Patch with random noise (ablation).\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    def noise_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(hidden_states) * noise_scale\n",
    "        hidden_states = hidden_states + noise\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "    \n",
    "    handle = model.transformer.h[patch_layer].register_forward_hook(noise_hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    handle.remove()\n",
    "    return outputs.logits\n",
    "\n",
    "# Compare noise vs clean patching\n",
    "original_logits, _ = run_with_cache(model, clean_prompt)\n",
    "noise_logits = noise_patch(model, clean_prompt, patch_layer=6, noise_scale=0.1)\n",
    "\n",
    "print(\"Original (no patching):\")\n",
    "for token, prob in get_top_prediction(original_logits)[:3]:\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nNoise patched (layer 6):\")\n",
    "for token, prob in get_top_prediction(noise_logits)[:3]:\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Compare Strategies\n",
    "\n",
    "Test both noise and clean patching across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_patching_strategies(clean_prompt, corrupted_prompt, target_token):\n",
    "    \"\"\"\n",
    "    Compare noise vs clean patching across layers.\n",
    "    \"\"\"\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    results = {'layer': [], 'noise': [], 'clean': []}\n",
    "    \n",
    "    for layer in range(model.config.n_layer):\n",
    "        # Clean activations\n",
    "        _, clean_cache = run_with_cache(model, clean_prompt, layer_idx=layer)\n",
    "        \n",
    "        # Noise patching\n",
    "        noise_logits = noise_patch(model, corrupted_prompt, layer, noise_scale=0.1)\n",
    "        noise_prob = torch.softmax(noise_logits[0, -1, :], dim=0)[target_id].item()\n",
    "        \n",
    "        # Clean patching\n",
    "        clean_patched_logits = run_with_patch(\n",
    "            model, corrupted_prompt, layer, clean_cache[f'layer_{layer}']\n",
    "        )\n",
    "        clean_prob = torch.softmax(clean_patched_logits[0, -1, :], dim=0)[target_id].item()\n",
    "        \n",
    "        results['layer'].append(layer)\n",
    "        results['noise'].append(noise_prob)\n",
    "        results['clean'].append(clean_prob)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "results = compare_patching_strategies(\n",
    "    \"The capital of France is\",\n",
    "    \"The capital of Germany is\",\n",
    "    \" Paris\"\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(results['layer'], results['noise'], marker='o', label='Noise Patching')\n",
    "plt.plot(results['layer'], results['clean'], marker='s', label='Clean Patching')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('P(\"Paris\")')\n",
    "plt.title('Patching Effect Across Layers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which layers show the strongest clean patching effect? What does this suggest about where information is processed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computing Causal Effects\n",
    "\n",
    "Let's formalize our measurements as causal effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ace(model, prompt, patch_layer, clean_activations, target_token):\n",
    "    \"\"\"\n",
    "    Compute Average Causal Effect of patching a layer.\n",
    "    \n",
    "    ACE = E[Y | do(X=clean)] - E[Y | do(X=corrupted)]\n",
    "    \"\"\"\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Corrupted (no patch)\n",
    "    corrupted_logits, _ = run_with_cache(model, prompt)\n",
    "    p_corrupted = torch.softmax(corrupted_logits[0, -1, :], dim=0)[target_id].item()\n",
    "    \n",
    "    # Clean (with patch)\n",
    "    patched_logits = run_with_patch(model, prompt, patch_layer, clean_activations)\n",
    "    p_patched = torch.softmax(patched_logits[0, -1, :], dim=0)[target_id].item()\n",
    "    \n",
    "    ace = p_patched - p_corrupted\n",
    "    \n",
    "    return ace, p_corrupted, p_patched\n",
    "\n",
    "# Example\n",
    "clean_prompt = \"The capital of France is\"\n",
    "corrupted_prompt = \"The capital of Germany is\"\n",
    "target = \" Paris\"\n",
    "\n",
    "_, clean_cache = run_with_cache(model, clean_prompt, layer_idx=6)\n",
    "ace, p_before, p_after = compute_ace(\n",
    "    model, corrupted_prompt, 6, clean_cache['layer_6'], target\n",
    ")\n",
    "\n",
    "print(f\"P(Paris | corrupted, no patch) = {p_before:.4f}\")\n",
    "print(f\"P(Paris | corrupted, patch layer 6) = {p_after:.4f}\")\n",
    "print(f\"\\nAverage Causal Effect = {ace:.4f}\")\n",
    "print(f\"\\nInterpretation: Patching layer 6 {'increases' if ace > 0 else 'decreases'} \")\n",
    "print(f\"the probability of 'Paris' by {abs(ace):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Effect and Indirect Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_effect(model, clean_prompt, corrupted_prompt, target_token):\n",
    "    \"\"\"\n",
    "    Total effect of changing the prompt.\n",
    "    \"\"\"\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    clean_logits, _ = run_with_cache(model, clean_prompt)\n",
    "    corrupted_logits, _ = run_with_cache(model, corrupted_prompt)\n",
    "    \n",
    "    p_clean = torch.softmax(clean_logits[0, -1, :], dim=0)[target_id].item()\n",
    "    p_corrupted = torch.softmax(corrupted_logits[0, -1, :], dim=0)[target_id].item()\n",
    "    \n",
    "    total_effect = p_clean - p_corrupted\n",
    "    \n",
    "    return total_effect, p_clean, p_corrupted\n",
    "\n",
    "# Compute\n",
    "te, p_clean, p_corrupt = compute_total_effect(\n",
    "    model, clean_prompt, corrupted_prompt, target\n",
    ")\n",
    "\n",
    "print(f\"P(Paris | France) = {p_clean:.4f}\")\n",
    "print(f\"P(Paris | Germany) = {p_corrupt:.4f}\")\n",
    "print(f\"\\nTotal Effect = {te:.4f}\")\n",
    "print(f\"\\nThis is the total causal effect of changing France→Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ROME-Style Causal Tracing\n",
    "\n",
    "Replicate ROME's methodology at a small scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_trace(model, clean_prompt, target_token):\n",
    "    \"\"\"\n",
    "    ROME-style causal tracing: add noise everywhere, \n",
    "    then restore clean activations one component at a time.\n",
    "    \"\"\"\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get clean activations for all layers\n",
    "    clean_logits, clean_cache = run_with_cache(model, clean_prompt)\n",
    "    \n",
    "    # Get baseline probability\n",
    "    p_clean = torch.softmax(clean_logits[0, -1, :], dim=0)[target_id].item()\n",
    "    \n",
    "    # Add noise to embeddings to create corrupted run\n",
    "    # (In full ROME, they corrupt all embeddings; here we'll use a simpler approach)\n",
    "    \n",
    "    results = []\n",
    "    for layer in range(model.config.n_layer):\n",
    "        # Run with clean activations restored at this layer\n",
    "        _, cache_at_layer = run_with_cache(model, clean_prompt, layer_idx=layer)\n",
    "        \n",
    "        # For simplicity, we'll measure the effect of having clean vs noisy at each layer\n",
    "        # In real ROME, they patch clean into a fully corrupted run\n",
    "        \n",
    "        # Compute indirect effect through this layer\n",
    "        ace, _, p_restored = compute_ace(\n",
    "            model, clean_prompt, layer, cache_at_layer[f'layer_{layer}'], target_token\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'layer': layer,\n",
    "            'restoration': p_restored,\n",
    "            'ace': ace\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run causal trace\n",
    "trace_results = causal_trace(model, \"The Eiffel Tower is located in\", \" Paris\")\n",
    "\n",
    "# Visualize\n",
    "layers = [r['layer'] for r in trace_results]\n",
    "restoration = [r['restoration'] for r in trace_results]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(layers, restoration, marker='o', linewidth=2)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('P(correct answer)')\n",
    "plt.title('ROME-style Causal Trace: Which layers are critical?')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=max(restoration), color='r', linestyle='--', alpha=0.5, label='Peak performance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find critical layers\n",
    "top_layers = sorted(trace_results, key=lambda x: x['restoration'], reverse=True)[:3]\n",
    "print(\"\\nTop 3 most important layers:\")\n",
    "for r in top_layers:\n",
    "    print(f\"  Layer {r['layer']}: P(Paris) = {r['restoration']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which layers show the highest restoration? How does this compare to ROME's findings about middle layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Gradient-Based Attribution\n",
    "\n",
    "Use gradients to efficiently estimate importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_attribution(model, clean_prompt, corrupted_prompt, target_token):\n",
    "    \"\"\"\n",
    "    Compute gradient-based attribution for each layer.\n",
    "    \n",
    "    Attribution ≈ gradient × (clean - corrupted)\n",
    "    \"\"\"\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get clean and corrupted activations\n",
    "    _, clean_cache = run_with_cache(model, clean_prompt)\n",
    "    _, corrupted_cache = run_with_cache(model, corrupted_prompt)\n",
    "    \n",
    "    # Run corrupted with gradients enabled\n",
    "    model.zero_grad()\n",
    "    inputs = tokenizer(corrupted_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    activations = {}\n",
    "    \n",
    "    def save_activation_with_grad(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                act = output[0]\n",
    "            else:\n",
    "                act = output\n",
    "            act.requires_grad_(True)\n",
    "            act.retain_grad()\n",
    "            activations[name] = act\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    handles = []\n",
    "    for i in range(model.config.n_layer):\n",
    "        handle = model.transformer.h[i].register_forward_hook(\n",
    "            save_activation_with_grad(f\"layer_{i}\")\n",
    "        )\n",
    "        handles.append(handle)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Compute loss w.r.t. target token\n",
    "    loss = -logits[0, -1, target_id]  # Negative because we want to maximize\n",
    "    loss.backward()\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions = {}\n",
    "    for i in range(model.config.n_layer):\n",
    "        grad = activations[f\"layer_{i}\"].grad\n",
    "        delta = clean_cache[f\"layer_{i}\"] - corrupted_cache[f\"layer_{i}\"]\n",
    "        \n",
    "        # Attribution = grad · delta (element-wise product, then sum)\n",
    "        attribution = (grad.cpu() * delta).sum().item()\n",
    "        attributions[i] = attribution\n",
    "    \n",
    "    # Clean up\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "# Compute attributions\n",
    "attrs = gradient_attribution(model, clean_prompt, corrupted_prompt, target)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(attrs)), [attrs[i] for i in range(len(attrs))])\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Attribution Score')\n",
    "plt.title('Gradient-Based Attribution: Which layers matter most?')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Top attributed layers\n",
    "top_attrs = sorted(attrs.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "print(\"\\nTop 5 attributed layers:\")\n",
    "for layer, score in top_attrs:\n",
    "    print(f\"  Layer {layer}: {score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do gradient attributions match the actual patching results from earlier? Why might they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Average Indirect Effect (AIE)\n",
    "\n",
    "Systematically measure causal importance across components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aie(model, clean_examples, corrupted_examples, target_tokens):\n",
    "    \"\"\"\n",
    "    Compute Average Indirect Effect across multiple examples.\n",
    "    \n",
    "    AIE_layer = mean over examples of: P(correct | patch layer) - P(correct | no patch)\n",
    "    \"\"\"\n",
    "    n_layers = model.config.n_layer\n",
    "    aie_scores = {layer: [] for layer in range(n_layers)}\n",
    "    \n",
    "    for clean_prompt, corrupted_prompt, target in zip(clean_examples, corrupted_examples, target_tokens):\n",
    "        target_id = tokenizer.encode(target, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Baseline (corrupted, no patch)\n",
    "        baseline_logits, _ = run_with_cache(model, corrupted_prompt)\n",
    "        p_baseline = torch.softmax(baseline_logits[0, -1, :], dim=0)[target_id].item()\n",
    "        \n",
    "        # Test each layer\n",
    "        for layer in range(n_layers):\n",
    "            # Get clean activations\n",
    "            _, clean_cache = run_with_cache(model, clean_prompt, layer_idx=layer)\n",
    "            \n",
    "            # Patch and measure\n",
    "            patched_logits = run_with_patch(\n",
    "                model, corrupted_prompt, layer, clean_cache[f'layer_{layer}']\n",
    "            )\n",
    "            p_patched = torch.softmax(patched_logits[0, -1, :], dim=0)[target_id].item()\n",
    "            \n",
    "            # Indirect effect for this example\n",
    "            ie = p_patched - p_baseline\n",
    "            aie_scores[layer].append(ie)\n",
    "    \n",
    "    # Average across examples\n",
    "    aie = {layer: np.mean(scores) for layer, scores in aie_scores.items()}\n",
    "    \n",
    "    return aie\n",
    "\n",
    "# Test AIE on multiple examples\n",
    "clean_examples = [\n",
    "    \"The capital of France is\",\n",
    "    \"The Eiffel Tower is located in\",\n",
    "    \"Paris is the capital of\",\n",
    "]\n",
    "\n",
    "corrupted_examples = [\n",
    "    \"The capital of Germany is\",\n",
    "    \"The Eiffel Tower is located in\",  # Same (as a control)\n",
    "    \"Berlin is the capital of\",\n",
    "]\n",
    "\n",
    "targets = [\" Paris\", \" Paris\", \" France\"]\n",
    "\n",
    "aie_results = compute_aie(model, clean_examples, corrupted_examples, targets)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(aie_results)), [aie_results[i] for i in range(len(aie_results))],\n",
    "       color=['red' if v < 0 else 'green' for v in aie_results.values()])\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Indirect Effect')\n",
    "plt.title('AIE Across Layers: Which mediate the causal effect?')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Top AIE layers\n",
    "top_aie = sorted(aie_results.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nTop 5 layers by AIE:\")\n",
    "for layer, score in top_aie:\n",
    "    print(f\"  Layer {layer}: AIE = {score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Function Vectors\n",
    "\n",
    "Extract vectors that encode specific functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_vector(model, positive_examples, negative_examples, layer_idx=-1):\n",
    "    \"\"\"\n",
    "    Extract a function vector from contrastive pairs.\n",
    "    \n",
    "    Similar to steering vectors, but for specific functions.\n",
    "    \"\"\"\n",
    "    pos_activations = []\n",
    "    neg_activations = []\n",
    "    \n",
    "    for pos_prompt, neg_prompt in zip(positive_examples, negative_examples):\n",
    "        # Positive\n",
    "        _, pos_cache = run_with_cache(model, pos_prompt, layer_idx=layer_idx)\n",
    "        pos_activations.append(pos_cache[f'layer_{layer_idx}'][0, -1, :].cpu())\n",
    "        \n",
    "        # Negative  \n",
    "        _, neg_cache = run_with_cache(model, neg_prompt, layer_idx=layer_idx)\n",
    "        neg_activations.append(neg_cache[f'layer_{layer_idx}'][0, -1, :].cpu())\n",
    "    \n",
    "    # Mean difference\n",
    "    pos_mean = torch.stack(pos_activations).mean(dim=0)\n",
    "    neg_mean = torch.stack(neg_activations).mean(dim=0)\n",
    "    \n",
    "    function_vector = pos_mean - neg_mean\n",
    "    \n",
    "    return function_vector\n",
    "\n",
    "# Example: Extract \"comparison reversal\" function\n",
    "comparison_pairs = [\n",
    "    (\"Paris is larger than Lyon\", \"Lyon is larger than Paris\"),\n",
    "    (\"Ten is greater than five\", \"Five is greater than ten\"),\n",
    "    (\"The elephant is bigger than the mouse\", \"The mouse is bigger than the elephant\"),\n",
    "]\n",
    "\n",
    "pos_examples = [p[0] for p in comparison_pairs]\n",
    "neg_examples = [p[1] for p in comparison_pairs]\n",
    "\n",
    "comparison_vector = extract_function_vector(model, pos_examples, neg_examples, layer_idx=8)\n",
    "\n",
    "print(f\"Extracted comparison function vector\")\n",
    "print(f\"Shape: {comparison_vector.shape}\")\n",
    "print(f\"Magnitude: {comparison_vector.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_vector(model, prompt, function_vector, layer_idx, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply a function vector to a prompt.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    function_vector = function_vector.to(device)\n",
    "    \n",
    "    def function_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        \n",
    "        # Add function vector to last position\n",
    "        hidden_states[0, -1, :] += alpha * function_vector\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "    \n",
    "    handle = model.transformer.h[layer_idx].register_forward_hook(function_hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    handle.remove()\n",
    "    \n",
    "    return outputs.logits\n",
    "\n",
    "# Test on new example\n",
    "test_prompt = \"A mountain is taller than a\"\n",
    "\n",
    "print(\"Original predictions:\")\n",
    "original_logits, _ = run_with_cache(model, test_prompt)\n",
    "for token, prob in get_top_prediction(original_logits)[:5]:\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nWith comparison reversal function (+):\")\n",
    "reversed_logits = apply_function_vector(model, test_prompt, comparison_vector, 8, alpha=2.0)\n",
    "for token, prob in get_top_prediction(reversed_logits)[:5]:\n",
    "    print(f\"  {token:15s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1: Extract Your Own Function Vector\n",
    "\n",
    "Design pairs for a function relevant to your concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your function pairs\n",
    "my_function_pairs = [\n",
    "    # (\"with function\", \"without function\"),\n",
    "]\n",
    "\n",
    "# Extract and test your function vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Designing Counterfactual Datasets\n",
    "\n",
    "Practice creating effective minimal pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_counterfactual_pair(model, pair, expected_difference=\"outputs should differ\"):\n",
    "    \"\"\"\n",
    "    Check if a counterfactual pair produces different outputs.\n",
    "    \"\"\"\n",
    "    prompt1, prompt2 = pair\n",
    "    \n",
    "    logits1, _ = run_with_cache(model, prompt1)\n",
    "    logits2, _ = run_with_cache(model, prompt2)\n",
    "    \n",
    "    pred1 = get_top_prediction(logits1, top_k=1)[0]\n",
    "    pred2 = get_top_prediction(logits2, top_k=1)[0]\n",
    "    \n",
    "    # Check structural similarity\n",
    "    tokens1 = tokenizer.tokenize(prompt1)\n",
    "    tokens2 = tokenizer.tokenize(prompt2)\n",
    "    length_diff = abs(len(tokens1) - len(tokens2))\n",
    "    \n",
    "    print(f\"Pair validation:\")\n",
    "    print(f\"  Prompt 1: '{prompt1}'\")\n",
    "    print(f\"    → {pred1[0]} ({pred1[1]:.4f})\")\n",
    "    print(f\"  Prompt 2: '{prompt2}'\")\n",
    "    print(f\"    → {pred2[0]} ({pred2[1]:.4f})\")\n",
    "    print(f\"\\n  Length difference: {length_diff} tokens\")\n",
    "    print(f\"  Predictions differ: {pred1[0] != pred2[0]}\")\n",
    "    \n",
    "    # Good pair criteria\n",
    "    if length_diff > 2:\n",
    "        print(\"  ⚠️  Warning: Large length difference may introduce confounds\")\n",
    "    if pred1[0] == pred2[0]:\n",
    "        print(\"  ⚠️  Warning: Same prediction - pair may not test your hypothesis\")\n",
    "    else:\n",
    "        print(\"  ✓ Good pair: minimal change, different predictions\")\n",
    "    \n",
    "    return pred1[0] != pred2[0]\n",
    "\n",
    "# Test examples\n",
    "good_pair = (\"The capital of France is\", \"The capital of Germany is\")\n",
    "bad_pair = (\"France's capital is\", \"The capital of Germany is\")\n",
    "\n",
    "print(\"GOOD PAIR:\")\n",
    "print(\"=\"*60)\n",
    "validate_counterfactual_pair(model, good_pair)\n",
    "\n",
    "print(\"\\n\\nBAD PAIR (structure differs):\")\n",
    "print(\"=\"*60)\n",
    "validate_counterfactual_pair(model, bad_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1: Design Dataset for Your Concept\n",
    "\n",
    "Create and validate 10-15 pairs for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your counterfactual pairs\n",
    "my_pairs = [\n",
    "    # (\"with concept\", \"without concept\"),\n",
    "]\n",
    "\n",
    "# Validate each pair\n",
    "# for pair in my_pairs:\n",
    "#     validate_counterfactual_pair(model, pair)\n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Answer these for your project writeup:\n",
    "\n",
    "1. **Patching Strategy**: When should you use noise patching vs clean patching for your concept?\n",
    "\n",
    "2. **Localization**: Which layers are most important for your concept? How does this compare to ROME (middle MLPs) or entity tracking (attention heads)?\n",
    "\n",
    "3. **Attribution**: Do gradient attributions match actual patching results? What might explain discrepancies?\n",
    "\n",
    "4. **AIE**: Which components show high AIE? What does this tell you about the causal pathway?\n",
    "\n",
    "5. **Function Vectors**: Can your concept be captured as a function vector? Why or why not?\n",
    "\n",
    "6. **Counterfactuals**: What makes a good counterfactual pair for your concept? What confounds did you have to avoid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "For your assignment:\n",
    "1. Design comprehensive counterfactual dataset (15-25 pairs)\n",
    "2. Run AIE analysis to localize important layers/components\n",
    "3. Apply gradient attribution for fine-grained identification\n",
    "4. Validate top findings with direct patching\n",
    "5. Extract function vectors if applicable\n",
    "6. Build mechanistic story of how your concept is processed\n",
    "\n",
    "These causal methods will be essential for finding complete circuits in Week 5!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
