<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Week 9: Representation Similarity Analysis</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: 'Georgia', serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      border-bottom: 3px solid #3498db;
      padding-bottom: 10px;
    }

    h2 {
      color: #34495e;
      margin-top: 30px;
      border-bottom: 2px solid #bdc3c7;
      padding-bottom: 5px;
    }

    h3 {
      color: #555;
      margin-top: 20px;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-box {
      background-color: #e8f4f8;
      border-left: 4px solid #3498db;
      padding: 15px;
      margin: 20px 0;
    }

    .key-insight {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .comparison-box {
      background-color: #f0f7ff;
      border: 1px solid #3498db;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .paper-card {
      background-color: #fff;
      border: 2px solid #3498db;
      border-radius: 8px;
      padding: 20px;
      margin: 20px 0;
    }

    .paper-card h4 {
      margin-top: 0;
      color: #2c3e50;
    }

    ul,
    ol {
      margin: 10px 0;
    }

    li {
      margin: 8px 0;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }

    .objectives {
      background-color: #f0f7ff;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
    }
  </style>
</head>

<body>
  <h1>Week 9: Representation Similarity Analysis</h1>

  <div class="objectives">
    <h3>Learning Objectives</h3>
    <ul>
      <li>Understand why comparing representations across layers, models, or modalities matters</li>
      <li>Master key similarity metrics: RSA, CKA, CCA, and Procrustes</li>
      <li>Compare concept representations to human semantic spaces</li>
      <li>Identify which layers encode specific concepts</li>
      <li>Validate interpretability findings through cross-model consistency</li>
      <li>Apply RSA methods to your research project</li>
      <li>Understand the limitations and failure modes of similarity metrics</li>
    </ul>
  </div>

  <h2>1. Why Compare Representations?</h2>

  <h3>The Comparison Problem</h3>
  <p>
    Throughout this course, you've analyzed individual models in isolation. But critical questions require
    <strong>comparison</strong>:
  </p>

  <ul>
    <li><strong>Which layer</strong> encodes your concept? (Layer comparison)</li>
    <li><strong>Is your finding</strong> model-specific or universal? (Cross-model comparison)</li>
    <li><strong>How does</strong> the model's representation relate to human understanding? (Model-to-human comparison)
    </li>
    <li><strong>Did fine-tuning</strong> change the representation space? (Before-after comparison)</li>
    <li><strong>Are two concepts</strong> encoded similarly? (Concept-to-concept comparison)</li>
  </ul>

  <div class="key-insight">
    <p><strong>The Challenge:</strong></p>
    <p>
      Neural network layers have different dimensionalities, use different coordinate systems, and may permute their
      axes arbitrarily. You can't just subtract vectors or compute correlations.
    </p>
    <p>
      <strong>Representation Similarity Analysis (RSA)</strong> provides geometry-aware methods to compare
      representational spaces despite these differences.
    </p>
  </div>

  <h3>Use Cases for Your Project</h3>

  <div class="example-box">
    <p><strong>Example 1: Finding Where Concepts Live</strong></p>
    <p>
      <strong>Question:</strong> Which transformer layer encodes musical key?
    </p>
    <p>
      <strong>Method:</strong> Compare each layer's representations to a "ground truth" label space (major vs minor
      keys) using CKA.
    </p>
    <p>
      <strong>Result:</strong> CKA peaks at layer 18, suggesting key is primarily encoded there.
    </p>

    <hr>

    <p><strong>Example 2: Validating Cross-Model Consistency</strong></p>
    <p>
      <strong>Question:</strong> Is the "politeness" representation specific to GPT-4 or universal across LLMs?
    </p>
    <p>
      <strong>Method:</strong> Extract politeness concept vectors from GPT-4 and Llama-3, compare using Procrustes
      alignment.
    </p>
    <p>
      <strong>Result:</strong> High similarity (r=0.82) suggests concept is universal, not model artifact.
    </p>

    <hr>

    <p><strong>Example 3: Comparing to Human Judgments</strong></p>
    <p>
      <strong>Question:</strong> Does the model's protein similarity match biochemist judgments?
    </p>
    <p>
      <strong>Method:</strong> Collect human similarity ratings for protein pairs, compare to model embedding
      distances using RSA.
    </p>
    <p>
      <strong>Result:</strong> Spearman ρ=0.71 indicates moderate alignment with human understanding.
    </p>
  </div>

  <h2>2. Representational Similarity Analysis (RSA)</h2>

  <div class="paper-card">
    <h4>Paper: "Representational Similarity Analysis"</h4>
    <p><strong>Authors:</strong> Nikolaus Kriegeskorte, Marieke Mur, Peter Bandettini</p>
    <p><strong>Published:</strong> Frontiers in Systems Neuroscience, 2008</p>
    <p><strong>Key contribution:</strong> Framework for comparing representations across different systems (brains,
      models, measurements)</p>
  </div>

  <h3>Core Idea</h3>

  <p>
    Instead of comparing representations directly, <strong>compare their similarity structures</strong>.
  </p>

  <div class="info-box">
    <p><strong>RSA Procedure:</strong></p>
    <ol>
      <li><strong>Build Representational Dissimilarity Matrix (RDM)</strong> for each system
        <ul>
          <li>For \(n\) stimuli/examples, compute pairwise distances</li>
          <li>Result: \(n \times n\) matrix where RDM[i,j] = distance between representations of stimulus i and j</li>
        </ul>
      </li>
      <li><strong>Compare RDMs</strong> using correlation (Spearman ρ or Pearson r)
        <ul>
          <li>Flatten RDMs into vectors (upper triangular entries)</li>
          <li>Compute correlation between vectors</li>
        </ul>
      </li>
      <li><strong>Interpret:</strong> High correlation → similar representational geometry</li>
    </ol>
  </div>

  <h3>Mathematical Formulation</h3>

  <p>
    Given representations \(X \in \mathbb{R}^{n \times d_1}\) (e.g., layer 10) and \(Y \in \mathbb{R}^{n \times d_2}\)
    (e.g., layer 20):
  </p>

  <p>
    1. <strong>Compute RDMs:</strong>
  </p>
  <p style="text-align: center;">
    \(\text{RDM}_X[i,j] = \|x_i - x_j\|_2\)
  </p>
  <p style="text-align: center;">
    \(\text{RDM}_Y[i,j] = \|y_i - y_j\|_2\)
  </p>

  <p>
    2. <strong>Flatten to vectors:</strong> Extract upper triangular entries (excluding diagonal)
  </p>

  <p>
    3. <strong>Compute similarity:</strong>
  </p>
  <p style="text-align: center;">
    \(\rho = \text{Spearman}(\text{vec}(\text{RDM}_X), \text{vec}(\text{RDM}_Y))\)
  </p>

  <h3>Strengths and Limitations</h3>

  <div class="comparison-box">
    <p><strong>✓ Strengths:</strong></p>
    <ul>
      <li><strong>Dimension-agnostic:</strong> Works for representations of any dimensionality</li>
      <li><strong>Scale-invariant:</strong> Uses rank correlation (Spearman), robust to rescaling</li>
      <li><strong>Interpretable:</strong> Directly tests if two systems organize stimuli similarly</li>
      <li><strong>Flexible distance metrics:</strong> Can use cosine, Euclidean, correlation distance</li>
    </ul>

    <p><strong>✗ Limitations:</strong></p>
    <ul>
      <li><strong>Sensitive to stimulus selection:</strong> Results depend heavily on which examples you include</li>
      <li><strong>Doesn't find alignment:</strong> Only measures similarity, doesn't transform one space to another</li>
      <li><strong>Computationally expensive:</strong> \(O(n^2)\) pairwise comparisons</li>
      <li><strong>No significance testing:</strong> Hard to know if ρ=0.6 is "good"</li>
    </ul>
  </div>

  <h2>3. Centered Kernel Alignment (CKA)</h2>

  <div class="paper-card">
    <h4>Paper: "Similarity of Neural Network Representations Revisited"</h4>
    <p><strong>Authors:</strong> Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton</p>
    <p><strong>Published:</strong> ICML 2019</p>
    <p><strong>Key contribution:</strong> CKA is invariant to invertible linear transformations, making it superior to
      earlier metrics</p>
  </div>

  <h3>Why CKA?</h3>

  <p>
    Early similarity metrics (like CCA) had problems:
  </p>
  <ul>
    <li>Sensitive to small dimensional subspaces</li>
    <li>Affected by random permutations</li>
    <li>Failed to detect trivial differences (e.g., layer width changes)</li>
  </ul>

  <p>
    <strong>CKA addresses these issues</strong> by comparing representations via kernel functions, focusing on the
    geometry of the data cloud rather than individual coordinates.
  </p>

  <h3>Mathematical Formulation</h3>

  <p>
    Given representations \(X \in \mathbb{R}^{n \times d_1}\) and \(Y \in \mathbb{R}^{n \times d_2}\):
  </p>

  <p>
    1. <strong>Compute Gram matrices:</strong>
  </p>
  <p style="text-align: center;">
    \(K = XX^T\) (similarity between examples in X-space)
  </p>
  <p style="text-align: center;">
    \(L = YY^T\) (similarity between examples in Y-space)
  </p>

  <p>
    2. <strong>Center the matrices:</strong>
  </p>
  <p style="text-align: center;">
    \(\bar{K} = HKH\), where \(H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^T\)
  </p>

  <p>
    3. <strong>Compute CKA:</strong>
  </p>
  <p style="text-align: center;">
    \(\text{CKA}(X, Y) = \frac{\|\bar{K} \odot \bar{L}\|_F}{\|\bar{K}\|_F \|\bar{L}\|_F}\)
  </p>

  <p>
    Where \(\odot\) is element-wise multiplication and \(\|\cdot\|_F\) is Frobenius norm.
  </p>

  <h3>Key Properties</h3>

  <div class="info-box">
    <ul>
      <li><strong>Range:</strong> CKA ∈ [0, 1], where 1 = identical geometry, 0 = orthogonal</li>
      <li><strong>Invariant to:</strong> Invertible linear transformations, isotropic scaling, orthogonal rotations</li>
      <li><strong>Not invariant to:</strong> Nonlinear transformations, permutations of features (which is good—these
        change meaning)</li>
      <li><strong>Interpretable:</strong> Measures what fraction of variance is shared between representations</li>
    </ul>
  </div>

  <h3>Practical Usage</h3>

  <div class="example-box">
    <p><strong>Example: Finding Where Concepts Emerge</strong></p>

    <p><strong>Task:</strong> Identify which layer encodes sentiment in movie reviews</p>

    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Create "ground truth" representation: one-hot encoding of sentiment labels (positive/negative)</li>
      <li>For each transformer layer, extract representations for 1000 reviews</li>
      <li>Compute CKA between layer representation and ground truth</li>
      <li>Plot CKA vs layer depth</li>
    </ol>

    <p><strong>Interpretation:</strong></p>
    <ul>
      <li>CKA increases with depth → concept emerges gradually</li>
      <li>Peak at layer 15 → sentiment is most explicit here</li>
      <li>CKA drops in final layers → sentiment abstracted away for generation</li>
    </ul>
  </div>

  <h2>4. Canonical Correlation Analysis (CCA) and Variants</h2>

  <div class="paper-card">
    <h4>Paper: "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability"
    </h4>
    <p><strong>Authors:</strong> Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein</p>
    <p><strong>Published:</strong> NeurIPS 2017</p>
    <p><strong>Key contribution:</strong> Combines SVD with CCA to handle high-dimensional, noisy representations</p>
  </div>

  <h3>Canonical Correlation Analysis (CCA)</h3>

  <p>
    CCA finds linear transformations of \(X\) and \(Y\) that maximize correlation.
  </p>

  <p>
    <strong>Goal:</strong> Find weight matrices \(W_X\) and \(W_Y\) such that:
  </p>
  <p style="text-align: center;">
    \(\text{corr}(XW_X, YW_Y)\) is maximized
  </p>

  <p>
    <strong>Output:</strong> Canonical correlation coefficients \(\rho_1, \rho_2, \ldots, \rho_k\) where \(k =
    \min(d_1, d_2)\)
  </p>

  <p>
    <strong>Interpretation:</strong> Mean of \(\rho_i\) indicates how well representations align after optimal rotation.
  </p>

  <h3>Singular Vector CCA (SVCCA)</h3>

  <p>
    Problem with vanilla CCA: Sensitive to noise in high dimensions.
  </p>

  <div class="info-box">
    <p><strong>SVCCA Solution:</strong></p>
    <ol>
      <li><strong>SVD preprocessing:</strong> For \(X\) and \(Y\), keep only top-\(k\) singular vectors (e.g., 99%
        variance)</li>
      <li><strong>Apply CCA:</strong> Run CCA on dimensionality-reduced representations</li>
      <li><strong>Aggregate:</strong> Mean canonical correlation = similarity score</li>
    </ol>

    <p><strong>Why this helps:</strong> Removes noisy dimensions that CCA might spuriously align</p>
  </div>

  <h3>When to Use CCA vs CKA</h3>

  <table>
    <tr>
      <th>Metric</th>
      <th>When to Use</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
    <tr>
      <td><strong>CCA</strong></td>
      <td>When you want to find explicit alignment between spaces</td>
      <td>Provides transformation matrices; interpretable components</td>
      <td>Sensitive to noise; biased toward small subspaces</td>
    </tr>
    <tr>
      <td><strong>SVCCA</strong></td>
      <td>High-dimensional, noisy representations</td>
      <td>Robust to noise; good for training dynamics</td>
      <td>Arbitrary SVD cutoff; loses information</td>
    </tr>
    <tr>
      <td><strong>CKA</strong></td>
      <td>Comparing overall geometry; different dimensionalities</td>
      <td>Invariant to rotations; no hyperparameters; robust</td>
      <td>Doesn't provide alignment; harder to interpret "why"</td>
    </tr>
    <tr>
      <td><strong>RSA</strong></td>
      <td>Comparing to human data; neuroscience applications</td>
      <td>Intuitive; flexible distance metrics</td>
      <td>Sensitive to stimuli; computationally expensive</td>
    </tr>
  </table>

  <h2>5. Procrustes Analysis</h2>

  <h3>The Alignment Problem</h3>

  <p>
    Sometimes you want more than similarity—you want to <strong>align</strong> two representation spaces.
  </p>

  <p>
    <strong>Use case:</strong> Compare concept vectors across models (e.g., "honesty" in GPT-4 vs Llama).
  </p>

  <h3>Procrustes Method</h3>

  <div class="info-box">
    <p><strong>Goal:</strong> Find rotation matrix \(R\) that best aligns \(X\) to \(Y\)</p>

    <p><strong>Minimize:</strong></p>
    <p style="text-align: center;">
      \(\|XR - Y\|_F^2\)
    </p>

    <p><strong>Solution (via SVD):</strong></p>
    <ol>
      <li>Compute \(M = Y^T X\)</li>
      <li>SVD: \(M = U \Sigma V^T\)</li>
      <li>Optimal rotation: \(R = VU^T\)</li>
    </ol>

    <p><strong>Similarity score:</strong> Procrustes distance = \(\|XR - Y\|_F\) (lower = more similar)</p>
  </div>

  <h3>Application: Cross-Model Concept Comparison</h3>

  <div class="example-box">
    <p><strong>Research Question:</strong> Is the "factual recall" mechanism universal across LLMs?</p>

    <p><strong>Method:</strong></p>
    <ol>
      <li>Extract factual recall concept vector from GPT-4 (using steering or probing)</li>
      <li>Extract factual recall concept vector from Llama-3</li>
      <li>Use Procrustes to align the two spaces</li>
      <li>Measure distance between aligned vectors</li>
    </ol>

    <p><strong>Result interpretation:</strong></p>
    <ul>
      <li>Small distance → concept is universal (same mechanism)</li>
      <li>Large distance → model-specific implementation</li>
    </ul>
  </div>

  <h2>6. Comparing Representations to Human Judgments</h2>

  <h3>Why Compare to Humans?</h3>

  <p>
    For domains like semantics, aesthetics, or morality, <strong>human similarity judgments</strong> are the ground
    truth.
  </p>

  <p>
    <strong>Question:</strong> Does the model's conceptual space match human understanding?
  </p>

  <h3>Method 1: Direct RSA</h3>

  <div class="example-box">
    <p><strong>Procedure:</strong></p>
    <ol>
      <li><strong>Collect human data:</strong> Ask humans to rate similarity of concept pairs (e.g., "How similar are
        'cat' and 'dog'?" on 1-7 scale)</li>
      <li><strong>Build human RDM:</strong> Matrix of human similarity judgments</li>
      <li><strong>Build model RDM:</strong> Pairwise distances in model's embedding space</li>
      <li><strong>Compare:</strong> Spearman correlation between RDMs</li>
    </ol>

    <p><strong>Interpretation:</strong></p>
    <ul>
      <li>High correlation (ρ > 0.7) → model's geometry matches human conceptual space</li>
      <li>Low correlation (ρ < 0.3) → model uses different organizing principles</li>
    </ul>
  </div>

  <h3>Method 2: Behavioral Alignment Tasks</h3>

  <p>
    Instead of explicit ratings, use <strong>behavioral tasks</strong>:
  </p>

  <ul>
    <li><strong>Odd-one-out:</strong> "Which doesn't belong: cat, dog, car?" (measures clustering)</li>
    <li><strong>Categorization:</strong> Free sorting into groups</li>
    <li><strong>Triplet comparisons:</strong> "Is A more similar to B or C?"</li>
  </ul>

  <p>
    These tasks reveal implicit similarity structure, which you can compare to model representations via RSA or CKA.
  </p>

  <h2>7. Identifying Where Concepts Are Encoded</h2>

  <h3>Layer-wise Analysis</h3>

  <p>
    A common research question: <strong>Which layer encodes concept X?</strong>
  </p>

  <div class="info-box">
    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Define concept operationally (e.g., sentiment = positive/negative labels)</li>
      <li>Create dataset with concept labels</li>
      <li>For each transformer layer, extract representations</li>
      <li>Compute CKA (or probe accuracy) between layer and concept labels</li>
      <li>Plot metric vs layer depth</li>
    </ol>

    <p><strong>Common patterns:</strong></p>
    <ul>
      <li><strong>Early layers:</strong> Syntax, surface form</li>
      <li><strong>Middle layers:</strong> Semantics, concepts, entities</li>
      <li><strong>Late layers:</strong> Task-specific features, pragmatics</li>
    </ul>
  </div>

  <h3>Example: Semantic Role Encoding</h3>

  <div class="example-box">
    <p><strong>Question:</strong> Where does BERT encode semantic roles (agent, patient, theme)?</p>

    <p><strong>Dataset:</strong> 500 sentences annotated with semantic roles</p>

    <p><strong>Method:</strong></p>
    <ol>
      <li>For each layer (0-12), extract token representations for role-bearing words</li>
      <li>Compute CKA between layer representations and one-hot role labels</li>
    </ol>

    <p><strong>Findings:</strong></p>
    <ul>
      <li>CKA peaks at layer 8 (middle layer)</li>
      <li>Early layers (1-3): Low CKA (surface syntax dominates)</li>
      <li>Late layers (10-12): CKA decreases (semantic roles abstracted for MLM task)</li>
    </ul>

    <p><strong>Implication:</strong> For semantic role probing, use layer 8 representations.</p>
  </div>

  <h2>8. Validation Through Cross-Model Consistency</h2>

  <h3>The Generalization Problem</h3>

  <p>
    You found a concept vector in GPT-4. But is it a <strong>general principle</strong> or a <strong>model
      artifact</strong>?
  </p>

  <div class="key-insight">
    <p><strong>Validation strategy:</strong> Test if the concept exists in multiple models.</p>
    <ul>
      <li>If consistent across models → likely a fundamental concept</li>
      <li>If model-specific → may be due to architecture or training data quirks</li>
    </ul>
  </div>

  <h3>Cross-Model Comparison Protocol</h3>

  <div class="example-box">
    <p><strong>Research question:</strong> Is "political bias" encoded similarly in different LLMs?</p>

    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Extract concept vector from GPT-4 using contrastive activation addition</li>
      <li>Extract concept vector from Llama-3, Claude, Gemini using same method</li>
      <li>Align representation spaces using Procrustes</li>
      <li>Measure pairwise distances between concept vectors</li>
      <li>Compute CKA between models' representations of political texts</li>
    </ol>

    <p><strong>Strong evidence of universality:</strong></p>
    <ul>
      <li>High CKA (> 0.8) between model representations</li>
      <li>Concept vectors align after Procrustes (cosine similarity > 0.7)</li>
      <li>Interventions using cross-model concept vectors transfer successfully</li>
    </ul>
  </div>

  <h2>9. Limitations and Pitfalls</h2>

  <h3>Common Mistakes</h3>

  <div class="info-box">
    <p><strong>1. Ignoring data distribution</strong></p>
    <ul>
      <li><strong>Problem:</strong> CKA is sensitive to example selection</li>
      <li><strong>Example:</strong> Using 90% negative, 10% positive examples inflates similarity to negative-only
        space</li>
      <li><strong>Solution:</strong> Balance datasets; report results across multiple samplings</li>
    </ul>

    <p><strong>2. Misinterpreting magnitude</strong></p>
    <ul>
      <li><strong>Problem:</strong> "CKA=0.6" has no absolute meaning—depends on task</li>
      <li><strong>Solution:</strong> Compare to baselines (random, different-layer, different-model)</li>
    </ul>

    <p><strong>3. Confusing similarity with functionality</strong></p>
    <ul>
      <li><strong>Problem:</strong> Similar representations don't imply same behavior</li>
      <li><strong>Example:</strong> Two models have high CKA but different downstream accuracy</li>
      <li><strong>Solution:</strong> Validate with behavioral metrics (probing, task performance)</li>
    </ul>

    <p><strong>4. Over-relying on linear metrics</strong></p>
    <ul>
      <li><strong>Problem:</strong> CCA, CKA assume linear or kernel-linear geometry</li>
      <li><strong>Failure case:</strong> Concepts encoded nonlinearly may show low CKA despite functional equivalence
      </li>
      <li><strong>Solution:</strong> Combine with nonlinear probes (Week 5) and causal interventions (Week 4)</li>
    </ul>
  </div>

  <h3>When RSA Fails</h3>

  <div class="example-box">
    <p><strong>Case study: Adversarial similarity</strong></p>

    <p>
      Two models could have high CKA on normal examples but completely different representations on adversarial
      perturbations.
    </p>

    <p><strong>Lesson:</strong> Test similarity across diverse data distributions, including edge cases.</p>

    <hr>

    <p><strong>Case study: Degenerate solutions</strong></p>

    <p>
      A model might encode a concept redundantly in multiple subspaces. CKA might show low similarity to each
      individual subspace, missing the overall encoding.
    </p>

    <p><strong>Lesson:</strong> Use multiple metrics (RSA, CKA, probing accuracy) and interpret jointly.</p>
  </div>

  <h2>10. Practical Implementation</h2>

  <h3>Code Example: CKA Computation</h3>

  <div class="example-box">
    <p><strong>Python implementation:</strong></p>
    <pre><code>import numpy as np

def center_gram(K):
    """Center a Gram matrix."""
    n = K.shape[0]
    H = np.eye(n) - np.ones((n, n)) / n
    return H @ K @ H

def cka(X, Y):
    """Compute CKA between two representation matrices."""
    # Gram matrices
    K = X @ X.T
    L = Y @ Y.T

    # Center
    K_c = center_gram(K)
    L_c = center_gram(L)

    # CKA formula
    numerator = np.linalg.norm(K_c * L_c, 'fro') ** 2
    denominator = np.linalg.norm(K_c, 'fro') * np.linalg.norm(L_c, 'fro')

    return numerator / denominator

# Example usage
# X: (n_examples, d1) - layer 10 representations
# Y: (n_examples, d2) - layer 20 representations
# similarity = cka(X, Y)  # returns value in [0, 1]
</code></pre>
  </div>

  <h2>11. Applying RSA to Your Project</h2>

  <h3>Integration with Previous Weeks</h3>

  <p>
    RSA complements methods from earlier weeks:
  </p>

  <table>
    <tr>
      <th>Week</th>
      <th>Method</th>
      <th>How RSA Helps</th>
    </tr>
    <tr>
      <td><strong>Week 4</strong></td>
      <td>Causal Mediation</td>
      <td>Identify which layer to patch by finding where concept is encoded (CKA peak)</td>
    </tr>
    <tr>
      <td><strong>Week 5</strong></td>
      <td>Probes</td>
      <td>Validate probe findings: Does CKA agree with probe accuracy?</td>
    </tr>
    <tr>
      <td><strong>Week 6</strong></td>
      <td>Attribution</td>
      <td>Check if attribution patterns are consistent across models (cross-model CKA)</td>
    </tr>
    <tr>
      <td><strong>Week 7</strong></td>
      <td>SAEs</td>
      <td>Compare SAE-discovered features to human semantic spaces (RSA)</td>
    </tr>
    <tr>
      <td><strong>Week 8</strong></td>
      <td>Circuits</td>
      <td>Test if circuit components are universal (Procrustes alignment of circuit vectors)</td>
    </tr>
  </table>

  <h3>Project Workflow</h3>

  <div class="info-box">
    <p><strong>Step 1: Locate Your Concept</strong></p>
    <ul>
      <li>Compute CKA between each layer and your concept labels</li>
      <li>Identify peak layer → focus subsequent analysis here</li>
    </ul>

    <p><strong>Step 2: Validate Across Models</strong></p>
    <ul>
      <li>Extract concept from GPT-4, Llama, Claude</li>
      <li>Use Procrustes to align, measure distance</li>
      <li>High consistency → stronger evidence</li>
    </ul>

    <p><strong>Step 3: Compare to Human Understanding</strong></p>
    <ul>
      <li>Collect human similarity judgments (even n=20 is useful)</li>
      <li>Compute RSA between model RDM and human RDM</li>
      <li>Interpret alignment or divergence</li>
    </ul>

    <p><strong>Step 4: Report Robustness</strong></p>
    <ul>
      <li>Test across multiple datasets</li>
      <li>Vary hyperparameters (SVD cutoffs, distance metrics)</li>
      <li>Report mean and variance of similarity metrics</li>
    </ul>
  </div>

  <h2>12. Summary</h2>

  <h3>Key Takeaways</h3>

  <ul>
    <li><strong>RSA compares representational geometry</strong> despite differences in dimensionality and coordinate
      systems</li>
    <li><strong>CKA is robust and invariant</strong> to linear transformations—preferred for layer/model comparison</li>
    <li><strong>CCA/SVCCA find explicit alignments</strong> useful when you need transformation matrices</li>
    <li><strong>Procrustes aligns spaces</strong> for direct concept vector comparison across models</li>
    <li><strong>Compare to human judgments</strong> to validate that model concepts match human understanding</li>
    <li><strong>Use RSA to identify where concepts live</strong> and whether findings generalize</li>
    <li><strong>Combine with other methods</strong>—RSA is most powerful when validating causal/probe/SAE findings</li>
  </ul>

  <h3>Best Practices</h3>

  <div class="info-box">
    <ul>
      <li>Always use multiple metrics (RSA, CKA, probing accuracy) and check for agreement</li>
      <li>Report baselines (random layers, different tasks) to interpret magnitude</li>
      <li>Test robustness across data distributions and hyperparameters</li>
      <li>Don't confuse representational similarity with functional equivalence—validate behaviorally</li>
      <li>Use cross-model consistency as evidence of generalization</li>
    </ul>
  </div>

  <h2>References & Resources</h2>

  <h3>Core Papers</h3>
  <ul>
    <li><strong>Kriegeskorte et al. (2008):</strong> "Representational similarity analysis - connecting the branches of
      systems neuroscience." <em>Frontiers in Systems Neuroscience</em>. <a
        href="https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full" target="_blank">Link</a></li>
    <li><strong>Kornblith et al. (2019):</strong> "Similarity of Neural Network Representations Revisited." <em>ICML</em>.
      <a href="https://arxiv.org/abs/1905.00414" target="_blank">arXiv:1905.00414</a>
    </li>
    <li><strong>Raghu et al. (2017):</strong> "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning
      Dynamics and Interpretability." <em>NeurIPS</em>. <a href="https://arxiv.org/abs/1706.05806"
        target="_blank">arXiv:1706.05806</a></li>
  </ul>

  <h3>Related Work</h3>
  <ul>
    <li><strong>Morcos et al. (2018):</strong> "Insights on representational similarity in neural networks with canonical
      correlation." <em>NeurIPS</em>.</li>
    <li><strong>Ding et al. (2021):</strong> "Grounding Representation Similarity with Statistical Testing." <em>NeurIPS</em>.</li>
    <li><strong>Klabunde et al. (2023):</strong> "Similarity of Neural Networks with Gradients." <em>ICLR</em>.</li>
    <li><strong>Gale et al. (2022):</strong> "Geometry of Polysemanticity." <em>arXiv</em>.</li>
  </ul>

  <h3>Tutorials and Code</h3>
  <ul>
    <li>Google Brain CKA implementation: <a href="https://github.com/google-research/google-research/tree/master/representation_similarity" target="_blank">GitHub</a></li>
    <li>PyTorch CKA library: <code>pip install torch-cka</code></li>
    <li>RSA Toolbox (MATLAB): <a href="https://github.com/rsagroup/rsatoolbox" target="_blank">GitHub</a></li>
  </ul>

</body>

</html>
