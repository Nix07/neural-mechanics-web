<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Week 9: Input Attribution and Saliency Methods</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: 'Georgia', serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      border-bottom: 3px solid #3498db;
      padding-bottom: 10px;
    }

    h2 {
      color: #34495e;
      margin-top: 30px;
      border-bottom: 2px solid #bdc3c7;
      padding-bottom: 5px;
    }

    h3 {
      color: #555;
      margin-top: 20px;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-box {
      background-color: #e8f4f8;
      border-left: 4px solid #3498db;
      padding: 15px;
      margin: 20px 0;
    }

    .warning-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .comparison-box {
      background-color: #f0f7ff;
      border: 1px solid #3498db;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    ul,
    ol {
      margin: 10px 0;
    }

    li {
      margin: 8px 0;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    pre {
      background-color: #f4f4f4;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }

    .objectives {
      background-color: #f0f7ff;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
    }

    .method-card {
      background-color: #fff;
      border: 2px solid #e0e0e0;
      border-radius: 8px;
      padding: 15px;
      margin: 15px 0;
    }

    .method-card h4 {
      margin-top: 0;
      color: #2c3e50;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
    }
  </style>
</head>

<body>
  <h1>Week 9: Input Attribution and Saliency Methods</h1>

  <div class="objectives">
    <h3>Learning Objectives</h3>
    <ul>
      <li>Understand what input attribution is and when to use it vs other interpretability methods</li>
      <li>Master gradient-based attribution methods (saliency, integrated gradients, DeepLIFT)</li>
      <li>Apply perturbation-based methods (LIME, SHAP, ablation) to LLMs</li>
      <li>Use attention-based attribution (attention rollout, attention flow)</li>
      <li>Work effectively with the Inseq library for sequence generation models</li>
      <li>Critically evaluate attribution methods and understand their limitations</li>
      <li>Address technical challenges (baseline selection, gradient saturation, computational cost)</li>
      <li>Apply attribution to debug model predictions and analyze your project concepts</li>
      <li>Integrate attribution with previous weeks' methods (circuits, probes, SAEs, causal validation)</li>
    </ul>
  </div>

  <h2>1. What is Input Attribution?</h2>

  <h3>The Core Question</h3>
  <p>
    <strong>Input attribution</strong> answers: <em>"Which parts of the input were most important for the model's
      output?"</em>
  </p>

  <p>
    For an LLM generating text, attribution identifies which input tokens influenced each output token. This helps us:
  </p>
  <ul>
    <li><strong>Debug predictions:</strong> Why did the model generate this specific word?</li>
    <li><strong>Detect bias:</strong> Is the model relying on demographic features inappropriately?</li>
    <li><strong>Validate understanding:</strong> Does the model attend to the right context?</li>
    <li><strong>Guide interventions:</strong> Which inputs should we modify to change behavior?</li>
  </ul>

  <h3>Attribution vs Other Interpretability Methods</h3>

  <div class="comparison-box">
    <table>
      <tr>
        <th>Method</th>
        <th>Question Answered</th>
        <th>Granularity</th>
      </tr>
      <tr>
        <td><strong>Attribution</strong><br>(This week)</td>
        <td>Which <em>inputs</em> matter?</td>
        <td>Input tokens</td>
      </tr>
      <tr>
        <td><strong>Circuits</strong><br>(Week 5)</td>
        <td>Which <em>components</em> compute the output?</td>
        <td>Attention heads, MLPs</td>
      </tr>
      <tr>
        <td><strong>Probes</strong><br>(Week 6)</td>
        <td>What <em>information</em> is encoded?</td>
        <td>Layer activations</td>
      </tr>
      <tr>
        <td><strong>SAEs</strong><br>(Week 7)</td>
        <td>What <em>features</em> are represented?</td>
        <td>Sparse feature directions</td>
      </tr>
      <tr>
        <td><strong>Causal Validation</strong><br>(Week 8)</td>
        <td>Are findings <em>causal</em>?</td>
        <td>Variable-level</td>
      </tr>
    </table>
  </div>

  <p>
    <strong>Key insight:</strong> Attribution is <em>input-level</em> explanation. It tells you <em>what</em> the model
    used, not <em>how</em> it was processed internally. Combine attribution with circuit analysis (Week 5) to get the
    full picture.
  </p>

  <h2>2. Gradient-Based Attribution Methods</h2>

  <p>
    Gradient-based methods use the model's gradients to measure input importance. Intuitively: <em>if changing an input
      token would change the output a lot (high gradient), that token is important</em>.
  </p>

  <h3>2.1 Saliency Maps (Vanilla Gradients)</h3>

  <div class="method-card">
    <h4>Method: Saliency Maps</h4>
    <p><strong>Idea:</strong> The gradient magnitude indicates importance.</p>
    <p><strong>Formula:</strong> \( \text{Saliency}(x_i) = \left| \frac{\partial f(x)}{\partial x_i} \right| \)</p>
    <p><strong>Pros:</strong> Simple, fast, easy to implement</p>
    <p><strong>Cons:</strong> Suffers from gradient saturation (near-zero gradients in deep networks)</p>
  </div>

  <div class="example-box">
    <p><strong>Example:</strong></p>
    <p><em>Input:</em> "The cat sat on the mat"</p>
    <p><em>Output:</em> "because" (predicted next token)</p>
    <p><em>Saliency scores:</em> [0.02, <strong>0.35</strong>, 0.15, 0.08, <strong>0.40</strong>, 0.12]</p>
    <p><em>Interpretation:</em> "cat" and "mat" have highest gradients → most influential for predicting "because"</p>
  </div>

  <div class="warning-box">
    <p><strong>Problem: Gradient Saturation</strong></p>
    <p>
      Deep networks with ReLU/sigmoid activations often have near-zero gradients even when inputs are important. This
      happens because the function flattens (saturates) in certain regions.
    </p>
    <p><em>Solution:</em> Use Integrated Gradients or other methods that accumulate gradients along a path.</p>
  </div>

  <h3>2.2 Input × Gradient</h3>

  <div class="method-card">
    <h4>Method: Input × Gradient</h4>
    <p><strong>Idea:</strong> Weight gradients by input magnitude to get better signal.</p>
    <p><strong>Formula:</strong> \( \text{Attribution}(x_i) = x_i \cdot \frac{\partial f(x)}{\partial x_i} \)</p>
    <p><strong>Pros:</strong> Better handles zero-input cases than vanilla gradients</p>
    <p><strong>Cons:</strong> Still affected by saturation</p>
  </div>

  <h3>2.3 Integrated Gradients (IG)</h3>

  <div class="method-card">
    <h4>Method: Integrated Gradients</h4>
    <p><strong>Idea:</strong> Accumulate gradients along a straight line from a baseline to the input.</p>
    <p><strong>Formula:</strong>
      \[
      \text{IG}(x_i) = (x_i - x_i') \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha \cdot (x - x'))}{\partial x_i}
      \, d\alpha
      \]
      where \( x' \) is a baseline input (e.g., all zeros or padding tokens).
    </p>
    <p><strong>Pros:</strong> Theoretically sound (satisfies completeness axiom), mitigates saturation</p>
    <p><strong>Cons:</strong> Computationally expensive, baseline-dependent, assumes linear path</p>
  </div>

  <div class="info-box">
    <h4>Why Integrated Gradients Works</h4>
    <p>
      By integrating gradients along a path, IG avoids the saturation problem of vanilla gradients. Even if the
      gradient is zero at the input, IG captures importance by looking at gradients throughout the interpolation.
    </p>
    <p>
      <strong>Completeness property:</strong> The sum of attributions equals the difference in model output between
      input and baseline:
      \[
      \sum_i \text{IG}(x_i) = f(x) - f(x')
      \]
    </p>
  </div>

  <h3>2.4 The Baseline Selection Problem</h3>

  <p>
    A critical challenge for IG: <strong>what baseline should we use?</strong>
  </p>

  <div class="warning-box">
    <p><strong>Baseline choices for LLMs:</strong></p>
    <ul>
      <li><strong>Zero embeddings:</strong> Common but may not be semantically meaningful</li>
      <li><strong>Padding tokens:</strong> Model-specific, represents "no information"</li>
      <li><strong>Mask tokens:</strong> For masked language models (BERT-style)</li>
      <li><strong>Average embeddings:</strong> Represents "typical" input</li>
      <li><strong>Random text:</strong> Contrast against unrelated content</li>
    </ul>
    <p>
      <strong>Impact:</strong> Baseline choice can dramatically change attribution scores. Always justify your baseline!
    </p>
  </div>

  <h3>2.5 Uniform Discretized Integrated Gradients (UDIG)</h3>

  <div class="method-card">
    <h4>Method: UDIG (2024)</h4>
    <p><strong>Problem with standard IG:</strong> Linear interpolation through embedding space doesn't respect the
      discrete, linguistic nature of words. Intermediate points may not correspond to real words.</p>
    <p><strong>Solution:</strong> Use a <em>nonlinear path</em> that stays closer to actual word embeddings.</p>
    <p><strong>Result:</strong> Better performance on NLP tasks (sentiment, QA) compared to standard IG.</p>
    <p><strong>When to use:</strong> For language models where discrete token structure matters.</p>
  </div>

  <h3>2.6 Other Gradient Methods</h3>

  <ul>
    <li><strong>DeepLIFT:</strong> Compares activations to reference activations, decomposes prediction differences</li>
    <li><strong>GradientSHAP:</strong> Combines gradients with Shapley value sampling</li>
    <li><strong>Layer-wise methods:</strong> LayerIntegratedGradients, LayerGradientXActivation for internal layers</li>
  </ul>

  <h2>3. Perturbation-Based Attribution Methods</h2>

  <p>
    Perturbation methods <strong>modify inputs</strong> and observe how outputs change. No gradients needed—purely
    empirical.
  </p>

  <h3>3.1 Occlusion / Ablation</h3>

  <div class="method-card">
    <h4>Method: Token Ablation</h4>
    <p><strong>Idea:</strong> Remove (or mask) each token and measure output change.</p>
    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Run model on full input → get output probability \( p \)</li>
      <li>For each token \( i \):
        <ul>
          <li>Remove or mask token \( i \)</li>
          <li>Run model → get new probability \( p_i \)</li>
          <li>Attribution \( = p - p_i \) (drop in probability)</li>
        </ul>
      </li>
    </ol>
    <p><strong>Pros:</strong> Intuitive, model-agnostic, no gradient computation</p>
    <p><strong>Cons:</strong> Computationally expensive (\( O(n) \) forward passes for \( n \) tokens), may create
      unnatural inputs</p>
  </div>

  <h3>3.2 LIME (Local Interpretable Model-Agnostic Explanations)</h3>

  <div class="method-card">
    <h4>Method: LIME</h4>
    <p><strong>Idea:</strong> Fit a simple linear model <em>locally</em> around the input to approximate the model's
      behavior.</p>
    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Generate perturbed inputs by randomly masking tokens</li>
      <li>Run model on perturbed inputs to get outputs</li>
      <li>Fit a weighted linear model: \( g(x') \approx f(x) \) where \( x' \) is in the perturbation neighborhood</li>
      <li>Linear coefficients = token importance</li>
    </ol>
    <p><strong>Pros:</strong> Model-agnostic, interpretable coefficients</p>
    <p><strong>Cons:</strong> Requires many model calls, local approximation may be inaccurate, random sampling
      variability</p>
  </div>

  <h3>3.3 SHAP (Shapley Additive Explanations)</h3>

  <div class="method-card">
    <h4>Method: SHAP</h4>
    <p><strong>Idea:</strong> Use game-theoretic Shapley values to assign fair credit to each token.</p>
    <p><strong>Formula:</strong> For each token \( i \), compute contribution by averaging over all possible subsets:
      \[
      \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! (|N| - |S| - 1)!}{|N|!} [f(S \cup \{i\}) - f(S)]
      \]
    </p>
    <p><strong>Pros:</strong> Theoretically grounded (unique solution satisfying fairness axioms), consistent</p>
    <p><strong>Cons:</strong> Exponentially expensive to compute exactly (approximations needed), requires defining
      "coalition" semantics for tokens</p>
  </div>

  <div class="warning-box">
    <h4>Critical Limitation: Additivity Assumption</h4>
    <p>
      Recent research (2024) shows that <strong>transformers cannot represent additive models</strong> due to their
      architecture (attention, LayerNorm). This casts doubt on LIME and SHAP's applicability, which assume local
      additivity.
    </p>
    <p>
      <strong>Implication:</strong> Use LIME/SHAP cautiously for transformers. Validate with other methods.
    </p>
  </div>

  <h3>3.4 ReAGent (Replace with Alternatives)</h3>

  <div class="method-card">
    <h4>Method: ReAGent</h4>
    <p><strong>Idea:</strong> Replace each token with plausible alternatives (e.g., from a masked LM) rather than just
      removing it.</p>
    <p><strong>Advantage:</strong> More natural perturbations than simple masking.</p>
    <p><strong>When to use:</strong> When you want to measure importance without creating out-of-distribution inputs.
    </p>
  </div>

  <h2>4. Attention-Based Attribution</h2>

  <p>
    Can we use <strong>attention weights</strong> as attribution? The answer is <em>complicated</em>.
  </p>

  <h3>4.1 Raw Attention Weights</h3>

  <div class="warning-box">
    <h4>Problem: Attention is Not Explanation</h4>
    <p>
      In transformers, <strong>information from different tokens gets increasingly mixed</strong> across layers through
      attention and residual connections. By the final layer, it's unclear which input tokens contributed to a
      representation.
    </p>
    <p>
      Raw attention weights show what the model <em>attends to</em> in a single layer, but they don't track information
      flow from input to output across the full network.
    </p>
  </div>

  <p>
    <strong>When raw attention is useful:</strong>
  </p>
  <ul>
    <li>Single-layer analysis (e.g., "does this head attend to previous token?")</li>
    <li>Qualitative exploration</li>
    <li>Sanity checks (e.g., "is the model attending to padding?")</li>
  </ul>

  <h3>4.2 Attention Rollout</h3>

  <div class="method-card">
    <h4>Method: Attention Rollout (Abnar & Zuidema, 2020)</h4>
    <p><strong>Idea:</strong> Propagate attention weights through layers, accounting for residual connections.</p>
    <p><strong>Assumption:</strong> Token identities are linearly combined based on attention weights.</p>
    <p><strong>Formula:</strong> Roll out attention from layer \( \ell \) to input:
      \[
      \tilde{A}^{(\ell)} = A^{(\ell)} \cdot \tilde{A}^{(\ell-1)}
      \]
      where \( A^{(\ell)} \) is attention at layer \( \ell \), adjusted for residual connections.
    </p>
    <p><strong>Pros:</strong> Accounts for multi-layer information flow</p>
    <p><strong>Cons:</strong> Linear approximation may be inaccurate for nonlinear models</p>
  </div>

  <h3>4.3 Attention Flow</h3>

  <div class="method-card">
    <h4>Method: Attention Flow (Abnar & Zuidema, 2020)</h4>
    <p><strong>Idea:</strong> Model the attention graph as a <em>flow network</em> and compute maximum flow from each
      input token to the output.</p>
    <p><strong>Algorithm:</strong> Use max-flow algorithms (e.g., Ford-Fulkerson) with attention weights as edge
      capacities.</p>
    <p><strong>Pros:</strong> Captures multi-path information flow</p>
    <p><strong>Cons:</strong> Computationally more expensive than rollout</p>
  </div>

  <div class="comparison-box">
    <h4>Attention Rollout vs Attention Flow</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>Attention Rollout</th>
        <th>Attention Flow</th>
      </tr>
      <tr>
        <td><strong>Assumption</strong></td>
        <td>Linear mixing of identities</td>
        <td>Flow network</td>
      </tr>
      <tr>
        <td><strong>Computation</strong></td>
        <td>Matrix multiplication (fast)</td>
        <td>Max-flow algorithm (slower)</td>
      </tr>
      <tr>
        <td><strong>Multi-path</strong></td>
        <td>Averages paths</td>
        <td>Finds bottlenecks</td>
      </tr>
      <tr>
        <td><strong>Use case</strong></td>
        <td>Quick approximate attribution</td>
        <td>Detailed flow analysis</td>
      </tr>
    </table>
    <p><strong>Key finding:</strong> Both methods correlate better with ablation and gradient-based methods than raw
      attention.</p>
  </div>

  <h2>5. The Inseq Library: Practical Tool</h2>

  <p>
    <a href="https://inseq.org/" target="_blank">Inseq</a> is a Python library that makes attribution analysis
    accessible for sequence generation models.
  </p>

  <h3>5.1 What Inseq Provides</h3>

  <ul>
    <li><strong>Unified API</strong> for gradient, perturbation, and attention methods</li>
    <li><strong>Model support:</strong> GPT-2, GPT-NeoX, Llama, BLOOM, T5, BART, Marian MT (via HuggingFace)</li>
    <li><strong>Methods supported:</strong>
      <ul>
        <li>Gradient-based: Saliency, Input×Gradient, Integrated Gradients, DeepLIFT, GradientSHAP, UDIG</li>
        <li>Attention-based: Attention weights, value zeroing</li>
        <li>Perturbation-based: Occlusion, LIME, ReAGent</li>
      </ul>
    </li>
    <li><strong>High-level interface:</strong> <code>model.attribute(input_text, target_text, method="integrated_gradients")</code></li>
    <li><strong>Visualization tools</strong> for displaying attributions</li>
    <li><strong>Step-level attribution</strong> for generation (attribute each output token separately)</li>
  </ul>

  <h3>5.2 Example Usage</h3>

  <pre><code>import inseq

# Load model
model = inseq.load_model("gpt2", "integrated_gradients")

# Run attribution
result = model.attribute(
    "The cat sat on the",
    n_steps=5,  # Generate 5 tokens
    step_scores=["probability"]
)

# Visualize
result.show()
</code></pre>

  <h3>5.3 Key Pedagogical Examples from Inseq</h3>

  <div class="example-box">
    <p><strong>Example 1: Gender Bias in Machine Translation</strong></p>
    <p>
      <em>Task:</em> Translate "The doctor asked the nurse to help" from English to gendered language (e.g., Italian).
    </p>
    <p>
      <em>Question:</em> Does the model assign gendered pronouns based on stereotypes?
    </p>
    <p>
      <em>Attribution reveals:</em> Which input words drive gendered predictions ("doctor" vs "nurse").
    </p>
  </div>

  <div class="example-box">
    <p><strong>Example 2: Factual Knowledge Localization in GPT-2</strong></p>
    <p>
      <em>Input:</em> "The capital of France is"
    </p>
    <p>
      <em>Output:</em> "Paris"
    </p>
    <p>
      <em>Attribution reveals:</em> Model strongly attends to "France" (expected) but may also use "capital" heavily,
      showing it recognizes the prompt structure.
    </p>
  </div>

  <h2>6. Comparing Attribution Methods</h2>

  <p>
    Different methods can give different results. How do we know which to trust?
  </p>

  <h3>6.1 Validation Techniques</h3>

  <div class="info-box">
    <p><strong>1. Perturbation Test (Ground Truth)</strong></p>
    <p>
      Remove tokens with high attribution scores. The output should change more than removing low-attribution tokens.
    </p>
    <p><strong>Metric:</strong> Correlation between attribution scores and output change when tokens are removed.</p>
  </div>

  <div class="info-box">
    <p><strong>2. Sanity Checks</strong></p>
    <ul>
      <li><strong>Random model test:</strong> Attributions should differ between trained and random models</li>
      <li><strong>Random label test:</strong> Attributions should differ for different target labels</li>
      <li><strong>Completeness test:</strong> Sum of attributions should match output difference (for IG)</li>
    </ul>
  </div>

  <div class="info-box">
    <p><strong>3. Cross-Method Agreement</strong></p>
    <p>
      Check if multiple methods (e.g., IG, LIME, ablation) agree on which tokens are important. High agreement
      increases confidence.
    </p>
  </div>

  <h3>6.2 Method Comparison Summary</h3>

  <table>
    <tr>
      <th>Method</th>
      <th>Speed</th>
      <th>Accuracy</th>
      <th>Interpretability</th>
      <th>LLM-Specific Issues</th>
    </tr>
    <tr>
      <td><strong>Saliency</strong></td>
      <td>⚡ Fast</td>
      <td>⚠️ Saturation</td>
      <td>✓ Clear</td>
      <td>Unreliable in deep networks</td>
    </tr>
    <tr>
      <td><strong>Integrated Gradients</strong></td>
      <td>⚠️ Slow</td>
      <td>✓✓ Good</td>
      <td>✓ Clear</td>
      <td>Baseline-dependent, linear path</td>
    </tr>
    <tr>
      <td><strong>UDIG</strong></td>
      <td>⚠️ Slow</td>
      <td>✓✓✓ Best</td>
      <td>✓ Clear</td>
      <td>Designed for discrete inputs</td>
    </tr>
    <tr>
      <td><strong>Ablation</strong></td>
      <td>⚠️⚠️ Very Slow</td>
      <td>✓✓✓ Ground truth</td>
      <td>✓✓ Very clear</td>
      <td>Unnatural inputs (gaps)</td>
    </tr>
    <tr>
      <td><strong>LIME</strong></td>
      <td>⚠️⚠️ Very Slow</td>
      <td>⚠️ Variable</td>
      <td>✓✓ Linear model</td>
      <td>Additivity assumption violated</td>
    </tr>
    <tr>
      <td><strong>SHAP</strong></td>
      <td>⚠️⚠️⚠️ Extremely Slow</td>
      <td>✓ Theoretically sound</td>
      <td>✓✓ Game-theoretic</td>
      <td>Additivity assumption violated</td>
    </tr>
    <tr>
      <td><strong>Attention Rollout</strong></td>
      <td>⚡⚡ Very Fast</td>
      <td>⚠️ Approximate</td>
      <td>⚠️ Hard to interpret</td>
      <td>Linear approximation</td>
    </tr>
    <tr>
      <td><strong>Attention Flow</strong></td>
      <td>⚡ Fast</td>
      <td>✓ Better than rollout</td>
      <td>⚠️ Hard to interpret</td>
      <td>Flow network assumption</td>
    </tr>
  </table>

  <h3>6.3 Recommendations</h3>

  <ul>
    <li><strong>For quick exploration:</strong> Saliency or Attention Rollout</li>
    <li><strong>For reliable attribution:</strong> Integrated Gradients (or UDIG for NLP)</li>
    <li><strong>For validation:</strong> Ablation (ground truth)</li>
    <li><strong>For publication:</strong> Use multiple methods, report agreement</li>
    <li><strong>For transformers:</strong> Prefer gradient-based over LIME/SHAP (additivity issues)</li>
  </ul>

  <h2>7. Technical Challenges and Solutions</h2>

  <h3>7.1 Gradient Saturation</h3>
  <p><strong>Problem:</strong> Gradients near zero despite input importance.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use Integrated Gradients (accumulates over path)</li>
    <li>Try Input×Gradient (scales by input magnitude)</li>
    <li>For circuit discovery: Use EAP-GP (adaptive path avoids saturated regions)</li>
  </ul>

  <h3>7.2 Computational Cost</h3>
  <p><strong>Problem:</strong> IG requires ~50-300 forward passes; SHAP/LIME even more.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use fewer interpolation steps (trade accuracy for speed)</li>
    <li>Attribute only critical tokens (e.g., content words, not punctuation)</li>
    <li>Use faster methods (saliency, attention) for exploration, then validate with IG</li>
    <li>Parallelize across GPUs</li>
  </ul>

  <h3>7.3 Long Sequences</h3>
  <p><strong>Problem:</strong> Attributing 1000+ token inputs is expensive and overwhelming.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Aggregate attributions (e.g., by sentence, by entity)</li>
    <li>Focus on specific output tokens of interest</li>
    <li>Use sliding windows for local context</li>
  </ul>

  <h3>7.4 Discrete vs Continuous Inputs</h3>
  <p><strong>Problem:</strong> Text is discrete, but IG assumes continuous interpolation.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use UDIG (nonlinear path respecting discrete structure)</li>
    <li>Apply IG to embeddings (continuous) but interpret results carefully</li>
    <li>Consider perturbation methods (naturally discrete)</li>
  </ul>

  <h2>8. Applications to Your Research</h2>

  <h3>8.1 Debugging Model Predictions</h3>

  <div class="example-box">
    <p><strong>Scenario:</strong> Your model incorrectly predicts subject-verb agreement.</p>
    <p><em>Input:</em> "The key to the cabinets <strong>are</strong>" (incorrect, should be "is")</p>
    <p><strong>Use attribution to investigate:</strong></p>
    <ol>
      <li>Run IG to see which input tokens influenced "are" prediction</li>
      <li>If attribution is high on "cabinets" (distractor), model is attending to wrong noun</li>
      <li>Guides intervention: Need to strengthen subject tracking (relate to Week 5 circuits)</li>
    </ol>
  </div>

  <h3>8.2 Analyzing Your Concept</h3>

  <p>
    For your project concept, attribution helps answer:
  </p>
  <ul>
    <li><strong>Which input features trigger your concept?</strong> (e.g., for "politeness," which words are most
      important?)</li>
    <li><strong>Are there spurious correlations?</strong> (e.g., does "musical key" attribution rely on irrelevant
      cues?)</li>
    <li><strong>How does context matter?</strong> (e.g., does the concept depend on distant words?)</li>
  </ul>

  <h3>8.3 Guiding Circuit Discovery</h3>

  <p>
    <strong>Workflow combining Week 5 and Week 9:</strong>
  </p>
  <ol>
    <li><strong>Attribution (Week 9):</strong> Identify which input tokens matter for your concept</li>
    <li><strong>Attention analysis:</strong> Which heads attend to those important tokens?</li>
    <li><strong>Path patching (Week 5):</strong> Test if those heads causally compute the concept</li>
    <li><strong>Validation (Week 8):</strong> Use IIA to confirm the circuit</li>
  </ol>

  <h2>9. Integration with Previous Weeks</h2>

  <h3>Connecting Attribution to Other Methods</h3>

  <table>
    <tr>
      <th>Previous Week</th>
      <th>How Attribution Helps</th>
    </tr>
    <tr>
      <td><strong>Week 5: Circuits</strong></td>
      <td>
        Use attribution to identify which input tokens activate your circuit.<br>
        Example: If a circuit computes induction, attribution should show high scores on the repeated token.
      </td>
    </tr>
    <tr>
      <td><strong>Week 6: Probes</strong></td>
      <td>
        Compare probe predictions with attribution. If a probe detects a feature, attribution should show which inputs
        provided that information.<br>
        Example: Probe detects "plural subject" → attribution should highlight the plural noun.
      </td>
    </tr>
    <tr>
      <td><strong>Week 7: SAEs</strong></td>
      <td>
        For an SAE feature representing your concept, use attribution to see which inputs activate it.<br>
        Example: "Politeness feature" activates → attribution reveals which politeness markers (e.g., "please," "thank
        you") trigger it.
      </td>
    </tr>
    <tr>
      <td><strong>Week 8: Causal Validation</strong></td>
      <td>
        Attribution ≠ causation! Validate high-attribution tokens with interventions.<br>
        Example: Attribution says "France" is important → intervene by changing "France" to "Italy" and verify output
        changes to "Rome."
      </td>
    </tr>
  </table>

  <h2>10. Limitations and Skepticism</h2>

  <div class="warning-box">
    <h4>Critical Limitations to Remember</h4>
    <ul>
      <li><strong>Correlation ≠ Causation:</strong> High attribution doesn't prove a token is <em>necessary</em>.
        Always validate with interventions (Week 8).</li>
      <li><strong>Additivity assumptions:</strong> LIME and SHAP assume local additivity, which transformers violate.
        Results may be misleading.</li>
      <li><strong>Baseline dependence:</strong> IG results change with baseline choice. Report your baseline and justify
        it.</li>
      <li><strong>Attention ≠ Explanation:</strong> Raw attention weights are insufficient for full attributions due to
        information mixing.</li>
      <li><strong>Out-of-distribution perturbations:</strong> Removing tokens creates unnatural inputs. Model behavior
        on these may not reflect normal operation.</li>
      <li><strong>Single-path assumptions:</strong> Standard IG uses a straight line, which may not capture the true
        path through representation space.</li>
    </ul>
  </div>

  <h3>When Attribution Fails</h3>

  <ul>
    <li><strong>Highly distributed concepts:</strong> If your concept depends on complex interactions of many tokens,
      attribution may not isolate individual contributions clearly.</li>
    <li><strong>Nonlinear interactions:</strong> Token A + Token B together matter, but individually they don't. Linear
      attribution methods will underestimate their importance.</li>
    <li><strong>Internal reasoning:</strong> If the model performs multi-step reasoning, input attribution alone won't
      reveal the internal steps (need circuit analysis).</li>
  </ul>

  <h2>11. Best Practices for Your Research</h2>

  <div class="info-box">
    <h3>Checklist for Using Attribution in Your Project</h3>
    <ul>
      <li>✓ <strong>Use multiple methods</strong> and report agreement</li>
      <li>✓ <strong>Validate with ablation</strong> (ground truth test)</li>
      <li>✓ <strong>Run sanity checks</strong> (random model, random label)</li>
      <li>✓ <strong>Justify baseline choice</strong> (for IG)</li>
      <li>✓ <strong>Report computational costs</strong></li>
      <li>✓ <strong>Visualize attributions</strong> clearly</li>
      <li>✓ <strong>Combine with causal validation</strong> (Week 8 IIA)</li>
      <li>✓ <strong>Aggregate for long sequences</strong></li>
      <li>✓ <strong>Don't over-interpret</strong> (attribution shows correlation, not causation)</li>
      <li>✓ <strong>Compare to human intuition</strong> (does attribution make sense?)</li>
    </ul>
  </div>

  <h2>12. Summary and Next Steps</h2>

  <h3>Key Takeaways</h3>

  <ul>
    <li><strong>Input attribution</strong> identifies which inputs matter for outputs—complementary to circuit/probe/SAE
      analysis</li>
    <li><strong>Gradient-based methods</strong> (IG, UDIG) are reliable for LLMs when used carefully</li>
    <li><strong>Perturbation-based methods</strong> (ablation) provide ground truth but are computationally expensive
    </li>
    <li><strong>Attention-based methods</strong> (rollout, flow) are fast but approximate</li>
    <li><strong>Inseq library</strong> makes attribution accessible for sequence generation models</li>
    <li><strong>Always validate</strong> attributions with multiple methods and causal interventions</li>
    <li><strong>Critical limitations:</strong> Baseline dependence, additivity assumptions, correlation ≠ causation</li>
  </ul>

  <h3>For Your Research Project</h3>

  <ol>
    <li>Use attribution to identify <strong>which input features activate your concept</strong></li>
    <li>Combine with <strong>circuit analysis</strong> (Week 5) to understand <em>how</em> those inputs are processed
    </li>
    <li>Validate findings with <strong>causal interventions</strong> (Week 8)</li>
    <li>Report attribution results in your paper with appropriate caveats</li>
  </ol>

  <h3>Looking Ahead</h3>

  <p>
    <strong>Week 10:</strong> Skepticism and interpretability illusions—when interpretability methods mislead us, and
    how to be rigorous.
  </p>

  <h2>References & Resources</h2>

  <h3>Core Papers</h3>
  <ul>
    <li><strong>Inseq Library:</strong> Sarti et al. (2023). "Inseq: An Interpretability Toolkit for Sequence
      Generation Models." <a href="https://arxiv.org/abs/2302.13942" target="_blank">arXiv:2302.13942</a></li>
    <li><strong>Integrated Gradients:</strong> Sundararajan et al. (2017). "Axiomatic Attribution for Deep Networks."
      ICML. <a href="https://arxiv.org/abs/1703.01365" target="_blank">arXiv:1703.01365</a></li>
    <li><strong>UDIG:</strong> Recent (2024). "Uniform Discretized Integrated Gradients." <a
        href="https://arxiv.org/abs/2412.03886" target="_blank">arXiv:2412.03886</a></li>
    <li><strong>Attention Flow:</strong> Abnar & Zuidema (2020). "Quantifying Attention Flow in Transformers." ACL. <a
        href="https://aclanthology.org/2020.acl-main.385/" target="_blank">ACL Anthology</a></li>
    <li><strong>LIME:</strong> Ribeiro et al. (2016). "Why Should I Trust You?" KDD. <a
        href="https://arxiv.org/abs/1602.04938" target="_blank">arXiv:1602.04938</a></li>
    <li><strong>SHAP:</strong> Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions." NIPS. <a
        href="https://arxiv.org/abs/1705.07874" target="_blank">arXiv:1705.07874</a></li>
    <li><strong>Baseline Selection:</strong> Sturmfels et al. (2020). "Visualizing the Impact of Feature Attribution
      Baselines." Distill. <a href="https://distill.pub/2020/attribution-baselines/"
        target="_blank">distill.pub</a></li>
  </ul>

  <h3>Tools & Libraries</h3>
  <ul>
    <li><strong>Inseq:</strong> <a href="https://inseq.org/" target="_blank">inseq.org</a> | <a
        href="https://github.com/inseq-team/inseq" target="_blank">GitHub</a></li>
    <li><strong>Captum (PyTorch):</strong> <a href="https://captum.ai/" target="_blank">captum.ai</a> | <a
        href="https://captum.ai/tutorials/Llama2_LLM_Attribution" target="_blank">LLM Tutorial</a></li>
    <li><strong>Transformer Interpret:</strong> <a href="https://github.com/cdpierse/transformers-interpret"
        target="_blank">GitHub</a></li>
  </ul>

  <h3>Supplementary Reading</h3>
  <ul>
    <li>Attention is not Explanation: Jain & Wallace (2019). <a href="https://arxiv.org/abs/1902.10186"
        target="_blank">arXiv:1902.10186</a></li>
    <li>Attention is not not Explanation: Wiegreffe & Pinter (2019). <a href="https://arxiv.org/abs/1908.04626"
        target="_blank">arXiv:1908.04626</a></li>
    <li>Sanity Checks for Saliency Maps: Adebayo et al. (2018). NIPS. <a href="https://arxiv.org/abs/1810.03292"
        target="_blank">arXiv:1810.03292</a></li>
    <li>Transformers Can't Represent Additive Models: Recent research on LIME/SHAP limitations for transformers (2024).
    </li>
  </ul>

</body>

</html>