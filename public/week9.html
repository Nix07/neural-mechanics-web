<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 9: Training Dynamics & Model Editing - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 15px 0;
    }

    th,
    td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: left;
    }

    th {
      background-color: #f0f0f0;
    }

    .info-box {
      background-color: #e8f4fc;
      border-left: 4px solid #0055a4;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .paper-card {
      background-color: #fff;
      border: 2px solid #0055a4;
      border-radius: 8px;
      padding: 20px;
      margin: 20px 0;
    }

    .paper-card h4 {
      margin-top: 0;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 9: Training Dynamics & Model Editing</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">&larr; Back to Course Home</a>
  </nav>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Understand how neural network mechanisms emerge during training</li>
      <li>Study phase transitions and grokking phenomena</li>
      <li>Track circuit formation across training checkpoints</li>
      <li>Apply interpretability methods to training trajectories</li>
      <li>Identify when your concept's representation emerges</li>
      <li>Understand why some mechanisms form suddenly vs gradually</li>
      <li>Connect training dynamics to final model behavior</li>
    </ul>
  </div>

  <div class="readings-section">
    <h2>Required Readings</h2>
    <ul class="reading-list">
      <li>
        <strong><a href="https://arxiv.org/abs/2301.05217" target="_blank">Progress Measures for Grokking via Mechanistic Interpretability</a></strong><br>
        <em>Nanda, Chan, Lieberum, Smith &amp; Steinhardt (2023). Fully reverse-engineers how transformers learn modular addition, decomposing grokking into three phases.</em>
      </li>
      <li>
        <strong><a href="https://arxiv.org/abs/2210.07229" target="_blank">Mass-Editing Memory in a Transformer</a></strong><br>
        <em>Meng, Sharma, Andonian, Belinkov &amp; Bau (2023). MEMIT: Extends ROME to edit thousands of facts simultaneously.</em>
      </li>
      <li>
        <strong><a href="https://arxiv.org/abs/2512.16902" target="_blank">In-Context Algebra</a></strong><br>
        <em>Todd, Wang, Gurnee &amp; Bau (2025). Studies how transformers learn mechanisms for in-context algebraic reasoning with pure variables.</em>
      </li>
    </ul>
    <h3>Supplementary Readings</h3>
    <ul class="reading-list">
      <li>
        <strong><a href="https://arxiv.org/abs/2404.07129" target="_blank">What Needs to Go Right for an Induction Head?</a></strong><br>
        <em>Edelman, Gurnee &amp; Edelman (2024). Optogenetics-inspired causal framework for studying how induction heads form.</em>
      </li>
    </ul>
  </div>

  <h2>1. Why Study Training Dynamics?</h2>

  <h3>The Formation Question</h3>
  <p>
    Weeks 1-8 studied your concept in a <strong>fully trained model</strong>. But critical questions remain:
  </p>

  <ul>
    <li><strong>When</strong> does your concept's circuit form during training?</li>
    <li><strong>Does it emerge</strong> suddenly (phase transition) or gradually?</li>
    <li><strong>Which components</strong> appear first? Which come later?</li>
    <li><strong>What training dynamics</strong> (loss curves, gradient patterns) coincide with emergence?</li>
    <li><strong>Can we predict</strong> when a capability will emerge from training dynamics?</li>
  </ul>

  <div class="key-insight">
    <p><strong>Why This Matters for Research:</strong></p>
    <p>
      Understanding <em>how</em> mechanisms form helps explain <em>why</em> they exist. Training dynamics reveal:
    </p>
    <ul>
      <li>What inductive biases lead models to learn your concept</li>
      <li>Whether your concept is a "natural" solution or coincidental</li>
      <li>How robust the mechanism is (sudden vs gradual = different stability)</li>
      <li>What minimal training is needed for the concept to appear</li>
    </ul>
  </div>

  <h3>Use Cases for Your Project</h3>

  <div class="example-box">
    <p><strong>Example: Induction Heads</strong></p>
    <p>
      Olsson et al. (2022) found induction heads emerge in a sharp phase transition around 2B tokens. Before: random attention. After: systematic pattern copying. This sudden emergence suggests induction is a "natural" circuit that SGD discovers reliably.
    </p>
  </div>

  <h2>2. Grokking: Delayed Generalization</h2>

  <h3>The Grokking Phenomenon</h3>

  <p>
    <strong>Grokking</strong> (Power et al., 2022) is when a model:
  </p>
  <ol>
    <li>Overfits training data quickly (100% train accuracy)</li>
    <li>Remains at chance on validation for many epochs</li>
    <li>Suddenly achieves perfect generalization after extended training</li>
  </ol>

  <div class="info-box">
    <p><strong>Key Observation:</strong> During the "grokking" phase, the model has already memorized the training data but is still learning to generalize. Something internal changes even though training loss is flat.</p>
  </div>

  <h3>Mechanistic Explanation</h3>

  <p>
    Nanda et al. (2023) "Progress measures for grokking via mechanistic interpretability" showed:
  </p>

  <ul>
    <li><strong>Two competing circuits:</strong> A "memorization circuit" (fast to learn, doesn't generalize) and a "generalizing circuit" (slow to learn, works on all data)</li>
    <li><strong>Circuit competition:</strong> Initially, memorization circuit dominates. Over time, weight decay and other regularization favor the generalizing circuit</li>
    <li><strong>Phase transition:</strong> When generalizing circuit becomes stronger, validation accuracy suddenly jumps</li>
  </ul>

  <div class="example-box">
    <p><strong>Experiment: Modular Addition</strong></p>
    <p>
      Task: Predict (a + b) mod 113
    </p>
    <p>
      <strong>Memorization circuit:</strong> Lookup table in early layers<br>
      <strong>Generalizing circuit:</strong> Fourier features that exploit group structure
    </p>
    <p>
      By tracking the strength of each circuit (via ablation) across checkpoints, Nanda et al. could predict exactly when grokking would occur—before it showed up in validation metrics!
    </p>
  </div>

  <h2>3. Phase Transitions in Learning</h2>

  <h3>Sudden vs Gradual Emergence</h3>

  <p>
    Not all mechanisms form gradually. Some appear in sharp <strong>phase transitions</strong>:
  </p>

  <table>
    <tr>
      <th>Mechanism Type</th>
      <th>Emergence Pattern</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>Simple features</td>
      <td>Gradual</td>
      <td>Edge detectors in CNNs</td>
    </tr>
    <tr>
      <td>Composed circuits</td>
      <td>Phase transition</td>
      <td>Induction heads (Olsson et al.)</td>
    </tr>
    <tr>
      <td>Algorithm discovery</td>
      <td>Phase transition (grokking)</td>
      <td>Modular arithmetic (Nanda et al.)</td>
    </tr>
  </table>

  <h3>The Induction Head Phase Transition</h3>

  <div class="paper-card">
    <h4>Case Study: Olsson et al. (2022) "In-context Learning and Induction Heads"</h4>

    <p><strong>Finding:</strong> Induction heads (the circuit enabling in-context learning) form suddenly during training</p>

    <p><strong>Timeline:</strong></p>
    <ul>
      <li><strong>Before 2B tokens:</strong> No induction behavior, random attention patterns</li>
      <li><strong>Around 2B tokens:</strong> Sharp increase in induction score over ~50M tokens</li>
      <li><strong>After 2.5B tokens:</strong> Fully formed induction heads, strong ICL performance</li>
    </ul>

    <p><strong>What emerges:</strong></p>
    <ol>
      <li><strong>Previous token heads:</strong> Attend to previous occurrence of current token</li>
      <li><strong>Induction heads:</strong> Attend to token after the previous occurrence</li>
      <li><strong>Composition:</strong> These heads compose via Q-K circuit</li>
    </ol>

    <p><strong>Key Insight:</strong> ICL ability correlates perfectly with induction head formation—suggesting induction heads <em>are</em> the mechanism for ICL, at least in small models.</p>
  </div>

  <h2>4. Methods: Tracking Mechanism Formation</h2>

  <h3>Checkpoint-Based Analysis</h3>

  <p>
    To study training dynamics, you need to apply your interpretability methods to models at different training stages:
  </p>

  <ol>
    <li><strong>Save checkpoints:</strong> Every N steps (e.g., every 1000 steps, or logarithmically spaced)</li>
    <li><strong>Apply methods:</strong> Run your Week 3-8 analyses on each checkpoint
      <ul>
        <li>Probing accuracy over time</li>
        <li>Circuit component strengths (ablation effects)</li>
        <li>Attention pattern analysis</li>
        <li>SAE feature activation patterns</li>
      </ul>
    </li>
    <li><strong>Plot trajectories:</strong> How do these metrics evolve?</li>
    <li><strong>Identify transitions:</strong> When do sudden changes occur?</li>
  </ol>

  <h3>Key Metrics to Track</h3>

  <table>
    <tr>
      <th>Metric</th>
      <th>What It Measures</th>
      <th>Interpretation</th>
    </tr>
    <tr>
      <td>Probe accuracy</td>
      <td>Linear readability of concept</td>
      <td>When does concept become linearly accessible?</td>
    </tr>
    <tr>
      <td>Ablation effect</td>
      <td>Causal importance of component</td>
      <td>When does component become necessary?</td>
    </tr>
    <tr>
      <td>Attention patterns</td>
      <td>Information routing</td>
      <td>When do specific attention heads form?</td>
    </tr>
    <tr>
      <td>Loss on concept-specific data</td>
      <td>Task-specific performance</td>
      <td>When does model learn the concept?</td>
    </tr>
    <tr>
      <td>Logit lens evolution</td>
      <td>Prediction development across layers</td>
      <td>How does processing change over training?</td>
    </tr>
  </table>

  <h3>Detecting Phase Transitions</h3>

  <div class="info-box">
    <p><strong>How to identify a phase transition:</strong></p>
    <ul>
      <li><strong>Sharp gradient:</strong> Metric changes rapidly over short training period</li>
      <li><strong>Temporal correlation:</strong> Multiple related metrics change simultaneously</li>
      <li><strong>Bifurcation point:</strong> Before = one behavior, after = qualitatively different</li>
      <li><strong>Reproducibility:</strong> Transition occurs at similar point across random seeds</li>
    </ul>
  </div>

  <h2>5. Circuit Formation Patterns</h2>

  <h3>Hierarchical Assembly</h3>

  <p>
    Complex circuits often form in stages:
  </p>

  <ol>
    <li><strong>Foundation components</strong> form first (e.g., attention heads that move information)</li>
    <li><strong>Processing components</strong> form next (e.g., MLPs that transform representations)</li>
    <li><strong>Composition</strong> emerges last (components wire together into full circuit)</li>
  </ol>

  <div class="example-box">
    <p><strong>Example: Induction Circuit Assembly</strong></p>
    <ol>
      <li><strong>Step 1 (~1.5B tokens):</strong> Previous token heads form</li>
      <li><strong>Step 2 (~2B tokens):</strong> Induction heads form</li>
      <li><strong>Step 3 (~2.2B tokens):</strong> Q-K composition circuit connects them</li>
    </ol>
    <p>
      Each stage builds on the previous. Without previous token heads, induction heads can't function.
    </p>
  </div>

  <h3>Redundancy and Pruning</h3>

  <p>
    Training dynamics often show:
  </p>

  <ul>
    <li><strong>Overproduction:</strong> Many components initially contribute to a task</li>
    <li><strong>Consolidation:</strong> Over training, contribution concentrates in fewer components</li>
    <li><strong>Specialization:</strong> Components become more selective/focused</li>
  </ul>

  <h2>6. Practical Considerations</h2>

  <h3>Computational Costs</h3>

  <div class="key-insight">
    <p><strong>Challenge:</strong> Full training runs are expensive. Strategies to reduce cost:</p>
    <ul>
      <li><strong>Use small models:</strong> GPT-2 Small, Pythia-160M for initial studies</li>
      <li><strong>Fine-tune instead of pretraining:</strong> Study how concept emerges during task-specific fine-tuning</li>
      <li><strong>Sparse checkpoints:</strong> Log-spaced checkpoints capture transitions with fewer saves</li>
      <li><strong>Targeted metrics:</strong> Don't run all analyses on all checkpoints—focus on key transitions</li>
    </ul>
  </div>

  <h3>Experimental Design</h3>

  <p><strong>For your project:</strong></p>

  <ol>
    <li><strong>Option A: Fine-tuning study</strong>
      <ul>
        <li>Start with pretrained model (e.g., GPT-2)</li>
        <li>Fine-tune on dataset emphasizing your concept</li>
        <li>Track when concept-specific circuit strengthens</li>
        <li>Faster and cheaper than pretraining from scratch</li>
      </ul>
    </li>
    <li><strong>Option B: Small-scale pretraining</strong>
      <ul>
        <li>Train small model (e.g., 2-layer transformer) from scratch</li>
        <li>On simplified task involving your concept</li>
        <li>Complete control over training dynamics</li>
        <li>Can run multiple seeds for robustness</li>
      </ul>
    </li>
    <li><strong>Option C: Using public checkpoints</strong>
      <ul>
        <li>Pythia suite provides checkpoints at many training steps</li>
        <li>No training cost—just analysis</li>
        <li>Limited to what checkpoints are available</li>
      </ul>
    </li>
  </ol>

  <h2>7. Connecting to Your Research</h2>

  <h3>Research Questions Enabled by Training Dynamics</h3>

  <ul>
    <li><strong>Necessity:</strong> Is your circuit necessary for the task, or just correlated? (If it forms when the capability emerges, it's likely necessary)</li>
    <li><strong>Sufficiency:</strong> Can early checkpoints solve the task if you manually strengthen the circuit? (Test via activation patching)</li>
    <li><strong>Robustness:</strong> Does the circuit form consistently across seeds? (Universal vs incidental)</li>
    <li><strong>Simplicity:</strong> Do simpler circuits form before more complex alternatives? (Inductive bias)</li>
  </ul>

  <h3>Integration with Previous Weeks</h3>

  <div class="comparison-box">
    <p><strong>Week-by-Week Application:</strong></p>
    <ul>
      <li><strong>Week 3 (Visualization):</strong> Track logit lens evolution—when does concept prediction sharpen?</li>
      <li><strong>Week 4 (Causal):</strong> Plot ablation effects over time—when do components become necessary?</li>
      <li><strong>Week 5 (Probes):</strong> Probe accuracy trajectory—when does linear representation emerge?</li>
      <li><strong>Week 7 (SAEs):</strong> Feature activation over training—do features sharpen or stay diffuse?</li>
      <li><strong>Week 8 (Circuits):</strong> Path patching at different checkpoints—when does composition happen?</li>
    </ul>
  </div>

  <h2>8. Case Studies</h2>

  <h3>Case 1: Modular Addition Grokking</h3>

  <div class="paper-card">
    <p><strong>Task:</strong> Learn (a + b) mod p for prime p</p>
    <p><strong>Observations:</strong></p>
    <ul>
      <li>Training accuracy: 100% at epoch 1000</li>
      <li>Validation accuracy: ~0% until epoch 10,000, then jumps to 100%</li>
    </ul>
    <p><strong>Mechanistic findings (Nanda et al.):</strong></p>
    <ul>
      <li>Memorization circuit (lookup table) forms by epoch 1000</li>
      <li>Fourier feature circuit (generalizable algorithm) forms gradually during epochs 1000-10,000</li>
      <li>Weight decay slowly shrinks memorization circuit while Fourier circuit grows stronger</li>
      <li>At crossover point (~epoch 9500), Fourier circuit dominates → grokking</li>
    </ul>
    <p><strong>Prediction:</strong> By tracking circuit strengths with ablation, could predict grokking 2000 epochs early</p>
  </div>

  <h3>Case 2: Induction Head Formation</h3>

  <div class="paper-card">
    <p><strong>Task:</strong> Language modeling (next-token prediction)</p>
    <p><strong>Observations (Olsson et al.):</strong></p>
    <ul>
      <li>In-context learning score near 0 before 2B tokens</li>
      <li>Sharp increase from 2.0B to 2.5B tokens</li>
      <li>Plateau at high ICL performance after 2.5B tokens</li>
    </ul>
    <p><strong>Mechanistic findings:</strong></p>
    <ul>
      <li><strong>1.5B tokens:</strong> Previous token heads form</li>
      <li><strong>2.0B tokens:</strong> Induction heads begin forming</li>
      <li><strong>2.2B tokens:</strong> Q-K composition between head types strengthens</li>
      <li><strong>2.5B tokens:</strong> Full induction circuit operational</li>
    </ul>
    <p><strong>Key insight:</strong> Phase transition is sharp (~500M tokens) but hierarchical assembly takes longer</p>
  </div>

  <h2>9. Common Patterns and Principles</h2>

  <h3>Empirical Observations</h3>

  <div class="info-box">
    <p><strong>Patterns across multiple studies:</strong></p>
    <ul>
      <li><strong>Simple before complex:</strong> Individual components form before composed circuits</li>
      <li><strong>Multiple solutions:</strong> Models often try many approaches before settling on one</li>
      <li><strong>Sudden composition:</strong> Components exist separately, then suddenly wire together</li>
      <li><strong>Lottery ticket-like:</strong> Some random initializations have "winning" subnetworks that form faster</li>
      <li><strong>Regularization matters:</strong> Weight decay, dropout affect which circuits win</li>
    </ul>
  </div>

  <h2>10. Limitations and Open Questions</h2>

  <h3>What We Don't Understand</h3>

  <ul>
    <li><strong>Why sharp transitions?</strong> What causes phase transitions vs smooth learning?</li>
    <li><strong>Prediction:</strong> Can we predict emergence timing from early training signals?</li>
    <li><strong>Intervention:</strong> Can we speed up or redirect circuit formation?</li>
    <li><strong>Scaling:</strong> Do these patterns hold in very large models?</li>
    <li><strong>Generality:</strong> Are these findings specific to transformers or universal?</li>
  </ul>

  <h2>11. Summary</h2>

  <h3>Key Takeaways</h3>

  <ul>
    <li><strong>Mechanisms don't exist from initialization—they form</strong> during training through specific dynamics</li>
    <li><strong>Formation can be sudden (phase transition) or gradual</strong> depending on circuit complexity</li>
    <li><strong>Grokking reveals competing circuits</strong>—memorization vs generalization</li>
    <li><strong>Induction heads show hierarchical assembly</strong>—components before composition</li>
    <li><strong>Checkpoint analysis</strong> lets you apply interpretability methods across training</li>
    <li><strong>Training dynamics reveal why mechanisms exist</strong>—what makes them "natural" solutions</li>
  </ul>

  <h3>For Your Project</h3>

  <div class="info-box">
    <ul>
      <li>Choose: fine-tuning study, small-scale pretraining, or public checkpoints (Pythia)</li>
      <li>Save checkpoints at appropriate intervals (log-spaced recommended)</li>
      <li>Apply your Week 3-8 methods to each checkpoint</li>
      <li>Look for phase transitions, grokking, or gradual emergence</li>
      <li>Connect emergence timing to your circuit's structure and function</li>
    </ul>
  </div>
  <h2>References & Resources</h2>

  <h3>Core Papers</h3>
  <ul>
    <li><strong>Nanda et al. (2023):</strong> "Progress measures for grokking via mechanistic interpretability." <em>ICLR</em>. <a href="https://arxiv.org/abs/2301.05217" target="_blank">arXiv:2301.05217</a></li>
    <li><strong>Olsson et al. (2022):</strong> "In-context Learning and Induction Heads." <em>Transformer Circuits Thread</em>. <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" target="_blank">Link</a></li>
    <li><strong>Power et al. (2022):</strong> "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets." <em>ICLR</em>. <a href="https://arxiv.org/abs/2201.02177" target="_blank">arXiv:2201.02177</a></li>
  </ul>

  <h3>Related Work</h3>
  <ul>
    <li><strong>Chughtai et al. (2024):</strong> "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations." <a href="https://arxiv.org/abs/2402.15057" target="_blank">arXiv:2402.15057</a></li>
    <li><strong>Chan et al. (2024):</strong> "The Developmental Landscape of In-Context Learning." <a href="https://arxiv.org/abs/2402.02364" target="_blank">arXiv:2402.02364</a></li>
    <li><strong>Elhage et al. (2022):</strong> "Toy Models of Superposition." <em>Transformer Circuits Thread</em>. <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank">Link</a></li>
    <li><strong>Wei et al. (2022):</strong> "Emergent Abilities of Large Language Models." <em>TMLR</em>. <a href="https://arxiv.org/abs/2206.07682" target="_blank">arXiv:2206.07682</a></li>
    <li><strong>Liu et al. (2023):</strong> "Towards Understanding Grokking: An Effective Theory of Representation Learning." <em>NeurIPS</em>. <a href="https://arxiv.org/abs/2205.10343" target="_blank">arXiv:2205.10343</a></li>
  </ul>

  <h3>Tools and Resources</h3>
  <ul>
    <li><strong>Pythia Suite:</strong> Models with checkpoints at many training steps <a href="https://github.com/EleutherAI/pythia" target="_blank">GitHub</a></li>
    <li><strong>TransformerLens:</strong> Library for mechanistic interpretability with checkpoint support <a href="https://github.com/neelnanda-io/TransformerLens" target="_blank">GitHub</a></li>
    <li><strong>Grokking reproduction code:</strong> Nanda's experiments <a href="https://github.com/neelnanda-io/Grokking" target="_blank">GitHub</a></li>
  </ul>

  <section id="in-class-exercise">
    <h2>In-Class Exercise: When Do Puns Emerge?</h2>
    <p>
      Using NDIF (National Deep Inference Fabric) to access OLMo training checkpoints, we will track
      when pun understanding emerges during pretraining. This connects our pun thread to training dynamics.
    </p>

    <h3>Part 1: Setup and Checkpoint Selection (15 min)</h3>
    <p>
      Access OLMo checkpoints through NDIF:
    </p>
    <ol>
      <li><strong>Connect to NDIF:</strong> Use the provided notebook to access the OLMo checkpoint suite</li>
      <li><strong>Select checkpoints:</strong> Choose 8-10 checkpoints spanning training:
        <ul>
          <li>Early: steps 1k, 5k, 10k (before most capabilities emerge)</li>
          <li>Middle: steps 50k, 100k, 200k (capability formation)</li>
          <li>Late: steps 500k, 1M, final (mature model)</li>
        </ul>
      </li>
      <li><strong>Prepare pun test set:</strong> Load your puns from Week 3 (10-15 examples)</li>
    </ol>

    <h3>Part 2: Track Pun Capabilities Over Training (25 min)</h3>
    <p>
      Measure pun understanding at each checkpoint:
    </p>
    <ol>
      <li><strong>Pun completion accuracy:</strong>
        <ul>
          <li>For each checkpoint, test: can it complete pun setups correctly?</li>
          <li>"I used to be a banker, but I lost ___" → does it predict "interest"?</li>
          <li>Record top-5 predictions and their probabilities</li>
        </ul>
      </li>
      <li><strong>Probe accuracy over training:</strong>
        <ul>
          <li>Apply your Week 6 pun probe to each checkpoint</li>
          <li>Does pun/non-pun separation improve over training?</li>
          <li>At which checkpoint does probe accuracy exceed 70%? 90%?</li>
        </ul>
      </li>
      <li><strong>Logit lens at each checkpoint:</strong>
        <ul>
          <li>For a fixed pun, run logit lens at early, middle, and late checkpoints</li>
          <li>Does the "correct" punchline emerge at earlier layers as training progresses?</li>
        </ul>
      </li>
    </ol>

    <h3>Part 3: Identify Phase Transitions (20 min)</h3>
    <p>
      Look for sudden changes in pun capability:
    </p>
    <ol>
      <li><strong>Plot capability curves:</strong>
        <ul>
          <li>X-axis: training step (log scale)</li>
          <li>Y-axis: pun completion accuracy, probe accuracy, or other metric</li>
        </ul>
      </li>
      <li><strong>Identify transitions:</strong>
        <ul>
          <li>Is pun understanding gradual or sudden?</li>
          <li>If sudden, at what training step does it emerge?</li>
          <li>Does it correlate with other capability emergence (e.g., general language ability)?</li>
        </ul>
      </li>
      <li><strong>Compare to in-context learning:</strong>
        <ul>
          <li>Test pun completion with and without pun examples in context</li>
          <li>At early checkpoints, does in-context help more than at late checkpoints?</li>
          <li>Does ICL for puns emerge at the same time as induction heads?</li>
        </ul>
      </li>
    </ol>

    <p>
      <strong>Discussion:</strong> Is pun understanding a distinct capability that emerges at a specific point,
      or does it improve gradually with general language ability? What does this tell us about how models
      learn to process humor?
    </p>

    <a href="https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week9/pun_emergence_olmo.ipynb" target="_blank" class="colab-button">
      Open Pun Emergence Notebook in Colab
    </a>
    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: Requires NDIF access for loading OLMo checkpoints.</em>
    </p>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 9</strong></p>
      <p>
        Study when and how your concept's circuit emerges during training. Apply your interpretability methods
        to multiple training checkpoints to track mechanism formation.
      </p>

      <h4>Training Dynamics Study</h4>
      <ul>
        <li><strong>Choose your approach:</strong>
          <ul>
            <li><strong>Option A (Recommended):</strong> Use Pythia checkpoints—apply your methods to pre-saved checkpoints at different training steps</li>
            <li><strong>Option B:</strong> Fine-tune your selected model on concept-rich data, saving checkpoints every N steps</li>
            <li><strong>Option C:</strong> Train a small model from scratch on a simplified task involving your concept</li>
          </ul>
        </li>
        <li><strong>Save/select checkpoints:</strong>
          <ul>
            <li>Choose 8-15 checkpoints spanning early to late training</li>
            <li>Log-spaced recommended (e.g., steps 100, 300, 1k, 3k, 10k, 30k, 100k)</li>
            <li>Ensure you capture pre-emergence and post-emergence states</li>
          </ul>
        </li>
        <li><strong>Apply interpretability methods to each checkpoint:</strong>
          <ul>
            <li><strong>From Week 5:</strong> Probe accuracy over training—when does concept become linearly accessible?</li>
            <li><strong>From Week 4:</strong> Ablation effects over training—when do components become necessary?</li>
            <li><strong>From Week 3:</strong> Logit lens evolution—when does prediction sharpen?</li>
            <li><strong>From Week 8:</strong> Circuit strength (path patching)—when does composition emerge?</li>
          </ul>
        </li>
        <li><strong>Analyze emergence pattern:</strong>
          <ul>
            <li>Is emergence gradual or sudden (phase transition)?</li>
            <li>Do multiple metrics change simultaneously?</li>
            <li>What is the timeline of component formation?</li>
            <li>Is there hierarchical assembly (some parts before others)?</li>
          </ul>
        </li>
      </ul>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Training trajectory plots:</strong>
          <ul>
            <li>Probe accuracy vs training step</li>
            <li>Ablation effect size vs training step</li>
            <li>Key metrics showing mechanism emergence</li>
            <li>Annotate identified phase transitions or inflection points</li>
          </ul>
        </li>
        <li><strong>Emergence analysis:</strong>
          <ul>
            <li>When does your concept's circuit form? (specific training step/token count)</li>
            <li>Is formation sudden or gradual?</li>
            <li>Do components form in sequence or simultaneously?</li>
            <li>Comparison to loss curves—does emergence coincide with capability gain?</li>
          </ul>
        </li>
        <li><strong>Mechanistic interpretation:</strong>
          <ul>
            <li>Why does the circuit form when it does?</li>
            <li>Is there evidence of competing mechanisms (grokking-style)?</li>
            <li>What does emergence timing reveal about your concept's role?</li>
          </ul>
        </li>
        <li><strong>Code:</strong> Notebook with checkpoint analysis and visualization</li>
      </ul>

      <p><em>
        Training dynamics reveal whether your circuit is a "natural" solution that forms reliably, or an
        accidental feature specific to your model's initialization and training procedure.
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">&larr; Back to Course Home</a></p>
  </footer>

</body>

</html>
