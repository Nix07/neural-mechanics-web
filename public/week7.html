<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 7: Unsupervised Feature Discovery and Superposition - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }

    .math {
      font-style: italic;
      text-align: center;
      margin: 10px 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 15px 0;
    }

    th,
    td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: left;
    }

    th {
      background-color: #f0f0f0;
    }

    .research-box {
      background-color: #e3f2fd;
      border-left: 4px solid #2196f3;
      padding: 15px;
      margin: 20px 0;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 7: Unsupervised Feature Discovery</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">← Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      Previous weeks used supervision (probes) or hypotheses (circuits) to find features. But what if we don't know
      which features to look for? This week addresses the fundamental challenge of <strong>unsupervised feature
        discovery</strong>: finding interpretable units without labels or prior hypotheses. We'll explore why this is
      hard (superposition), how sparse autoencoders offer a solution, and how to validate that discovered features are
      "real."
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Explain the superposition hypothesis and why linear probes are insufficient for finding all features</li>
      <li>Understand toy models of superposition and how features interfere in linear representations</li>
      <li>Explain how sparse autoencoders perform unsupervised feature discovery</li>
      <li>Distinguish between feature discovery (SAEs), feature reading (probes), and mechanism discovery (circuits)
      </li>
      <li>Interpret SAE features using automated labeling methods</li>
      <li>Validate feature quality: monosemanticity, consistency, and causal effects</li>
      <li>Understand transcoders as tools for decomposing computation (not just representation)</li>
      <li>Compare SAE-discovered features with steering vectors and probe directions</li>
      <li>Identify when features split or merge across different SAE capacities</li>
      <li>Evaluate open research questions: feature completeness, universality, and causality</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Readings</h2>

    <h3>Core Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank">Toy Models of
            Superposition</a>
        </div>
        <div class="reading-description">Foundational work on why features interfere in neural networks</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" target="_blank">Towards
            Monosemanticity: Decomposing Language Models With Dictionary Learning</a>
        </div>
        <div class="reading-description">First large-scale SAE applied to transformers, introducing the approach</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" target="_blank">Scaling
            Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a>
        </div>
        <div class="reading-description">Scaling SAEs to production models, feature quality at scale</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2406.11944" target="_blank">Transcoders Find Interpretable LLM Feature
            Circuits</a>
        </div>
        <div class="reading-description">Using SAEs to decompose computation between layers</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2410.13928" target="_blank">Automatically Interpreting Millions of Features in
            Large Language Models</a>
        </div>
        <div class="reading-description">Automated feature labeling at scale using LLMs</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2406.04341" target="_blank">Improving Dictionary Learning with Gated Sparse
            Autoencoders</a>
        </div>
        <div class="reading-description">Technical improvements to SAE architecture</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition"
            target="_blank">Taking Features Out of Superposition with Sparse Autoencoders</a>
        </div>
        <div class="reading-description">Research report on SAE effectiveness and limitations</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2309.08600" target="_blank">The Geometry of Categorical and Hierarchical
            Concepts in Large Language Models</a>
        </div>
        <div class="reading-description">Understanding feature geometry (bonus topic)</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: From Superposition to Discovery</h2>

    <h3>1. The Feature Discovery Problem</h3>
    <p>
      So far, we've found features in three ways:
    </p>

    <ul>
      <li><strong>Probes (Week 6):</strong> Supervised—requires labeled data for specific concepts</li>
      <li><strong>Circuits (Week 5):</strong> Hypothesis-driven—requires knowing what mechanism to look for</li>
      <li><strong>Steering (Week 2):</strong> Contrastive—requires paired examples</li>
    </ul>

    <p>
      But what if we want to discover <em>all</em> the features a model uses, without prior knowledge? This is the
      <strong>unsupervised feature discovery problem</strong>.
    </p>

    <h4>Why This Matters</h4>
    <ul>
      <li><strong>Unknown concepts:</strong> Models may use features we haven't thought to look for</li>
      <li><strong>Comprehensive understanding:</strong> Need the full feature repertoire, not cherry-picked examples
      </li>
      <li><strong>Novel domains:</strong> For non-CS concepts, we may not know what features exist</li>
      <li><strong>Safety:</strong> Dangerous capabilities might use unexpected features</li>
    </ul>

    <h4>The Naive Approach Fails</h4>
    <p>
      Why not just treat each neuron as a feature?
    </p>

    <div class="diagram">
      Neuron 47 activates for:<br>
      • Base64 strings<br>
      • DNA sequences<br>
      • URLs<br>
      • Canadian place names<br>
      → Polysemantic (responds to unrelated concepts)
    </div>

    <p>
      Individual neurons are <strong>polysemantic</strong>—they respond to multiple, unrelated concepts. Why?
    </p>

    <h3>2. The Superposition Hypothesis</h3>
    <p>
      <strong>Superposition hypothesis:</strong> Neural networks represent more features than they have dimensions by
      allowing features to <em>interfere</em> with each other in superposition.
    </p>

    <h4>The Core Intuition</h4>
    <p>
      Imagine you have 100 dimensions but want to represent 1000 features. If most features are rarely active
      (sparse), you can "pack" multiple features into each dimension, accepting that when they're simultaneously active
      they'll interfere.
    </p>

    <div class="diagram">
      2D space, but 4 features:<br>
      Feature A: [1.0, 0.1]<br>
      Feature B: [0.1, 1.0]<br>
      Feature C: [-1.0, 0.1]<br>
      Feature D: [0.1, -1.0]<br><br>
      If only one is active at a time → no interference<br>
      If multiple active → they interfere
    </div>

    <p>
      This is efficient but makes features hard to read out linearly. A single neuron detects multiple overlapping
      features.
    </p>

    <h4>Toy Model of Superposition</h4>
    <p>
      Anthropic's toy model demonstrates this clearly:
    </p>

    <ol>
      <li><strong>Setup:</strong> Train autoencoder with bottleneck smaller than input</li>
      <li><strong>Sparsity:</strong> Most input features are zero most of the time</li>
      <li><strong>Result:</strong> Network learns to pack features in superposition</li>
      <li><strong>Evidence:</strong> Reconstructions show feature interference patterns</li>
    </ol>

    <p><strong>Key finding:</strong> The level of superposition increases with:</p>
    <ul>
      <li>More features than dimensions (higher compression)</li>
      <li>Sparser features (lower interference cost)</li>
      <li>Features with varying importance (pack less important features more densely)</li>
    </ul>

    <h4>Implications for Interpretability</h4>
    <table>
      <tr>
        <th>Approach</th>
        <th>Problem with Superposition</th>
      </tr>
      <tr>
        <td>Reading neurons</td>
        <td>Each neuron is polysemantic</td>
      </tr>
      <tr>
        <td>Linear probes</td>
        <td>Can only find features in linear subspaces</td>
      </tr>
      <tr>
        <td>Activation patching</td>
        <td>Ablating one neuron affects multiple features</td>
      </tr>
      <tr>
        <td>Circuits</td>
        <td>Hard to trace feature-specific pathways</td>
      </tr>
    </table>

    <p>
      <strong>Bottom line:</strong> We need a method that can disentangle superposed features.
    </p>

    <h3>3. Sparse Autoencoders as Solution</h3>
    <p>
      <strong>Sparse autoencoders (SAEs)</strong> decompose model activations into a sparse, overcomplete basis—reversing
      superposition.
    </p>

    <h4>The Core Idea</h4>
    <p>
      If the network compresses features into superposition, we can "uncompress" them with a sparse autoencoder:
    </p>

    <div class="diagram">
      Model activation: [768 dims]<br>
      ↓ SAE Encoder<br>
      Sparse code: [16384 dims, mostly zeros]<br>
      ↓ SAE Decoder<br>
      Reconstructed: [768 dims]
    </div>

    <h4>Architecture</h4>
    <div class="math">
      h = activations from model (d dimensions)<br>
      f = ReLU(W_enc · h + b_enc) (d_hidden dimensions)<br>
      ĥ = W_dec · f + b_dec<br><br>
      Loss = ||h - ĥ||² + λ||f||₁
    </div>

    <p><strong>Key components:</strong></p>
    <ul>
      <li><strong>Overcomplete:</strong> d_hidden >> d (e.g., 16k features for 768 dims)</li>
      <li><strong>Sparsity penalty:</strong> λ||f||₁ encourages few active features</li>
      <li><strong>Reconstruction:</strong> ||h - ĥ||² ensures we don't lose information</li>
    </ul>

    <h4>Why This Works</h4>
    <ol>
      <li><strong>Overcomplete basis:</strong> More features than dimensions, room to separate superposed features</li>
      <li><strong>Sparsity:</strong> Forces clean separation (can't just rotate the basis)</li>
      <li><strong>Learned:</strong> Discovers natural feature boundaries from data</li>
    </ol>

    <h4>What SAEs Find</h4>
    <p>
      Examples from Claude 3 Sonnet SAEs:
    </p>
    <ul>
      <li>Feature 34M: "Arabic script and Islamic religious concepts"</li>
      <li>Feature 2.1M: "Code errors and debugging"</li>
      <li>Feature 7.3M: "Sarcasm and irony"</li>
      <li>Feature 12M: "Mathematical proofs"</li>
    </ul>

    <p>
      These are <strong>monosemantic</strong>—each feature responds to one coherent concept.
    </p>

    <h3>4. Three Types of Feature Discovery</h3>
    <p>
      Let's clarify how SAEs differ from previous methods:
    </p>

    <table>
      <tr>
        <th>Method</th>
        <th>Supervision</th>
        <th>What It Finds</th>
        <th>Strength</th>
        <th>Limitation</th>
      </tr>
      <tr>
        <td><strong>Probes</strong><br>(Week 6)</td>
        <td>Supervised labels</td>
        <td>Known concepts you test for</td>
        <td>Validates specific hypotheses</td>
        <td>Only finds what you look for</td>
      </tr>
      <tr>
        <td><strong>Circuits</strong><br>(Week 5)</td>
        <td>Hypothesis-driven</td>
        <td>Mechanisms for specific behaviors</td>
        <td>Causal, interpretable</td>
        <td>Requires knowing the task</td>
      </tr>
      <tr>
        <td><strong>SAEs</strong><br>(This week)</td>
        <td>Unsupervised</td>
        <td>All features used by model</td>
        <td>Discovers unknown features</td>
        <td>Not guaranteed causal</td>
      </tr>
    </table>

    <div class="research-box">
      <strong>Research Insight:</strong> These methods are complementary, not competing. SAEs discover features →
      Probes validate they're used → Circuits explain how they're computed.
    </div>

    <h3>5. Transcoders: Decomposing Computation</h3>
    <p>
      Standard SAEs decompose <em>representations</em> (activations at one layer). <strong>Transcoders</strong>
      decompose <em>computation</em> (transformations between layers).
    </p>

    <h4>The Motivation</h4>
    <p>
      In transformers, layers communicate through the residual stream. A transcoder learns:
    </p>

    <div class="math">
      Layer N output → [Sparse features] → Layer N+1 input
    </div>

    <h4>Architecture</h4>
    <p>
      Instead of reconstructing the same layer:
    </p>
    <div class="diagram">
      h_n = activations from layer n<br>
      f = ReLU(W_enc · h_n + b_enc)<br>
      ĥ_{n+1} = W_dec · f + b_dec<br><br>
      Loss = ||h_{n+1} - ĥ_{n+1}||² + λ||f||₁
    </div>

    <p>
      The sparse features <code>f</code> now represent the <strong>computational pathway</strong> from layer n to n+1.
    </p>

    <h4>What Transcoders Reveal</h4>
    <ul>
      <li><strong>Feature circuits:</strong> Which features in layer n activate which features in layer n+1</li>
      <li><strong>Information flow:</strong> How concepts are processed and transformed</li>
      <li><strong>Layer specialization:</strong> What computation each layer performs</li>
    </ul>

    <h4>Comparison: SAEs vs Transcoders</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>SAE</th>
        <th>Transcoder</th>
      </tr>
      <tr>
        <td>Decomposes</td>
        <td>Representations (within layer)</td>
        <td>Computation (between layers)</td>
      </tr>
      <tr>
        <td>Input/Output</td>
        <td>Same layer</td>
        <td>Layer n → Layer n+1</td>
      </tr>
      <tr>
        <td>Use Case</td>
        <td>"What features are present?"</td>
        <td>"How do features transform?"</td>
      </tr>
      <tr>
        <td>Connects to</td>
        <td>Probing, steering</td>
        <td>Circuits, path patching</td>
      </tr>
    </table>

    <h3>6. Feature Interpretation</h3>
    <p>
      Once SAEs find features, how do we interpret them?
    </p>

    <h4>Manual Interpretation</h4>
    <p><strong>Max-activating examples:</strong> Find texts where feature activates most strongly</p>

    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Feature 2.1M top activations:<br>
      "...syntax error on line 47..."<br>
      "...undefined variable foo..."<br>
      "...debug trace shows..."<br>
      → Interpretation: "Code errors and debugging"
    </code>

    <p>
      <strong>Limitations:</strong> Slow, subjective, doesn't scale to millions of features
    </p>

    <h4>Automated Interpretation</h4>
    <p>
      Use an LLM to label features:
    </p>

    <ol>
      <li>Extract max-activating examples for feature</li>
      <li>Prompt LLM: "What concept do these examples have in common?"</li>
      <li>LLM generates description: "This feature detects code errors"</li>
      <li>Validate description on held-out examples</li>
    </ol>

    <h4>Validation Methods</h4>
    <p><strong>1. Quantitative agreement:</strong></p>
    <ul>
      <li>Generate more examples matching description</li>
      <li>Check if feature activates on them</li>
      <li>Compute precision/recall</li>
    </ul>

    <p><strong>2. Simulation scoring:</strong></p>
    <ul>
      <li>Ask LLM to predict feature activation from text</li>
      <li>Compare with actual activations</li>
      <li>High correlation = good explanation</li>
    </ul>

    <p><strong>3. Human evaluation:</strong></p>
    <ul>
      <li>Show humans feature activations + descriptions</li>
      <li>Rate quality of explanation</li>
    </ul>

    <h3>7. Feature Quality and Validation</h3>
    <p>
      Not all SAE features are equally meaningful. How do we validate quality?
    </p>

    <h4>Criterion 1: Monosemanticity</h4>
    <p>
      <strong>Question:</strong> Does the feature respond to one coherent concept?
    </p>

    <p><strong>Tests:</strong></p>
    <ul>
      <li>Examine diverse activating examples—are they related?</li>
      <li>Check for spurious correlations</li>
      <li>Compare early vs late activating examples—same concept?</li>
    </ul>

    <p><strong>Red flags:</strong></p>
    <ul>
      <li>Feature activates on seemingly unrelated concepts</li>
      <li>Can't write coherent description</li>
      <li>High activations are confusing/random</li>
    </ul>

    <h4>Criterion 2: Consistency</h4>
    <p>
      <strong>Question:</strong> Does the feature activate consistently across contexts?
    </p>

    <p><strong>Tests:</strong></p>
    <ul>
      <li>Paraphrase test: different phrasings of same concept → similar activation?</li>
      <li>Context variation: concept in different contexts → consistent?</li>
      <li>Negation test: negated concept → low activation?</li>
    </ul>

    <h4>Criterion 3: Causality</h4>
    <p>
      <strong>Question:</strong> Is the feature actually used by the model?
    </p>

    <div class="research-box">
      <strong>Critical:</strong> SAE feature presence doesn't prove causal use (same issue as probes in Week 6)
    </div>

    <p><strong>Validation approaches:</strong></p>
    <ul>
      <li><strong>Clamping:</strong> Set feature to zero, does output change?</li>
      <li><strong>Steering:</strong> Amplify feature, does behavior change predictably?</li>
      <li><strong>Compare with circuits:</strong> Do features correspond to causally important components?</li>
    </ul>

    <h4>Feature Quality Metrics</h4>
    <table>
      <tr>
        <th>Metric</th>
        <th>What It Measures</th>
        <th>How to Compute</th>
      </tr>
      <tr>
        <td>L0 Sparsity</td>
        <td>How many features active</td>
        <td>Average number of non-zero features</td>
      </tr>
      <tr>
        <td>Reconstruction Loss</td>
        <td>Information preservation</td>
        <td>||h - ĥ||²</td>
      </tr>
      <tr>
        <td>Explained Variance</td>
        <td>How much SAE captures</td>
        <td>1 - (reconstruction_loss / variance)</td>
      </tr>
      <tr>
        <td>Interpretation Score</td>
        <td>Feature coherence</td>
        <td>LLM agreement on activations</td>
      </tr>
    </table>

    <h3>8. Comparing with Other Methods</h3>
    <p>
      How do SAE features relate to steering vectors and probe directions?
    </p>

    <h4>SAE Features vs Steering Vectors (Week 2)</h4>
    <ul>
      <li><strong>Steering vectors:</strong> Contrastive (positive - negative examples)</li>
      <li><strong>SAE features:</strong> Unsupervised discovery</li>
      <li><strong>Relationship:</strong> Steering vectors often align with SAE features</li>
      <li><strong>Advantage of SAEs:</strong> Find features you didn't think to contrast</li>
    </ul>

    <h4>SAE Features vs Probe Directions (Week 6)</h4>
    <ul>
      <li><strong>Probes:</strong> Supervised, find directions for labeled concepts</li>
      <li><strong>SAE features:</strong> Unsupervised, find natural feature basis</li>
      <li><strong>Relationship:</strong> Probe directions may be combinations of SAE features</li>
      <li><strong>Advantage of SAEs:</strong> Discover novel concepts without labels</li>
    </ul>

    <h4>Empirical Findings</h4>
    <p>
      Studies show:
    </p>
    <ul>
      <li>70-80% of steering vectors align well with single SAE features</li>
      <li>Remaining 20-30% are combinations of multiple features</li>
      <li>SAEs discover features not found by probes or steering</li>
      <li>Best results: use SAE features for steering (Week 2 methods with Week 7 features)</li>
    </ul>

    <h3>9. Open Research Questions</h3>
    <p>
      SAEs are powerful but leave major questions unanswered:
    </p>

    <h4>Question 1: Feature Completeness</h4>
    <p>
      <strong>Problem:</strong> Do SAEs find <em>all</em> features, or just easy-to-decompose ones?
    </p>

    <p><strong>Evidence for incompleteness:</strong></p>
    <ul>
      <li>Reconstruction loss never reaches zero</li>
      <li>Some model behaviors can't be explained by SAE features</li>
      <li>Features may exist in higher-order interactions</li>
    </ul>

    <p><strong>Open directions:</strong></p>
    <ul>
      <li>How to detect missing features?</li>
      <li>Are some features fundamentally non-separable?</li>
      <li>Can we quantify feature coverage?</li>
    </ul>

    <h4>Question 2: Feature Universality</h4>
    <p>
      <strong>Problem:</strong> Do different models learn the same features?
    </p>

    <p><strong>Initial findings:</strong></p>
    <ul>
      <li>Some features appear universal (e.g., "base64 encoding")</li>
      <li>Others are model-specific</li>
      <li>Feature similarity decreases as models differ more</li>
    </ul>

    <p><strong>Research implications:</strong></p>
    <ul>
      <li>Universal features → general properties of language/concepts</li>
      <li>Model-specific features → training or architecture artifacts</li>
      <li>Important for transfer and safety (are dangerous features universal?)</li>
    </ul>

    <h4>Question 3: Feature Causality</h4>
    <p>
      <strong>Problem:</strong> Are SAE features causally used or just present?
    </p>

    <div class="research-box">
      <strong>Critical gap:</strong> Most SAE research focuses on discovery and interpretation, not validation of
      causal role.
    </div>

    <p><strong>Why this matters:</strong></p>
    <ul>
      <li>For safety: need to know which features actually affect behavior</li>
      <li>For understanding: presence ≠ use (same issue as probes)</li>
      <li>For steering: need causally effective features</li>
    </ul>

    <p><strong>Validation needed:</strong></p>
    <ul>
      <li>Intervention experiments (clamping, steering)</li>
      <li>Comparison with circuit analysis</li>
      <li>Out-of-distribution testing</li>
    </ul>

    <h4>Question 4: Feature Splitting</h4>
    <p>
      <strong>Problem:</strong> As SAE capacity increases, features split into subfeatres. When does this help vs hurt?
    </p>

    <p><strong>Example:</strong></p>
    <div class="diagram">
      16k features: "Programming" (broad)<br>
      ↓<br>
      65k features: "Python syntax", "JavaScript syntax", "Debugging"<br>
      ↓<br>
      256k features: "Python list comprehensions", "Python error handling", etc.
    </div>

    <p><strong>Trade-offs:</strong></p>
    <ul>
      <li><strong>More splitting:</strong> Finer-grained understanding, but interpretability burden</li>
      <li><strong>Less splitting:</strong> Simpler, but may miss important distinctions</li>
      <li><strong>Question:</strong> What's the "right" granularity for features?</li>
    </ul>

    <h4>Question 5: Feature Geometry</h4>
    <p>
      <strong>Problem:</strong> How are features organized relative to each other?
    </p>

    <p><strong>Findings:</strong></p>
    <ul>
      <li>Related features are nearby in feature space</li>
      <li>Hierarchical structure (broad → specific)</li>
      <li>Some features form "concepts clusters"</li>
    </ul>

    <p><strong>Open questions:</strong></p>
    <ul>
      <li>Is there a universal feature geometry?</li>
      <li>How does geometry relate to semantic structure?</li>
      <li>Can we use geometry for better interpretation?</li>
    </ul>

    <h3>10. Research Workflow with SAEs</h3>

    <ol>
      <li><strong>Discovery:</strong>
        <ul>
          <li>Train SAE on target layer(s)</li>
          <li>Extract features and their activations</li>
          <li>Automated interpretation for all features</li>
        </ul>
      </li>

      <li><strong>Filtering:</strong>
        <ul>
          <li>Identify features relevant to your concept</li>
          <li>Check monosemanticity and consistency</li>
          <li>Compare with steering/probe findings</li>
        </ul>
      </li>

      <li><strong>Validation:</strong>
        <ul>
          <li>Test causal effects (steering, clamping)</li>
          <li>Check if features participate in circuits</li>
          <li>Out-of-distribution testing</li>
        </ul>
      </li>

      <li><strong>Analysis:</strong>
        <ul>
          <li>How do features combine for your concept?</li>
          <li>Are there unexpected related features?</li>
          <li>How does feature activity change across contexts?</li>
        </ul>
      </li>

      <li><strong>Integration:</strong>
        <ul>
          <li>Use SAE features for steering (Week 2)</li>
          <li>Compare with probe findings (Week 6)</li>
          <li>Trace feature circuits (Week 5)</li>
        </ul>
      </li>
    </ol>

    <div class="research-box">
      <strong>Key principle:</strong> SAEs are a discovery tool, not a complete explanation. Always validate with
      causal methods.
    </div>
  </section>

  <section id="code-exercise">
    <h2>Code Exercise</h2>
    <p>
      This week's exercise provides hands-on experience with superposition and SAEs:
    </p>
    <ul>
      <li>Build and analyze toy models of superposition</li>
      <li>Train sparse autoencoders on transformer activations</li>
      <li>Interpret discovered features with automated methods</li>
      <li>Validate feature quality (monosemanticity, causality)</li>
      <li>Compare SAE features with steering vectors and probes</li>
      <li>Train transcoders to decompose computation</li>
      <li>Explore feature splitting across SAE capacities</li>
    </ul>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER" target="_blank" class="colab-button">
      Open Exercise in Google Colab
    </a>

    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be created and linked here.</em>
    </p>
  </section>

  <section id="assignment">
    <h2>Project Assignment</h2>

    <div class="assignment-box">
      <h3 style="margin-top: 0;">Discovering Features for Your Concept with SAEs</h3>

      <p>
        <strong>Goal:</strong> Use sparse autoencoders to discover features related to your concept, validate their
        quality, and compare with previous methods.
      </p>

      <h4>Requirements:</h4>
      <ol>
        <li><strong>SAE Training:</strong>
          <ul>
            <li>Train SAEs on relevant layers (or use pre-trained from Neuronpedia)</li>
            <li>Experiment with different dictionary sizes</li>
            <li>Evaluate reconstruction quality and sparsity</li>
          </ul>
        </li>

        <li><strong>Feature Discovery:</strong>
          <ul>
            <li>Extract all features and their activations on your dataset</li>
            <li>Use automated interpretation to label features</li>
            <li>Identify 10-20 features most relevant to your concept</li>
          </ul>
        </li>

        <li><strong>Feature Validation:</strong>
          <ul>
            <li>Test monosemanticity: examine diverse activating examples</li>
            <li>Test consistency: paraphrases, negations, context variations</li>
            <li>Test causality: steering with features, compare with patching results</li>
          </ul>
        </li>

        <li><strong>Comparison with Previous Methods:</strong>
          <ul>
            <li>Compare SAE features with steering vectors (Week 2)</li>
            <li>Compare with probe-discovered directions (Week 6)</li>
            <li>Do SAEs discover novel features not found by other methods?</li>
          </ul>
        </li>

        <li><strong>Feature Interactions:</strong>
          <ul>
            <li>Do multiple features activate together for your concept?</li>
            <li>Test feature combinations with steering</li>
            <li>Explore feature splitting at different capacities</li>
          </ul>
        </li>

        <li><strong>Open Questions Analysis:</strong>
          <ul>
            <li>Are all aspects of your concept captured by SAE features?</li>
            <li>Which features are causally important vs merely present?</li>
            <li>How do features relate to circuits from Week 5?</li>
          </ul>
        </li>
      </ol>

      <h4>Deliverables:</h4>
      <ul>
        <li>Jupyter notebook with all experiments and code</li>
        <li>Written report (6-7 pages) including:
          <ul>
            <li>SAE training details and evaluation metrics</li>
            <li>Top 10-20 features relevant to your concept (with interpretations)</li>
            <li>Feature validation results (monosemanticity, consistency, causality)</li>
            <li>Comparison with steering vectors and probe directions</li>
            <li>Analysis of feature combinations and interactions</li>
            <li>Discussion of completeness and open questions</li>
          </ul>
        </li>
        <li>Visualizations:
          <ul>
            <li>Feature activation patterns on your dataset</li>
            <li>SAE reconstruction quality and sparsity curves</li>
            <li>Feature comparison matrix (SAEs vs steering vs probes)</li>
            <li>Feature splitting across dictionary sizes</li>
            <li>Causal validation results (steering effects)</li>
          </ul>
        </li>
      </ul>

      <p>
        <strong>Due:</strong> Before Week 8 class
      </p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">← Back to Course Home</a></p>
  </footer>

</body>

</html>
