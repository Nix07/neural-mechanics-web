{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Input Attribution and Saliency Methods - Exercises\n",
    "\n",
    "This notebook provides hands-on exercises for understanding and applying input attribution methods to LLMs.\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Implement saliency maps and integrated gradients from scratch\n",
    "2. Use the Inseq library for multiple attribution methods\n",
    "3. Compare attention rollout vs attention flow\n",
    "4. Validate attributions with ablation experiments\n",
    "5. Apply attribution to your project concept\n",
    "6. Debug model failures using attribution\n",
    "\n",
    "**Structure:**\n",
    "- Parts 1-2: Implement gradient-based methods\n",
    "- Parts 3-4: Use Inseq library and compare methods\n",
    "- Parts 5-6: Attention-based attribution\n",
    "- Parts 7-8: Validation and debugging\n",
    "- Parts 9-10: Project application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install inseq transformers torch captum matplotlib numpy pandas seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Saliency Maps from Scratch\n",
    "\n",
    "We'll start by implementing the simplest gradient-based attribution method: saliency maps.\n",
    "\n",
    "**Exercise 1.1:** Implement saliency map computation for a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def compute_saliency(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    target_token_idx: Optional[int] = None\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute saliency map: gradient magnitude w.r.t. input embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        input_text: Input string\n",
    "        target_token_idx: Index of output token to attribute (default: last token)\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of input tokens\n",
    "        saliency: Array of saliency scores (shape: [num_tokens])\n",
    "    \"\"\"\n",
    "    # TODO: Implement saliency computation\n",
    "    # 1. Tokenize input\n",
    "    # 2. Get embeddings with requires_grad=True\n",
    "    # 3. Forward pass\n",
    "    # 4. Compute loss for target token (negative log probability)\n",
    "    # 5. Backward pass to get gradients\n",
    "    # 6. Compute saliency as gradient magnitude (L2 norm across embedding dim)\n",
    "    \n",
    "    raise NotImplementedError(\"Implement saliency computation\")\n",
    "\n",
    "# Test saliency computation\n",
    "input_text = \"The cat sat on the\"\n",
    "tokens, saliency = compute_saliency(model, tokenizer, input_text)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Saliency scores: {saliency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** Visualize saliency scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attribution(\n",
    "    tokens: List[str],\n",
    "    scores: np.ndarray,\n",
    "    title: str = \"Attribution Scores\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attribution scores as a bar plot and heatmap.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    ax1.bar(range(len(tokens)), scores, color='skyblue')\n",
    "    ax1.set_xticks(range(len(tokens)))\n",
    "    ax1.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Attribution Score')\n",
    "    ax1.set_title(title)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Heatmap (text with color background)\n",
    "    normalized_scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)\n",
    "    ax2.imshow([normalized_scores], cmap='Reds', aspect='auto')\n",
    "    ax2.set_xticks(range(len(tokens)))\n",
    "    ax2.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize saliency\n",
    "visualize_attribution(tokens, saliency, \"Saliency Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Integrated Gradients\n",
    "\n",
    "Now let's implement integrated gradients to address the gradient saturation problem.\n",
    "\n",
    "**Exercise 2.1:** Implement integrated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_integrated_gradients(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    baseline: str = \"\",  # Empty string as baseline\n",
    "    n_steps: int = 50,\n",
    "    target_token_idx: Optional[int] = None\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute integrated gradients along a path from baseline to input.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        input_text: Input string\n",
    "        baseline: Baseline string (default: empty)\n",
    "        n_steps: Number of interpolation steps\n",
    "        target_token_idx: Index of output token to attribute\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of input tokens\n",
    "        ig_scores: Array of IG attribution scores\n",
    "    \"\"\"\n",
    "    # TODO: Implement integrated gradients\n",
    "    # 1. Tokenize input and baseline\n",
    "    # 2. Get embeddings for both\n",
    "    # 3. Create interpolation path: baseline + alpha * (input - baseline) for alpha in [0, 1]\n",
    "    # 4. For each interpolation step:\n",
    "    #    - Forward pass with interpolated embeddings\n",
    "    #    - Compute gradients w.r.t. embeddings\n",
    "    #    - Accumulate gradients\n",
    "    # 5. IG = (input_emb - baseline_emb) * avg_gradients\n",
    "    # 6. Aggregate across embedding dimension (e.g., L2 norm)\n",
    "    \n",
    "    raise NotImplementedError(\"Implement integrated gradients\")\n",
    "\n",
    "# Test IG\n",
    "tokens_ig, ig_scores = compute_integrated_gradients(model, tokenizer, input_text, n_steps=50)\n",
    "print(f\"IG scores: {ig_scores}\")\n",
    "\n",
    "# Compare saliency vs IG\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.bar(range(len(tokens)), saliency, color='skyblue')\n",
    "ax1.set_xticks(range(len(tokens)))\n",
    "ax1.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax1.set_title('Saliency Map')\n",
    "ax1.set_ylabel('Score')\n",
    "\n",
    "ax2.bar(range(len(tokens_ig)), ig_scores, color='coral')\n",
    "ax2.set_xticks(range(len(tokens_ig)))\n",
    "ax2.set_xticklabels(tokens_ig, rotation=45, ha='right')\n",
    "ax2.set_title('Integrated Gradients')\n",
    "ax2.set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2:** Test baseline sensitivity.\n",
    "\n",
    "Try different baselines and see how IG scores change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baselines(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    baselines: Dict[str, str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare IG scores with different baselines.\n",
    "    \n",
    "    Args:\n",
    "        baselines: Dict mapping baseline name to baseline text\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, baseline in baselines.items():\n",
    "        # TODO: Compute IG with this baseline\n",
    "        # Store results\n",
    "        raise NotImplementedError(\"Implement baseline comparison\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    # TODO: Create subplots comparing all baselines\n",
    "    raise NotImplementedError(\"Implement visualization\")\n",
    "\n",
    "# Test different baselines\n",
    "baselines = {\n",
    "    'Empty': '',\n",
    "    'Padding': tokenizer.pad_token * 6,  # Same length as input\n",
    "    'Random': 'hello world test example',  # Unrelated text\n",
    "}\n",
    "\n",
    "# compare_baselines(model, tokenizer, input_text, baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using the Inseq Library\n",
    "\n",
    "Now let's use the Inseq library to access multiple attribution methods easily.\n",
    "\n",
    "**Exercise 3.1:** Set up Inseq and run basic attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inseq\n",
    "\n",
    "# Load model with Inseq\n",
    "# Available methods: 'saliency', 'input_x_gradient', 'integrated_gradients',\n",
    "#                    'attention', 'deeplift', 'gradient_shap', etc.\n",
    "model_inseq = inseq.load_model(\"gpt2\", \"integrated_gradients\")\n",
    "\n",
    "# Run attribution\n",
    "input_text = \"The cat sat on the mat because\"\n",
    "result = model_inseq.attribute(\n",
    "    input_text,\n",
    "    n_steps=1,  # Generate 1 token\n",
    "    step_scores=[\"probability\"]  # Track output probabilities\n",
    ")\n",
    "\n",
    "# Show results\n",
    "result.show()\n",
    "\n",
    "# Access attribution scores\n",
    "print(f\"Generated token: {result.sequence_attributions[0].target[0]}\")\n",
    "print(f\"Source tokens: {result.sequence_attributions[0].source}\")\n",
    "print(f\"Attribution scores shape: {result.sequence_attributions[0].source_attributions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:** Compare multiple attribution methods using Inseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attribution_methods(\n",
    "    input_text: str,\n",
    "    methods: List[str] = ['saliency', 'input_x_gradient', 'integrated_gradients', 'attention']\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Compare multiple attribution methods on the same input.\n",
    "    \n",
    "    Args:\n",
    "        input_text: Input string\n",
    "        methods: List of attribution method names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping method name to attribution result\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        # TODO: Load model with this method\n",
    "        # TODO: Run attribution\n",
    "        # TODO: Extract and store scores\n",
    "        raise NotImplementedError(\"Implement method comparison\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_method_comparison(results: Dict[str, any]):\n",
    "    \"\"\"\n",
    "    Visualize comparison of attribution methods.\n",
    "    \"\"\"\n",
    "    # TODO: Create subplots for each method\n",
    "    # TODO: Show which tokens are most important according to each method\n",
    "    # TODO: Compute correlation between methods\n",
    "    raise NotImplementedError(\"Implement comparison visualization\")\n",
    "\n",
    "# Compare methods\n",
    "# comparison_results = compare_attribution_methods(input_text)\n",
    "# visualize_method_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Perturbation-Based Attribution\n",
    "\n",
    "Implement token ablation to get ground truth attribution.\n",
    "\n",
    "**Exercise 4.1:** Implement token ablation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ablation_attribution(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    target_token: Optional[str] = None\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute attribution by ablating (removing) each token.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        input_text: Input string\n",
    "        target_token: Target token to predict (if None, use model's top prediction)\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of input tokens\n",
    "        ablation_scores: Drop in target token probability when each input token is removed\n",
    "    \"\"\"\n",
    "    # TODO: Implement token ablation\n",
    "    # 1. Tokenize input\n",
    "    # 2. Get baseline probability for target token (full input)\n",
    "    # 3. For each input token:\n",
    "    #    - Create input with that token removed\n",
    "    #    - Get probability for target token\n",
    "    #    - Attribution = baseline_prob - ablated_prob\n",
    "    \n",
    "    raise NotImplementedError(\"Implement ablation attribution\")\n",
    "\n",
    "# Test ablation\n",
    "# tokens_abl, ablation_scores = compute_ablation_attribution(model, tokenizer, input_text)\n",
    "# visualize_attribution(tokens_abl, ablation_scores, \"Token Ablation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2:** Validate gradient-based methods with ablation.\n",
    "\n",
    "Compute correlation between IG scores and ablation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_ablation(\n",
    "    input_text: str,\n",
    "    methods: List[str] = ['saliency', 'integrated_gradients']\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate gradient-based methods against ablation (ground truth).\n",
    "    \"\"\"\n",
    "    # TODO: Compute ablation scores\n",
    "    # TODO: Compute scores for each gradient-based method\n",
    "    # TODO: Compute Pearson/Spearman correlation\n",
    "    # TODO: Visualize: scatter plot of method scores vs ablation scores\n",
    "    \n",
    "    raise NotImplementedError(\"Implement validation\")\n",
    "\n",
    "# Run validation\n",
    "# validate_with_ablation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Attention-Based Attribution\n",
    "\n",
    "Extract and visualize attention weights.\n",
    "\n",
    "**Exercise 5.1:** Extract attention weights and compute attention to input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_weights(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str\n",
    ") -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract raw attention weights from all layers and heads.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of input tokens\n",
    "        attentions: Attention weights (n_layers, n_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: Tokenize input\n",
    "    # TODO: Forward pass with output_attentions=True\n",
    "    # TODO: Stack attention tensors from all layers\n",
    "    \n",
    "    raise NotImplementedError(\"Implement attention extraction\")\n",
    "\n",
    "def visualize_attention(\n",
    "    tokens: List[str],\n",
    "    attention: torch.Tensor,\n",
    "    layer: int = -1,\n",
    "    head: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a specific layer and head.\n",
    "    \"\"\"\n",
    "    # TODO: Extract attention for specified layer/head\n",
    "    # TODO: Create heatmap showing which tokens attend to which\n",
    "    \n",
    "    raise NotImplementedError(\"Implement attention visualization\")\n",
    "\n",
    "# Extract and visualize attention\n",
    "# tokens_attn, attentions = extract_attention_weights(model, tokenizer, input_text)\n",
    "# visualize_attention(tokens_attn, attentions, layer=-1, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Attention Rollout and Attention Flow\n",
    "\n",
    "Implement attention rollout to track information flow across layers.\n",
    "\n",
    "**Exercise 6.1:** Implement attention rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_rollout(\n",
    "    attentions: torch.Tensor,\n",
    "    head_fusion: str = 'mean',  # How to combine heads: 'mean', 'max', 'min'\n",
    "    discard_ratio: float = 0.9  # Discard bottom X% of attention mass (noise reduction)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute attention rollout across layers.\n",
    "    \n",
    "    Args:\n",
    "        attentions: Attention weights (n_layers, n_heads, seq_len, seq_len)\n",
    "        head_fusion: How to combine attention heads\n",
    "        discard_ratio: Fraction of attention to discard as noise\n",
    "    \n",
    "    Returns:\n",
    "        rollout: Attention rollout (seq_len, seq_len) showing final attention from output to inputs\n",
    "    \"\"\"\n",
    "    # TODO: Implement attention rollout\n",
    "    # 1. Combine heads (average or max)\n",
    "    # 2. Add residual connections: A_aug = 0.5*A + 0.5*I\n",
    "    # 3. Normalize rows to sum to 1\n",
    "    # 4. Roll out through layers: A_rollout = A_L @ A_{L-1} @ ... @ A_1\n",
    "    \n",
    "    raise NotImplementedError(\"Implement attention rollout\")\n",
    "\n",
    "# Test attention rollout\n",
    "# tokens_attn, attentions = extract_attention_weights(model, tokenizer, input_text)\n",
    "# rollout = compute_attention_rollout(attentions)\n",
    "# \n",
    "# # Visualize: attention from last token to all input tokens\n",
    "# final_token_attention = rollout[-1, :].detach().cpu().numpy()\n",
    "# visualize_attribution(tokens_attn, final_token_attention, \"Attention Rollout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.2:** Compare attention rollout with gradient-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_vs_gradients(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare attention rollout with IG and ablation.\n",
    "    \"\"\"\n",
    "    # TODO: Compute attention rollout\n",
    "    # TODO: Compute IG\n",
    "    # TODO: Compute ablation\n",
    "    # TODO: Visualize all three methods side by side\n",
    "    # TODO: Compute correlations\n",
    "    \n",
    "    raise NotImplementedError(\"Implement comparison\")\n",
    "\n",
    "# compare_attention_vs_gradients(model, tokenizer, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Validation with Sanity Checks\n",
    "\n",
    "Implement sanity checks to verify attribution quality.\n",
    "\n",
    "**Exercise 7.1:** Implement sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_random_model(\n",
    "    trained_model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    method: str = 'integrated_gradients'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test if attributions differ between trained and random model.\n",
    "    A good attribution method should give different results.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with correlation and difference metrics\n",
    "    \"\"\"\n",
    "    # TODO: Compute attribution on trained model\n",
    "    # TODO: Randomize model weights\n",
    "    # TODO: Compute attribution on random model\n",
    "    # TODO: Compare (correlation, L2 distance)\n",
    "    \n",
    "    raise NotImplementedError(\"Implement random model test\")\n",
    "\n",
    "def sanity_check_random_labels(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    target_tokens: List[str],\n",
    "    method: str = 'integrated_gradients'\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Test if attributions differ for different target tokens.\n",
    "    A good attribution method should give different results for different targets.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping target token to attribution scores\n",
    "    \"\"\"\n",
    "    # TODO: For each target token:\n",
    "    #   - Compute attribution conditioning on that target\n",
    "    #   - Store results\n",
    "    # TODO: Compare attributions across targets\n",
    "    \n",
    "    raise NotImplementedError(\"Implement random label test\")\n",
    "\n",
    "# Run sanity checks\n",
    "# random_model_results = sanity_check_random_model(model, tokenizer, input_text)\n",
    "# print(f\"Random model correlation: {random_model_results['correlation']:.3f}\")\n",
    "# print(\"(Low correlation is good - attributions should differ)\")\n",
    "\n",
    "# random_label_results = sanity_check_random_labels(\n",
    "#     model, tokenizer, input_text, \n",
    "#     target_tokens=['mat', 'floor', 'chair']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Debugging Model Predictions\n",
    "\n",
    "Use attribution to debug a specific model failure.\n",
    "\n",
    "**Exercise 8.1:** Analyze a subject-verb agreement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_prediction(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    expected_output: str,\n",
    "    methods: List[str] = ['integrated_gradients', 'attention']\n",
    "):\n",
    "    \"\"\"\n",
    "    Debug a model prediction using multiple attribution methods.\n",
    "    \n",
    "    Args:\n",
    "        input_text: Input prompt\n",
    "        expected_output: What the model should predict (or what we want to analyze)\n",
    "        methods: Attribution methods to use\n",
    "    \"\"\"\n",
    "    # TODO: Get model's actual prediction\n",
    "    # TODO: For each attribution method:\n",
    "    #   - Compute attribution for the predicted token\n",
    "    #   - Compute attribution for the expected token (if different)\n",
    "    # TODO: Identify which input tokens are causing the error\n",
    "    # TODO: Visualize comparison\n",
    "    \n",
    "    raise NotImplementedError(\"Implement prediction debugging\")\n",
    "\n",
    "# Example: Subject-verb agreement error\n",
    "# The model might incorrectly attend to \"cabinets\" (distractor) instead of \"key\" (subject)\n",
    "incorrect_input = \"The key to the cabinets\"\n",
    "# debug_prediction(\n",
    "#     model, tokenizer, \n",
    "#     incorrect_input,\n",
    "#     expected_output=\"is\",  # Correct singular verb\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8.2:** Analyze gender bias in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bias(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_templates: List[str],\n",
    "    method: str = 'integrated_gradients'\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze potential bias by comparing attributions across input variations.\n",
    "    \n",
    "    Example:\n",
    "        \"The doctor asked the nurse\" vs \"The nurse asked the doctor\"\n",
    "        Does the model attribute differently based on gender-stereotyped professions?\n",
    "    \"\"\"\n",
    "    # TODO: For each input template:\n",
    "    #   - Generate prediction\n",
    "    #   - Compute attribution\n",
    "    #   - Identify which words drive gendered predictions\n",
    "    # TODO: Compare attributions across templates\n",
    "    # TODO: Visualize bias patterns\n",
    "    \n",
    "    raise NotImplementedError(\"Implement bias analysis\")\n",
    "\n",
    "# Example: Gender bias in profession contexts\n",
    "# bias_templates = [\n",
    "#     \"The doctor told the nurse that he\",\n",
    "#     \"The doctor told the nurse that she\",\n",
    "#     \"The nurse told the doctor that he\",\n",
    "#     \"The nurse told the doctor that she\",\n",
    "# ]\n",
    "# analyze_bias(model, tokenizer, bias_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Apply Attribution to Your Project Concept\n",
    "\n",
    "Now apply attribution methods to understand your research concept.\n",
    "\n",
    "**Exercise 9.1:** Identify input features that activate your concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PROJECT APPLICATION ==========\n",
    "\n",
    "# Define your concept\n",
    "MY_CONCEPT = \"[Your concept here, e.g., 'politeness', 'causality', 'temporal reasoning']\"\n",
    "\n",
    "# Create test examples where your concept is present vs absent\n",
    "concept_present_examples = [\n",
    "    # TODO: Add 5-10 examples where your concept is clearly present\n",
    "    # Example: \"Could you please pass the salt?\" (politeness present)\n",
    "]\n",
    "\n",
    "concept_absent_examples = [\n",
    "    # TODO: Add 5-10 examples where your concept is absent\n",
    "    # Example: \"Pass the salt.\" (politeness absent)\n",
    "]\n",
    "\n",
    "def analyze_concept_features(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    present_examples: List[str],\n",
    "    absent_examples: List[str],\n",
    "    concept_name: str,\n",
    "    method: str = 'integrated_gradients'\n",
    "):\n",
    "    \"\"\"\n",
    "    Identify which input features are associated with your concept.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Compute attribution for each example (present and absent)\n",
    "    2. Identify tokens with consistently high attribution in 'present' examples\n",
    "    3. Check if those tokens have low attribution in 'absent' examples\n",
    "    4. These are candidate \"concept markers\"\n",
    "    \"\"\"\n",
    "    # TODO: Implement concept feature analysis\n",
    "    # 1. Compute attributions for all examples\n",
    "    # 2. Aggregate attribution by token type (e.g., average attribution for \"please\")\n",
    "    # 3. Compare present vs absent\n",
    "    # 4. Identify discriminative tokens\n",
    "    \n",
    "    raise NotImplementedError(\"Implement concept feature analysis\")\n",
    "\n",
    "# Run analysis\n",
    "# analyze_concept_features(\n",
    "#     model, tokenizer,\n",
    "#     concept_present_examples,\n",
    "#     concept_absent_examples,\n",
    "#     MY_CONCEPT\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9.2:** Integrate with previous weeks' findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_with_circuits(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    input_text: str,\n",
    "    circuit_components: List[Tuple[int, int]]  # (layer, head) pairs from Week 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine attribution (Week 9) with circuit analysis (Week 5).\n",
    "    \n",
    "    Questions to answer:\n",
    "    1. Do the circuit components attend to high-attribution input tokens?\n",
    "    2. Is there a mismatch (circuit attends elsewhere)?\n",
    "    3. Does this suggest the circuit computes something different than expected?\n",
    "    \"\"\"\n",
    "    # TODO: Compute input attribution (which tokens matter)\n",
    "    # TODO: Extract attention from circuit components\n",
    "    # TODO: Check if attention aligns with attribution\n",
    "    # TODO: Visualize alignment/mismatch\n",
    "    \n",
    "    raise NotImplementedError(\"Implement circuit-attribution integration\")\n",
    "\n",
    "def integrate_with_sae_features(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    sae_encoder: nn.Module,  # From Week 7\n",
    "    input_texts: List[str],\n",
    "    feature_idx: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine attribution (Week 9) with SAE features (Week 7).\n",
    "    \n",
    "    Questions to answer:\n",
    "    1. Which input tokens activate a specific SAE feature?\n",
    "    2. Does attribution reveal what information the feature captures?\n",
    "    \"\"\"\n",
    "    # TODO: For each input:\n",
    "    #   - Compute SAE feature activation\n",
    "    #   - Compute input attribution\n",
    "    #   - Correlate: do certain input tokens consistently activate the feature?\n",
    "    # TODO: Identify \"feature triggers\"\n",
    "    \n",
    "    raise NotImplementedError(\"Implement SAE-attribution integration\")\n",
    "\n",
    "# Example: If you found a circuit for your concept in Week 5\n",
    "# my_circuit = [(5, 2), (5, 7), (6, 11)]  # Example layer-head pairs\n",
    "# integrate_with_circuits(model, tokenizer, concept_present_examples[0], my_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Validation and Reporting\n",
    "\n",
    "Prepare attribution results for your research paper.\n",
    "\n",
    "**Exercise 10.1:** Create publication-ready attribution visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_publication_figure(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    examples: List[str],\n",
    "    methods: List[str] = ['integrated_gradients', 'attention'],\n",
    "    title: str = \"Attribution Analysis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a publication-quality figure showing attribution results.\n",
    "    \n",
    "    Good practices:\n",
    "    - Show multiple examples\n",
    "    - Compare methods\n",
    "    - Use clear color scales\n",
    "    - Annotate key findings\n",
    "    - Include error bars if using multiple runs\n",
    "    \"\"\"\n",
    "    # TODO: Compute attributions for all examples and methods\n",
    "    # TODO: Create multi-panel figure\n",
    "    # TODO: Add clear labels and legends\n",
    "    # TODO: Save as high-resolution PDF\n",
    "    \n",
    "    raise NotImplementedError(\"Implement publication figure\")\n",
    "\n",
    "# create_publication_figure(\n",
    "#     model, tokenizer,\n",
    "#     concept_present_examples[:3],  # Show 3 examples\n",
    "#     methods=['integrated_gradients', 'attention'],\n",
    "#     title=f\"Input Attribution for {MY_CONCEPT}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10.2:** Compute validation metrics for your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    test_examples: List[str],\n",
    "    method: str = 'integrated_gradients'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute validation metrics to report in your paper.\n",
    "    \n",
    "    Metrics to include:\n",
    "    1. Perturbation correlation (attribution vs ablation)\n",
    "    2. Cross-method agreement (IG vs attention, etc.)\n",
    "    3. Sanity check results (random model, random labels)\n",
    "    4. Stability (std across multiple runs with different random seeds)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of validation metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # TODO: Compute perturbation test correlation\n",
    "    # metrics['perturbation_corr'] = ...\n",
    "    \n",
    "    # TODO: Compute cross-method agreement\n",
    "    # metrics['method_agreement'] = ...\n",
    "    \n",
    "    # TODO: Run sanity checks\n",
    "    # metrics['random_model_corr'] = ...\n",
    "    \n",
    "    # TODO: Compute stability\n",
    "    # metrics['attribution_std'] = ...\n",
    "    \n",
    "    raise NotImplementedError(\"Implement validation metrics\")\n",
    "\n",
    "# Compute and report metrics\n",
    "# validation_metrics = compute_validation_metrics(model, tokenizer, concept_present_examples)\n",
    "# print(\"Validation Metrics for Paper:\")\n",
    "# for metric, value in validation_metrics.items():\n",
    "#     print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Research Guidelines\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Multiple methods:** Always use multiple attribution methods and report agreement\n",
    "2. **Validation:** Validate with ablation (ground truth) and sanity checks\n",
    "3. **Baselines matter:** Document and justify your baseline choice for IG\n",
    "4. **Correlation ≠ Causation:** Combine attribution with causal validation (Week 8)\n",
    "5. **Integration:** Use attribution to guide circuit discovery and SAE interpretation\n",
    "\n",
    "**For Your Research Paper:**\n",
    "\n",
    "✅ **DO:**\n",
    "- Report which methods you used and why\n",
    "- Include validation metrics (perturbation correlation, cross-method agreement)\n",
    "- Show multiple examples, not cherry-picked cases\n",
    "- Discuss limitations (baseline dependence, additivity assumptions)\n",
    "- Combine with causal interventions for strong claims\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Rely on a single attribution method\n",
    "- Make causal claims from correlation alone\n",
    "- Ignore sanity check failures\n",
    "- Over-interpret attention weights\n",
    "- Use LIME/SHAP for transformers without caveats\n",
    "\n",
    "**Next Steps:**\n",
    "1. Apply attribution to your project concept\n",
    "2. Integrate findings with circuits (Week 5), SAEs (Week 7), and IIA validation (Week 8)\n",
    "3. Prepare visualizations and validation metrics for your paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
