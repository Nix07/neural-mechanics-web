<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 6: Probes - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }

    .math {
      font-style: italic;
      text-align: center;
      margin: 10px 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 15px 0;
    }

    th,
    td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: left;
    }

    th {
      background-color: #f0f0f0;
    }

    .warning-box {
      background-color: #fff3cd;
      border-left: 4px solid #ff9800;
      padding: 15px;
      margin: 20px 0;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 6: Probes</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">← Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      Previous weeks focused on direct intervention: patching activations, tracing circuits, measuring causal effects.
      This week introduces <strong>auxiliary models</strong>—small models trained to help us understand larger ones.
      Probes extract specific information from representations, while masks identify which components matter. These
      methods are faster and differentiable, but require careful interpretation to avoid spurious conclusions.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Explain the role of auxiliary models in interpretability and when to use them vs direct intervention methods
      </li>
      <li>Implement linear probes to extract specific information from model representations</li>
      <li>Compare linear vs nonlinear (MLP) probes and explain the interpretability tradeoff</li>
      <li>Design probe training protocols: dataset construction, train/test splits, and control tasks</li>
      <li>Interpret probe accuracy: distinguish between "information is present" vs "information is used causally"</li>
      <li>Use control tasks to validate that probes measure intended concepts (e.g., selectivity tests)</li>
      <li>Implement learned masks to identify important components (neurons, heads, layers)</li>
      <li>Explain the difference between hard ablation, soft masking, and learned masks</li>
      <li>Apply regularization techniques (L0, L1) to encourage sparse, interpretable masks</li>
      <li>Compare probing results with causal intervention results: when do they agree/disagree?</li>
      <li>Identify probe pitfalls: overfitting (learning spurious correlations), underfitting (probe too simple), and the
        "information presence ≠ causal use" fallacy</li>
      <li>Use masking for automated component selection and pruning</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1711.11279" target="_blank">Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</a>
        </div>
        <div class="reading-description">Kim et al. (2018). Foundational probing method that asks "how sensitive is the model to this human-defined concept?"</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1909.03368" target="_blank">Designing and Interpreting Probes with Control Tasks</a>
        </div>
        <div class="reading-description">Hewitt &amp; Liang (2019). Critical methodology: how to validate probes and ensure they reveal genuine representations.</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/1905.06316" target="_blank">What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations</a>
        </div>
        <div class="reading-description">Tenney et al. (2019). The "edge probing" methodology for systematically studying encoded linguistic structures.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Auxiliary Models for Interpretability</h2>

    <h3>1. Why Use Auxiliary Models?</h3>
    <p>
      In previous weeks, we used <strong>direct intervention</strong>: patching activations, ablating components,
      tracing circuits. These methods are powerful but have limitations:
    </p>

    <ul>
      <li><strong>Expensive:</strong> Testing every component requires many forward passes</li>
      <li><strong>Discrete:</strong> Hard ablations aren't differentiable</li>
      <li><strong>Hypothesis-driven:</strong> You need to know what to look for</li>
    </ul>

    <p>
      <strong>Auxiliary models</strong> offer complementary benefits:
    </p>

    <ul>
      <li><strong>Efficient:</strong> Train once, test anywhere</li>
      <li><strong>Differentiable:</strong> Can use gradient-based optimization</li>
      <li><strong>Exploratory:</strong> Can discover patterns you didn't expect</li>
    </ul>

    <h4>Two Main Types</h4>
    <p><strong>Probes:</strong> Small models that read information from representations</p>
    <div class="diagram">
      Hidden State → [Probe] → Predicted Concept
    </div>

    <p><strong>Masks:</strong> Learned parameters that identify important components</p>
    <div class="diagram">
      Component × [Mask Weight] → Masked Component → Output
    </div>

    <h4>Key Question: What Do They Tell Us?</h4>
    <p>
      <strong>Critical distinction:</strong> Probes and masks show what <em>could be done</em> with representations,
      not necessarily what the model <em>actually does</em>. Always validate with causal interventions.
    </p>

    <h3>2. Linear Probes: Reading Out Information</h3>
    <p>
      A <strong>linear probe</strong> is the simplest auxiliary model: a linear classifier trained to extract specific
      information from hidden states.
    </p>

    <h4>The Setup</h4>
    <p>
      Given hidden states <code>h</code> at some layer, train a linear classifier to predict concept <code>y</code>:
    </p>

    <div class="math">
      ŷ = Wh + b
    </div>

    <p>
      Where <code>W</code> is a weight matrix and <code>b</code> is a bias vector.
    </p>

    <h4>Training Procedure</h4>
    <ol>
      <li><strong>Extract representations:</strong> Run model on labeled data, save hidden states</li>
      <li><strong>Freeze main model:</strong> Don't update the main model's weights</li>
      <li><strong>Train classifier:</strong> Optimize <code>W</code> and <code>b</code> to predict labels</li>
      <li><strong>Evaluate:</strong> Test on held-out data</li>
    </ol>

    <h4>What High Probe Accuracy Means</h4>
    <p>
      If a linear probe achieves high accuracy, it tells us:
    </p>
    <ul>
      <li>✓ The information is <strong>linearly accessible</strong> in the representation</li>
      <li>✓ A simple linear transformation can extract it</li>
      <li>✗ Does NOT prove the model uses this information causally</li>
      <li>✗ Does NOT prove the model uses it in this way</li>
    </ul>

    <h4>Example: Probing for Sentiment</h4>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      Text: "This movie was terrible"<br>
      Hidden state at layer 8: [h₁, h₂, ..., h₇₆₈]<br>
      Linear probe: ŷ = W·h + b<br>
      Prediction: Negative sentiment (98% confidence)
    </code>

    <p>
      <strong>Interpretation:</strong> Layer 8 contains linearly accessible sentiment information. But does the model
      actually <em>use</em> this information? Need to verify with interventions.
    </p>

    <h3>3. Nonlinear Probes: MLP Probes</h3>
    <p>
      Sometimes information isn't linearly accessible. <strong>MLP probes</strong> (multi-layer perceptrons) can
      extract nonlinear patterns.
    </p>

    <h4>Architecture</h4>
    <div class="math">
      ŷ = W₂ · ReLU(W₁h + b₁) + b₂
    </div>

    <p>
      This adds a hidden layer with nonlinear activation, allowing more complex transformations.
    </p>

    <h4>The Interpretability Tradeoff</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>Linear Probe</th>
        <th>MLP Probe</th>
      </tr>
      <tr>
        <td>Expressiveness</td>
        <td>Limited (linear only)</td>
        <td>High (nonlinear patterns)</td>
      </tr>
      <tr>
        <td>Interpretability</td>
        <td>Clear (linear direction)</td>
        <td>Opaque (nonlinear transformation)</td>
      </tr>
      <tr>
        <td>What it measures</td>
        <td>Linearly accessible info</td>
        <td>Computationally accessible info</td>
      </tr>
      <tr>
        <td>Overfitting risk</td>
        <td>Lower</td>
        <td>Higher</td>
      </tr>
    </table>

    <h4>When to Use Each</h4>
    <ul>
      <li><strong>Start with linear:</strong> Simpler, more interpretable, sufficient for many concepts</li>
      <li><strong>Try MLP if:</strong> Linear probe fails but you believe information is present</li>
      <li><strong>Compare both:</strong> Gap between linear and MLP performance reveals nonlinearity</li>
    </ul>

    <div class="warning-box">
      <strong>Warning:</strong> High MLP probe accuracy with low linear probe accuracy suggests information is present
      but not in a simple, interpretable form. The model may not use it the way your MLP probe does.
    </div>

    <h3>4. Probe Training: Methodology and Best Practices</h3>

    <h4>Dataset Construction</h4>
    <p><strong>1. Balanced classes:</strong></p>
    <ul>
      <li>Equal (or known) distribution of labels</li>
      <li>Prevents probe from learning spurious correlations</li>
      <li>Example: 50% positive, 50% negative sentiment</li>
    </ul>

    <p><strong>2. Minimal confounds:</strong></p>
    <ul>
      <li>Control for other variables that correlate with target</li>
      <li>Example: If probing for tense, balance for sentiment</li>
      <li>Otherwise probe might learn sentiment instead of tense</li>
    </ul>

    <p><strong>3. Diverse examples:</strong></p>
    <ul>
      <li>Different sentence structures, lengths, vocabularies</li>
      <li>Test generalization, not memorization</li>
    </ul>

    <h4>Train/Test Splits</h4>
    <ul>
      <li><strong>Standard split:</strong> 80% train, 10% validation, 10% test</li>
      <li><strong>Cross-validation:</strong> For small datasets, use k-fold CV</li>
      <li><strong>Distribution shift test:</strong> Create test set from different domain</li>
    </ul>

    <h4>Training Hyperparameters</h4>
    <ul>
      <li><strong>Learning rate:</strong> Start with 1e-3, tune if needed</li>
      <li><strong>Regularization:</strong> L2 penalty to prevent overfitting</li>
      <li><strong>Early stopping:</strong> Stop when validation accuracy plateaus</li>
      <li><strong>Batch size:</strong> 32-128 typically works well</li>
    </ul>

    <h3>5. Control Tasks: Validating Probe Behavior</h3>
    <p>
      <strong>Control tasks</strong> verify that probes actually measure what you think they measure, not spurious
      correlations.
    </p>

    <h4>Selectivity Test</h4>
    <p>
      <strong>Question:</strong> Does the probe specifically extract your target concept, or does it respond to other
      properties?
    </p>

    <p><strong>Method:</strong></p>
    <ol>
      <li>Train probe on your concept (e.g., "is sentence past tense?")</li>
      <li>Create control dataset with different concept (e.g., "is sentence negative?")</li>
      <li>Test probe on control dataset</li>
      <li>Probe should have ~random accuracy on control task</li>
    </ol>

    <p>
      <strong>If probe succeeds on control:</strong> It's not specifically measuring your concept, it's picking up a
      confound.
    </p>

    <h4>Random Label Test</h4>
    <p>
      <strong>Question:</strong> Is the probe overfitting to training data?
    </p>

    <p><strong>Method:</strong></p>
    <ol>
      <li>Randomize labels in training set</li>
      <li>Train probe on random labels</li>
      <li>If probe achieves high accuracy, it's overfitting</li>
      <li>Real patterns should not be learnable from random labels</li>
    </ol>

    <h4>Layer Progression Test</h4>
    <p>
      <strong>Question:</strong> Which layers contain the information?
    </p>

    <p><strong>Method:</strong></p>
    <ol>
      <li>Train probes at every layer</li>
      <li>Plot accuracy vs layer</li>
      <li>Reveals where information emerges and flows</li>
    </ol>

    <h3>6. Probe Pitfalls: What Can Go Wrong</h3>

    <h4>Pitfall 1: Overfitting</h4>
    <p>
      <strong>Problem:</strong> Probe learns spurious patterns specific to training data.
    </p>

    <p><strong>Symptoms:</strong></p>
    <ul>
      <li>High training accuracy, low test accuracy</li>
      <li>Probe uses many parameters (high capacity)</li>
      <li>Works on training distribution, fails on shifted distribution</li>
    </ul>

    <p><strong>Solutions:</strong></p>
    <ul>
      <li>Regularization (L2 penalty, dropout)</li>
      <li>More training data</li>
      <li>Simpler probe (linear instead of MLP)</li>
      <li>Early stopping based on validation set</li>
      <li>Cross-validation</li>
    </ul>

    <h4>Pitfall 2: Underfitting</h4>
    <p>
      <strong>Problem:</strong> Probe is too simple to extract available information.
    </p>

    <p><strong>Symptoms:</strong></p>
    <ul>
      <li>Low accuracy on both training and test sets</li>
      <li>Linear probe fails but concept seems extractable</li>
      <li>Gap between human intuition and probe performance</li>
    </ul>

    <p><strong>Solutions:</strong></p>
    <ul>
      <li>Try nonlinear probe (MLP)</li>
      <li>Increase probe capacity</li>
      <li>Check if information is actually present (use intervention)</li>
      <li>Try different layers</li>
      <li>Verify data quality and labels</li>
    </ul>

    <h4>Pitfall 3: The "Information Presence ≠ Causal Use" Fallacy</h4>
    <p>
      <strong>Problem:</strong> High probe accuracy doesn't prove the model uses that information.
    </p>

    <div class="diagram">
      Probe success: Information is ACCESSIBLE<br>
      ≠<br>
      Model uses it: Information is USED CAUSALLY
    </div>

    <p><strong>Example:</strong></p>
    <code style="display: block; margin: 10px 0; padding: 10px; background-color: #f4f4f4;">
      You can probe for "number of vowels in sentence"<br>
      Probe achieves 95% accuracy<br>
      → Information is linearly accessible<br>
      <br>
      But does the model use vowel count for predictions?<br>
      → Probably not! It's likely a spurious byproduct
    </code>

    <p><strong>Solution: Validate with interventions</strong></p>
    <ul>
      <li>Use activation patching to modify the probed direction</li>
      <li>If output changes as expected, information is causally used</li>
      <li>If output unchanged, information is merely present</li>
    </ul>

    <h4>Pitfall 4: Confounds and Spurious Correlations</h4>
    <p>
      <strong>Problem:</strong> Probe learns correlated features instead of target concept.
    </p>

    <p><strong>Example:</strong></p>
    <ul>
      <li>Goal: Probe for whether sentence mentions "France"</li>
      <li>Confound: Most France examples also mention "Paris"</li>
      <li>Probe might learn to detect "Paris" instead of "France"</li>
    </ul>

    <p><strong>Solutions:</strong></p>
    <ul>
      <li>Use control tasks to test selectivity</li>
      <li>Balance confounds in training data</li>
      <li>Test on adversarial examples (France without Paris)</li>
    </ul>

    <h3>7. Learned Masks: Identifying Important Components</h3>
    <p>
      Instead of hard ablation, <strong>learned masks</strong> use continuous parameters to identify which components
      matter.
    </p>

    <h4>The Setup</h4>
    <p>
      Each component (neuron, head, layer) gets a mask parameter <code>m ∈ [0, 1]</code>:
    </p>

    <div class="math">
      output = m × component_activation
    </div>

    <p>
      Where <code>m=1</code> means "keep component" and <code>m=0</code> means "ablate component".
    </p>

    <h4>Training Objective</h4>
    <p>
      Optimize masks to maintain model performance while minimizing mask sum:
    </p>

    <div class="math">
      Loss = Task_Loss + λ · Σ(mask_weights)
    </div>

    <ul>
      <li><strong>Task_Loss:</strong> Keep model predictions accurate</li>
      <li><strong>Σ(mask_weights):</strong> Prefer sparse masks (fewer active components)</li>
      <li><strong>λ:</strong> Regularization strength (controls sparsity)</li>
    </ul>

    <h4>Interpretation</h4>
    <ul>
      <li><strong>High mask weight (m ≈ 1):</strong> Component is important for task</li>
      <li><strong>Low mask weight (m ≈ 0):</strong> Component is not necessary</li>
      <li><strong>Intermediate weights:</strong> Component has some effect</li>
    </ul>

    <h3>8. Hard Ablation vs Soft Masking vs Learned Masks</h3>

    <table>
      <tr>
        <th>Method</th>
        <th>How it Works</th>
        <th>Advantages</th>
        <th>Disadvantages</th>
      </tr>
      <tr>
        <td><strong>Hard Ablation</strong></td>
        <td>Set activation to zero</td>
        <td>
          • Clear interpretation<br>
          • Causal claim<br>
          • No training needed
        </td>
        <td>
          • Not differentiable<br>
          • Expensive (test each)<br>
          • Binary (on/off)
        </td>
      </tr>
      <tr>
        <td><strong>Soft Masking</strong></td>
        <td>Multiply by weight m ∈ [0,1]</td>
        <td>
          • Differentiable<br>
          • Continuous effect<br>
          • Can interpolate
        </td>
        <td>
          • Still need to set m<br>
          • Interpretation less clear<br>
          • Not truly causal
        </td>
      </tr>
      <tr>
        <td><strong>Learned Masks</strong></td>
        <td>Train m to optimize objective</td>
        <td>
          • Automated discovery<br>
          • Differentiable<br>
          • Finds importance jointly
        </td>
        <td>
          • Requires training<br>
          • Task-specific<br>
          • Correlation not causation
        </td>
      </tr>
    </table>

    <h4>When to Use Each</h4>
    <ul>
      <li><strong>Hard ablation:</strong> Testing specific hypotheses, validating causal claims</li>
      <li><strong>Soft masking:</strong> Analyzing effect magnitude, interpolating between states</li>
      <li><strong>Learned masks:</strong> Exploratory discovery, automated pruning, gradient-based search</li>
    </ul>

    <h3>9. Sparse Regularization for Interpretable Masks</h3>
    <p>
      Without regularization, learned masks tend to keep most components active (not interpretable). <strong>Sparsity
        regularization</strong> encourages simpler solutions.
    </p>

    <h4>L1 Regularization</h4>
    <p>
      Add sum of absolute mask weights to loss:
    </p>

    <div class="math">
      Loss = Task_Loss + λ · Σ|mᵢ|
    </div>

    <p><strong>Effect:</strong> Drives small weights to exactly zero (sparse solution)</p>

    <h4>L0 Regularization (Approximate)</h4>
    <p>
      Penalize the <em>number</em> of non-zero weights:
    </p>

    <div class="math">
      Loss = Task_Loss + λ · ||m||₀
    </div>

    <p>
      Since L0 norm is non-differentiable, use continuous relaxations:
    </p>
    <ul>
      <li><strong>Hard concrete distribution:</strong> Samples binary masks during training</li>
      <li><strong>Sigmoid with temperature:</strong> Sharp sigmoid approximates step function</li>
    </ul>

    <h4>Choosing Regularization Strength (λ)</h4>
    <ul>
      <li><strong>Too low:</strong> Dense masks (many components active), not interpretable</li>
      <li><strong>Too high:</strong> Over-pruning, task performance degrades</li>
      <li><strong>Sweet spot:</strong> Maximum sparsity while maintaining performance</li>
    </ul>

    <p><strong>Practical approach:</strong></p>
    <ol>
      <li>Train with multiple λ values</li>
      <li>Plot: task accuracy vs number of active components</li>
      <li>Choose λ at the "elbow" of the curve</li>
    </ol>

    <h4>Benefits of Sparse Masks</h4>
    <ul>
      <li><strong>Interpretability:</strong> Easier to understand with fewer components</li>
      <li><strong>Efficiency:</strong> Can actually remove components (pruning)</li>
      <li><strong>Generalization:</strong> Simpler models often generalize better</li>
      <li><strong>Falsifiability:</strong> Clear claims about which components matter</li>
    </ul>

    <h3>10. Comparing Probes with Causal Interventions</h3>
    <p>
      Probes and interventions often give different answers. Understanding when and why reveals what each method
      measures.
    </p>

    <h4>Case 1: Agreement (Best Case)</h4>
    <div class="diagram">
      Probe: Concept X is linearly accessible at layer 8<br>
      Intervention: Patching layer 8 transfers concept X<br>
      → Information is both PRESENT and CAUSALLY USED
    </div>

    <p><strong>Interpretation:</strong> Strong evidence that layer 8 processes concept X.</p>

    <h4>Case 2: Probe Succeeds, Intervention Fails</h4>
    <div class="diagram">
      Probe: 95% accuracy for concept X<br>
      Intervention: Patching has no effect on X-related outputs<br>
      → Information is PRESENT but NOT CAUSALLY USED
    </div>

    <p><strong>Interpretation:</strong> The model computes X but doesn't use it for this task. It might be:</p>
    <ul>
      <li>A byproduct of other computations</li>
      <li>Used for different tasks/contexts</li>
      <li>Spuriously correlated information</li>
    </ul>

    <h4>Case 3: Probe Fails, Intervention Succeeds</h4>
    <div class="diagram">
      Probe: Low accuracy (random guessing)<br>
      Intervention: Patching strongly affects X-related outputs<br>
      → Information is USED but NOT LINEARLY ACCESSIBLE
    </div>

    <p><strong>Interpretation:</strong> The model uses X in a nonlinear or distributed way that linear probes can't
      extract. Try:</p>
    <ul>
      <li>Nonlinear (MLP) probes</li>
      <li>Different layers</li>
      <li>Multiple layers combined</li>
    </ul>

    <h4>Case 4: Both Fail</h4>
    <div class="diagram">
      Probe: Low accuracy<br>
      Intervention: No effect<br>
      → Information is ABSENT (or you're looking in the wrong place)
    </div>

    <h4>Best Practice: Use Both Methods</h4>
    <ol>
      <li><strong>Start with probes:</strong> Fast exploration, identify candidate layers</li>
      <li><strong>Validate with interventions:</strong> Test causal role of high-probe layers</li>
      <li><strong>Investigate disagreements:</strong> Learn about representational structure</li>
    </ol>

    <h3>11. Using Masks for Automated Discovery</h3>
    <p>
      Learned masks can automate component selection, complementing manual circuit discovery.
    </p>

    <h4>Workflow</h4>
    <ol>
      <li><strong>Define task:</strong> What behavior are you studying?</li>
      <li><strong>Initialize masks:</strong> One parameter per component (all start at 1.0)</li>
      <li><strong>Train masks:</strong> Optimize to maintain behavior with sparsity penalty</li>
      <li><strong>Threshold:</strong> Components with high masks are important</li>
      <li><strong>Validate:</strong> Use ablation to verify causal importance</li>
    </ol>

    <h4>Hierarchical Masking</h4>
    <p>
      Progressively narrow down from coarse to fine-grained:
    </p>
    <ol>
      <li><strong>Layer-level masks:</strong> Which layers matter?</li>
      <li><strong>Head-level masks:</strong> Within important layers, which heads?</li>
      <li><strong>Neuron-level masks:</strong> Within important heads, which neurons?</li>
    </ol>

    <p>
      This reduces search space and computational cost.
    </p>

    <h4>Comparison with ACDC (Week 5)</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>ACDC (Path Patching)</th>
        <th>Learned Masks</th>
      </tr>
      <tr>
        <td>Method</td>
        <td>Ablation-based</td>
        <td>Gradient-based</td>
      </tr>
      <tr>
        <td>Causal claim</td>
        <td>Strong (actual intervention)</td>
        <td>Weak (correlation)</td>
      </tr>
      <tr>
        <td>Speed</td>
        <td>Slow (test each edge)</td>
        <td>Fast (parallel gradient)</td>
      </tr>
      <tr>
        <td>Granularity</td>
        <td>Edges between components</td>
        <td>Individual components</td>
      </tr>
      <tr>
        <td>Best for</td>
        <td>Validating specific circuits</td>
        <td>Exploratory discovery</td>
      </tr>
    </table>

    <h3>12. Research Workflow: Combining Probes and Interventions</h3>
    <ol>
      <li><strong>Explore with probes:</strong>
        <ul>
          <li>Train probes at all layers for your concept</li>
          <li>Identify candidate layers with high accuracy</li>
          <li>Fast, covers entire model</li>
        </ul>
      </li>

      <li><strong>Discover with masks:</strong>
        <ul>
          <li>Train learned masks on concept-relevant task</li>
          <li>Identify important components (high mask weights)</li>
          <li>Hierarchical: layers → heads → neurons</li>
        </ul>
      </li>

      <li><strong>Validate with interventions:</strong>
        <ul>
          <li>Use activation patching on probe-identified layers</li>
          <li>Ablate mask-identified components</li>
          <li>Verify causal role</li>
        </ul>
      </li>

      <li><strong>Investigate disagreements:</strong>
        <ul>
          <li>If probe succeeds but intervention fails: information present but unused</li>
          <li>If intervention succeeds but probe fails: nonlinear or distributed representation</li>
          <li>Both reveal important properties of how the model works</li>
        </ul>
      </li>

      <li><strong>Iterate:</strong>
        <ul>
          <li>Use findings to refine hypotheses</li>
          <li>Design better probes based on intervention results</li>
          <li>Test new components suggested by masks</li>
        </ul>
      </li>
    </ol>

    <p>
      <strong>Goal:</strong> Triangulate understanding using multiple methods. No single technique tells the whole
      story.
    </p>
  </section>


  <section id="tcav-cbms">
    <h2>Extension: Concept-Based Methods (TCAV and Concept Bottleneck Models)</h2>

    <h3>Beyond Token-Level Probes: Concept Activation Vectors</h3>
    <p>
      Probes answer: "Is information X encoded at layer L?" But what if you want to test for higher-level concepts like
      "politeness," "factual knowledge," or "sentiment"? <strong>Testing with Concept Activation Vectors (TCAV)</strong>
      extends probing to human-interpretable concepts.
    </p>

    <h3>1. TCAV: Testing with Concept Activation Vectors (Kim et al., 2018)</h3>

    <h4>The Core Idea</h4>
    <p>
      Instead of training a probe on labeled data, TCAV uses <strong>user-provided examples</strong> to define concepts:
    </p>

    <div class="diagram">
      <strong>Traditional Probe:</strong> "Is part-of-speech encoded?" → Need labeled POS data<br><br>
      <strong>TCAV:</strong> "Does the model use 'politeness'?" → Provide examples of polite/rude text
    </div>

    <h4>The TCAV Method</h4>
    <ol>
      <li><strong>Collect concept examples:</strong>
        <ul>
          <li>Positive examples: sentences exhibiting your concept (e.g., polite requests)</li>
          <li>Negative examples: random or concept-absent sentences</li>
        </ul>
      </li>

      <li><strong>Extract activations:</strong>
        <ul>
          <li>Run model on all examples</li>
          <li>Collect activations at target layer(s)</li>
        </ul>
      </li>

      <li><strong>Train linear classifier:</strong>
        <ul>
          <li>Separate positive from negative examples</li>
          <li>The decision boundary normal vector is the <strong>Concept Activation Vector (CAV)</strong></li>
        </ul>
      </li>

      <li><strong>Compute TCAV score:</strong>
        <ul>
          <li>For a given prediction task, measure: <code>TCAV = fraction of examples where gradient aligns with CAV</code></li>
          <li>High TCAV → model uses this concept for predictions</li>
        </ul>
      </li>
    </ol>

    <h4>Example: Detecting Sentiment Concepts</h4>
    <div class="diagram">
      <strong>Concept:</strong> "Positive sentiment"<br>
      <strong>Positive examples:</strong> "Great!", "I love this", "Excellent work"<br>
      <strong>Negative examples:</strong> Random sentences<br><br>

      <strong>Train CAV</strong> → direction in layer 10 that separates these<br><br>

      <strong>Test:</strong> For sentiment classification task, do gradients point toward CAV?<br>
      → If TCAV = 0.82, then 82% of examples use "positive sentiment" concept
    </div>

    <h4>TCAV vs Standard Probes</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>Standard Probe</th>
        <th>TCAV</th>
      </tr>
      <tr>
        <td><strong>Data needed</strong></td>
        <td>Labeled dataset</td>
        <td>Concept examples (no labels needed)</td>
      </tr>
      <tr>
        <td><strong>Question</strong></td>
        <td>"Is X encoded?"</td>
        <td>"Is X used causally?"</td>
      </tr>
      <tr>
        <td><strong>Measure</strong></td>
        <td>Probe accuracy</td>
        <td>TCAV score (gradient alignment)</td>
      </tr>
      <tr>
        <td><strong>Concept flexibility</strong></td>
        <td>Fixed to dataset labels</td>
        <td>User-defined concepts</td>
      </tr>
      <tr>
        <td><strong>Causal claim</strong></td>
        <td>Weaker (encoding ≠ use)</td>
        <td>Stronger (tests gradient direction)</td>
      </tr>
    </table>

    <h4>Limitations of TCAV</h4>
    <ul>
      <li><strong>Concept definition:</strong> Results depend on how you define concept examples</li>
      <li><strong>Linear assumption:</strong> Assumes concept is a linear direction (may miss nonlinear concepts)</li>
      <li><strong>Statistical testing:</strong> Requires enough examples for reliable CAV</li>
      <li><strong>Confounds:</strong> Concept examples may differ in unintended ways (length, frequency, structure)</li>
    </ul>

    <h3>2. Concept Bottleneck Models (CBMs)</h3>

    <p>
      While TCAV tests concepts <em>post-hoc</em> in trained models, <strong>Concept Bottleneck Models (CBMs)</strong>
      build concepts <em>into</em> the architecture during training.
    </p>

    <h4>The CBM Architecture</h4>
    <div class="diagram">
      Input → Concept Layer (interpretable) → Prediction Layer<br><br>

      <strong>Example (image classification):</strong><br>
      Image → [has_beak, has_wings, is_colorful, ...] → Bird species
    </div>

    <p>
      The middle layer is constrained to represent <strong>predefined concepts</strong> (e.g., object attributes). This
      makes the model inherently interpretable.
    </p>

    <h4>Advantages of CBMs</h4>
    <ul>
      <li><strong>Interpretability by design:</strong> Predictions explicitly flow through concepts</li>
      <li><strong>Human intervention:</strong> Users can correct concept predictions at test time</li>
      <li><strong>Debugging:</strong> If model fails, inspect which concept was wrong</li>
      <li><strong>Transfer:</strong> Concept representations can transfer to new tasks</li>
    </ul>

    <h4>Challenges with CBMs</h4>
    <ul>
      <li><strong>Concept annotation:</strong> Requires labeled concept data (expensive)</li>
      <li><strong>Concept completeness:</strong> Must define all relevant concepts upfront</li>
      <li><strong>Performance tradeoff:</strong> May sacrifice accuracy vs end-to-end models</li>
      <li><strong>Concept leakage:</strong> Information may bypass concept bottleneck through residual connections</li>
    </ul>

    <h4>CBMs for LLMs</h4>
    <p>
      While CBMs originated in vision, they're being adapted for NLP:
    </p>
    <ul>
      <li><strong>Text classification:</strong> Route through semantic concepts (topic, sentiment, formality)</li>
      <li><strong>Question answering:</strong> Explicitly extract relevant facts before answering</li>
      <li><strong>Controllable generation:</strong> Condition on concept activations</li>
    </ul>

    <h3>3. Validating Concept Methods</h3>

    <p>
      Concept-based methods inherit validation challenges from probes, plus additional concerns:
    </p>

    <h4>Validation Checklist for TCAV/CBMs</h4>
    <div class="assignment-box">
      <ol>
        <li><strong>Concept definition:</strong>
          <ul>
            <li>Are positive examples truly representative of the concept?</li>
            <li>Do negative examples appropriately contrast?</li>
            <li>Test on held-out concept examples</li>
          </ul>
        </li>

        <li><strong>Confound control:</strong>
          <ul>
            <li>Do concept examples differ only in the target concept?</li>
            <li>Test with minimal pairs (Week 4 counterfactuals)</li>
            <li>Check for length, frequency, and structural confounds</li>
          </ul>
        </li>

        <li><strong>Causal validation (TCAV):</strong>
          <ul>
            <li>High TCAV score suggests concept is used, but verify with intervention</li>
            <li>Try steering along CAV direction - does behavior change as predicted?</li>
            <li>Ablate CAV direction - does it break concept-related predictions?</li>
          </ul>
        </li>

        <li><strong>Completeness (CBMs):</strong>
          <ul>
            <li>Measure concept leakage: does model bypass concept bottleneck?</li>
            <li>Test with concept interventions: manually set concept values</li>
            <li>Verify predictions change appropriately</li>
          </ul>
        </li>
      </ol>
    </div>

    <h3>4. Integrating TCAV with Other Methods</h3>

    <p>
      TCAV is most powerful when combined with other interpretability techniques:
    </p>

    <table>
      <tr>
        <th>Combine With</th>
        <th>What You Learn</th>
        <th>Example</th>
      </tr>
      <tr>
        <td><strong>Probes (this week)</strong></td>
        <td>Is concept encoded AND used?</td>
        <td>Probe shows "politeness" is encoded in layer 8.<br>TCAV shows it's used for formality predictions.</td>
      </tr>
      <tr>
        <td><strong>Patching (Week 4)</strong></td>
        <td>Which components implement the concept?</td>
        <td>TCAV identifies concept direction.<br>Patch along that direction to verify causality.</td>
      </tr>
      <tr>
        <td><strong>Circuits (Week 8)</strong></td>
        <td>What circuit computes the concept?</td>
        <td>TCAV finds concept, circuits show how it's computed.</td>
      </tr>
      <tr>
        <td><strong>SAEs (Week 7)</strong></td>
        <td>Do discovered features align with concepts?</td>
        <td>Compare SAE features with CAVs for same concept.</td>
      </tr>
    </table>

    <h3>5. Application to Your Project</h3>

    <p>
      If your concept is high-level or domain-specific, TCAV may be more appropriate than standard probes:
    </p>

    <div class="example-box" style="background-color: #f9f9f9; border: 1px solid #ddd; padding: 15px; margin: 20px 0;">
      <p><strong>Example: Musical Key</strong></p>
      <ul>
        <li><strong>Standard probe:</strong> Need labeled dataset of texts annotated with musical keys (hard to get)</li>
        <li><strong>TCAV:</strong> Provide examples of texts in C major, D major, etc. → easier</li>
        <li>Train CAV, test if model uses "key" concept for music-related predictions</li>
      </ul>

      <p><strong>Example: Scientific Reasoning</strong></p>
      <ul>
        <li><strong>Concept:</strong> "Causal reasoning" in scientific texts</li>
        <li>Collect examples with causal language ("because," "leads to," "causes")</li>
        <li>Train CAV, measure TCAV score for scientific QA task</li>
        <li>High score → model relies on causal reasoning; Low score → uses other heuristics</li>
      </ul>
    </div>

    <h3>Looking Ahead: Validation and Skepticism</h3>
    <p>
      Like standard probes, concept methods can mislead if not carefully validated. We'll see in <strong>Week 6
      (Skepticism)</strong> that even compelling concept interpretations can be illusory. Apply the validation
      framework from Week 4 rigorously:
    </p>

    <ul>
      <li>Run sanity checks (random model test)</li>
      <li>Test on multiple concept example sets</li>
      <li>Validate with causal interventions</li>
      <li>Compare TCAV with probe results</li>
      <li>Report when concept detection fails</li>
    </ul>
  </section>
  <section id="in-class-exercise">
    <h2>In-Class Exercise: Training a Pun Detector Probe</h2>
    <p>
      Building on our pun dataset and geometric analysis, we will train probes to detect puns from model activations
      and analyze whether pun understanding is linearly accessible.
    </p>

    <h3>Part 1: Probe Training Setup (15 min)</h3>
    <p>
      Prepare your pun dataset for probe training:
    </p>
    <ol>
      <li><strong>Load your pun dataset:</strong> Use examples from Weeks 3-4 (at least 100 puns, 100 non-puns)</li>
      <li><strong>Extract activations:</strong> Run all examples through the model, save activations at key layers
        <ul>
          <li>Focus on layers identified as important in Week 5's causal analysis</li>
          <li>Extract from the final token position (punchline)</li>
        </ul>
      </li>
      <li><strong>Split data:</strong> 70% train, 15% validation, 15% test</li>
    </ol>

    <h3>Part 2: Train and Evaluate Probes (25 min)</h3>
    <p>
      Train linear probes across multiple layers:
    </p>
    <ol>
      <li><strong>Train layer-by-layer:</strong>
        <ul>
          <li>For layers 0, 5, 10, 15, 20, 25, 30 (adjust for your model size)</li>
          <li>Train a logistic regression classifier: pun (1) vs non-pun (0)</li>
        </ul>
      </li>
      <li><strong>Evaluate performance:</strong>
        <ul>
          <li>Report accuracy on test set for each layer</li>
          <li>Create layer-wise accuracy plot</li>
        </ul>
      </li>
      <li><strong>Compare with Week 4:</strong>
        <ul>
          <li>Does probe accuracy peak at the same layers where PCA showed separation?</li>
          <li>Compare probe weight direction to your mass-mean-difference "pun direction"</li>
          <li>Compute cosine similarity between probe weights and pun direction</li>
        </ul>
      </li>
    </ol>

    <h3>Part 3: Control Tasks and Validation (20 min)</h3>
    <p>
      Validate that your probe is detecting humor, not confounds:
    </p>
    <ol>
      <li><strong>Random label baseline:</strong>
        <ul>
          <li>Shuffle labels and retrain probes</li>
          <li>If accuracy is still high, probe is overfitting</li>
        </ul>
      </li>
      <li><strong>Selectivity test:</strong>
        <ul>
          <li>Does your pun probe fire on non-pun wordplay?</li>
          <li>Does it fire on jokes that are not puns?</li>
          <li>Test on edge cases to understand what it really detects</li>
        </ul>
      </li>
      <li><strong>Linear vs MLP comparison:</strong>
        <ul>
          <li>Train a 1-hidden-layer MLP probe at the best layer</li>
          <li>Is accuracy much higher? If so, pun representation may be nonlinear</li>
        </ul>
      </li>
      <li><strong>Causal validation:</strong>
        <ul>
          <li>Use your probe weight vector as a steering direction</li>
          <li>Does steering along this direction affect pun-like outputs?</li>
        </ul>
      </li>
    </ol>

    <p>
      <strong>Discussion:</strong> What does probe accuracy tell us about how the model represents humor?
      Is "pun detection" a linearly accessible concept, or does the model use more complex representations?
    </p>

    <a href="https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week6/probe_training_ndif.ipynb" target="_blank" class="colab-button">
      Open Probe Training Notebook in Colab
    </a>
  </section>

  <section id="code-exercise">
    <h2>Code Exercise</h2>
    <p>
      This week's exercise provides hands-on experience with probes and masks:
    </p>
    <ul>
      <li>Train linear probes for concept detection across layers</li>
      <li>Compare linear vs MLP probe performance</li>
      <li>Implement control tasks to validate probes</li>
      <li>Test for overfitting and underfitting</li>
      <li>Train learned masks with L1/L0 regularization</li>
      <li>Compare probe findings with intervention results</li>
      <li>Identify and interpret disagreements between methods</li>
    </ul>

    <a href="https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week6/probe_training_ndif.ipynb" target="_blank" class="colab-button">
      Open Exercise in Colab
    </a>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 5</strong></p>
      <p>
        Train linear probes to extract your concept from model activations. Analyze probe weights and decision boundaries
        to understand how the concept is encoded, and test where the concept is linearly accessible.
      </p>

      <h4>Probe Training and Analysis</h4>
      <ul>
        <li><strong>Prepare training data:</strong>
          <ul>
            <li>Use your benchmark dataset (minimum 2000-5000 examples recommended)</li>
            <li>Ensure balanced classes (equal positive and negative examples)</li>
            <li>Split into train/validation/test sets (70/15/15 or similar)</li>
          </ul>
        </li>
        <li><strong>Train probes across layers:</strong>
          <ul>
            <li>Train linear probes at every layer (or every N layers for large models)</li>
            <li>Focus especially on layers identified as important in Week 4's causal analysis</li>
            <li>Train on the same token positions you identified as critical in Week 4</li>
          </ul>
        </li>
        <li><strong>Evaluate probe performance:</strong>
          <ul>
            <li>Report accuracy, precision, recall, F1 for each layer</li>
            <li>Create layer-wise accuracy plot to visualize where concept is most accessible</li>
            <li>Compare across different token positions</li>
          </ul>
        </li>
        <li><strong>Analyze probe weights and decision boundaries:</strong>
          <ul>
            <li>Examine probe weight vectors: what directions do they learn?</li>
            <li>Compare to steering vectors from Week 1: are they similar?</li>
            <li>Visualize decision boundaries in PCA space (from Week 3)</li>
            <li>Are decision boundaries clean and linear, or complex?</li>
          </ul>
        </li>
        <li><strong>Control experiments:</strong>
          <ul>
            <li>Random label baseline: does probe learn structure or just memorize?</li>
            <li>Compare linear vs nonlinear (MLP) probes: is concept linearly represented?</li>
            <li>Selectivity test: does probe respond to related but distinct concepts?</li>
          </ul>
        </li>
      </ul>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Probe performance results:</strong>
          <ul>
            <li>Layer-wise accuracy plot</li>
            <li>Performance table (accuracy/F1) for key layers</li>
            <li>Comparison across token positions</li>
          </ul>
        </li>
        <li><strong>Probe analysis:</strong>
          <ul>
            <li>Weight vector analysis and comparison to steering vectors</li>
            <li>Decision boundary visualizations</li>
            <li>Linear vs nonlinear probe comparison</li>
          </ul>
        </li>
        <li><strong>Control experiment results:</strong>
          <ul>
            <li>Random baseline performance</li>
            <li>Selectivity test results</li>
          </ul>
        </li>
        <li><strong>Interpretation:</strong>
          <ul>
            <li>Where is your concept most linearly accessible?</li>
            <li>Does this match causal findings from Week 4?</li>
            <li>Is the representation truly linear or does it require nonlinearity?</li>
          </ul>
        </li>
        <li><strong>Code:</strong> Notebook with probe training, evaluation, and analysis</li>
      </ul>

      <p><em>
        Strong probe performance with clean linear decision boundaries suggests your concept has a simple geometric structure
        that can be reliably extracted—ideal for building explanations and interventions.
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">← Back to Course Home</a></p>
  </footer>

</body>

</html>
