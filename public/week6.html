<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Week 6: Input Attribution and Saliency Methods + Skepticism</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: 'Georgia', serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      border-bottom: 3px solid #3498db;
      padding-bottom: 10px;
    }

    h2 {
      color: #34495e;
      margin-top: 30px;
      border-bottom: 2px solid #bdc3c7;
      padding-bottom: 5px;
    }

    h3 {
      color: #555;
      margin-top: 20px;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-box {
      background-color: #e8f4f8;
      border-left: 4px solid #3498db;
      padding: 15px;
      margin: 20px 0;
    }

    .warning-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .comparison-box {
      background-color: #f0f7ff;
      border: 1px solid #3498db;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    ul,
    ol {
      margin: 10px 0;
    }

    li {
      margin: 8px 0;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    pre {
      background-color: #f4f4f4;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }

    .objectives {
      background-color: #f0f7ff;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
    }

    .method-card {
      background-color: #fff;
      border: 2px solid #e0e0e0;
      border-radius: 8px;
      padding: 15px;
      margin: 15px 0;
    }

    .method-card h4 {
      margin-top: 0;
      color: #2c3e50;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
    }
  </style>
</head>

<body>
  <h1>Week 6: Input Attribution and Saliency Methods + Skepticism</h1>

  <div class="objectives">
    <h3>Learning Objectives</h3>
    <ul>
      <li>Understand what input attribution is and when to use it vs other interpretability methods</li>
      <li>Master gradient-based attribution methods (saliency, integrated gradients, DeepLIFT)</li>
      <li>Apply perturbation-based methods (LIME, SHAP, ablation) to LLMs</li>
      <li>Use attention-based attribution (attention rollout, attention flow)</li>
      <li>Work effectively with the Inseq library for sequence generation models</li>
      <li>Critically evaluate attribution methods and understand their limitations</li>
      <li>Address technical challenges (baseline selection, gradient saturation, computational cost)</li>
      <li>Apply attribution to debug model predictions and analyze your project concepts</li>
      <li>Integrate attribution with previous weeks' methods (circuits, probes, SAEs, causal validation)</li>
    </ul>
  </div>

  <h2>1. What is Input Attribution?</h2>

  <h3>The Core Question</h3>
  <p>
    <strong>Input attribution</strong> answers: <em>"Which parts of the input were most important for the model's
      output?"</em>
  </p>

  <p>
    For an LLM generating text, attribution identifies which input tokens influenced each output token. This helps us:
  </p>
  <ul>
    <li><strong>Debug predictions:</strong> Why did the model generate this specific word?</li>
    <li><strong>Detect bias:</strong> Is the model relying on demographic features inappropriately?</li>
    <li><strong>Validate understanding:</strong> Does the model attend to the right context?</li>
    <li><strong>Guide interventions:</strong> Which inputs should we modify to change behavior?</li>
  </ul>

  <h3>Attribution vs Other Interpretability Methods</h3>

  <div class="comparison-box">
    <table>
      <tr>
        <th>Method</th>
        <th>Question Answered</th>
        <th>Granularity</th>
      </tr>
      <tr>
        <td><strong>Attribution</strong><br>(This week)</td>
        <td>Which <em>inputs</em> matter?</td>
        <td>Input tokens</td>
      </tr>
      <tr>
        <td><strong>Circuits</strong><br>(Week 5)</td>
        <td>Which <em>components</em> compute the output?</td>
        <td>Attention heads, MLPs</td>
      </tr>
      <tr>
        <td><strong>Probes</strong><br>(Week 6)</td>
        <td>What <em>information</em> is encoded?</td>
        <td>Layer activations</td>
      </tr>
      <tr>
        <td><strong>SAEs</strong><br>(Week 7)</td>
        <td>What <em>features</em> are represented?</td>
        <td>Sparse feature directions</td>
      </tr>
      <tr>
        <td><strong>Causal Validation</strong><br>(Week 8)</td>
        <td>Are findings <em>causal</em>?</td>
        <td>Variable-level</td>
      </tr>
    </table>
  </div>

  <p>
    <strong>Key insight:</strong> Attribution is <em>input-level</em> explanation. It tells you <em>what</em> the model
    used, not <em>how</em> it was processed internally. Combine attribution with circuit analysis (Week 5) to get the
    full picture.
  </p>

  <h2>2. Gradient-Based Attribution Methods</h2>

  <p>
    Gradient-based methods use the model's gradients to measure input importance. Intuitively: <em>if changing an input
      token would change the output a lot (high gradient), that token is important</em>.
  </p>

  <h3>2.1 Saliency Maps (Vanilla Gradients)</h3>

  <div class="method-card">
    <h4>Method: Saliency Maps</h4>
    <p><strong>Idea:</strong> The gradient magnitude indicates importance.</p>
    <p><strong>Formula:</strong> \( \text{Saliency}(x_i) = \left| \frac{\partial f(x)}{\partial x_i} \right| \)</p>
    <p><strong>Pros:</strong> Simple, fast, easy to implement</p>
    <p><strong>Cons:</strong> Suffers from gradient saturation (near-zero gradients in deep networks)</p>
  </div>

  <div class="example-box">
    <p><strong>Example:</strong></p>
    <p><em>Input:</em> "The cat sat on the mat"</p>
    <p><em>Output:</em> "because" (predicted next token)</p>
    <p><em>Saliency scores:</em> [0.02, <strong>0.35</strong>, 0.15, 0.08, <strong>0.40</strong>, 0.12]</p>
    <p><em>Interpretation:</em> "cat" and "mat" have highest gradients → most influential for predicting "because"</p>
  </div>

  <div class="warning-box">
    <p><strong>Problem: Gradient Saturation</strong></p>
    <p>
      Deep networks with ReLU/sigmoid activations often have near-zero gradients even when inputs are important. This
      happens because the function flattens (saturates) in certain regions.
    </p>
    <p><em>Solution:</em> Use Integrated Gradients or other methods that accumulate gradients along a path.</p>
  </div>

  <h3>2.2 Input × Gradient</h3>

  <div class="method-card">
    <h4>Method: Input × Gradient</h4>
    <p><strong>Idea:</strong> Weight gradients by input magnitude to get better signal.</p>
    <p><strong>Formula:</strong> \( \text{Attribution}(x_i) = x_i \cdot \frac{\partial f(x)}{\partial x_i} \)</p>
    <p><strong>Pros:</strong> Better handles zero-input cases than vanilla gradients</p>
    <p><strong>Cons:</strong> Still affected by saturation</p>
  </div>

  <h3>2.3 Integrated Gradients (IG)</h3>

  <div class="method-card">
    <h4>Method: Integrated Gradients</h4>
    <p><strong>Idea:</strong> Accumulate gradients along a straight line from a baseline to the input.</p>
    <p><strong>Formula:</strong>
      \[
      \text{IG}(x_i) = (x_i - x_i') \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha \cdot (x - x'))}{\partial x_i}
      \, d\alpha
      \]
      where \( x' \) is a baseline input (e.g., all zeros or padding tokens).
    </p>
    <p><strong>Pros:</strong> Theoretically sound (satisfies completeness axiom), mitigates saturation</p>
    <p><strong>Cons:</strong> Computationally expensive, baseline-dependent, assumes linear path</p>
  </div>

  <div class="info-box">
    <h4>Why Integrated Gradients Works</h4>
    <p>
      By integrating gradients along a path, IG avoids the saturation problem of vanilla gradients. Even if the
      gradient is zero at the input, IG captures importance by looking at gradients throughout the interpolation.
    </p>
    <p>
      <strong>Completeness property:</strong> The sum of attributions equals the difference in model output between
      input and baseline:
      \[
      \sum_i \text{IG}(x_i) = f(x) - f(x')
      \]
    </p>
  </div>

  <h3>2.4 The Baseline Selection Problem</h3>

  <p>
    A critical challenge for IG: <strong>what baseline should we use?</strong>
  </p>

  <div class="warning-box">
    <p><strong>Baseline choices for LLMs:</strong></p>
    <ul>
      <li><strong>Zero embeddings:</strong> Common but may not be semantically meaningful</li>
      <li><strong>Padding tokens:</strong> Model-specific, represents "no information"</li>
      <li><strong>Mask tokens:</strong> For masked language models (BERT-style)</li>
      <li><strong>Average embeddings:</strong> Represents "typical" input</li>
      <li><strong>Random text:</strong> Contrast against unrelated content</li>
    </ul>
    <p>
      <strong>Impact:</strong> Baseline choice can dramatically change attribution scores. Always justify your baseline!
    </p>
  </div>

  <h3>2.5 Uniform Discretized Integrated Gradients (UDIG)</h3>

  <div class="method-card">
    <h4>Method: UDIG (2024)</h4>
    <p><strong>Problem with standard IG:</strong> Linear interpolation through embedding space doesn't respect the
      discrete, linguistic nature of words. Intermediate points may not correspond to real words.</p>
    <p><strong>Solution:</strong> Use a <em>nonlinear path</em> that stays closer to actual word embeddings.</p>
    <p><strong>Result:</strong> Better performance on NLP tasks (sentiment, QA) compared to standard IG.</p>
    <p><strong>When to use:</strong> For language models where discrete token structure matters.</p>
  </div>

  <h3>2.6 Other Gradient Methods</h3>

  <ul>
    <li><strong>DeepLIFT:</strong> Compares activations to reference activations, decomposes prediction differences</li>
    <li><strong>GradientSHAP:</strong> Combines gradients with Shapley value sampling</li>
    <li><strong>Layer-wise methods:</strong> LayerIntegratedGradients, LayerGradientXActivation for internal layers</li>
  </ul>

  <h2>3. Perturbation-Based Attribution Methods</h2>

  <p>
    Perturbation methods <strong>modify inputs</strong> and observe how outputs change. No gradients needed—purely
    empirical.
  </p>

  <h3>3.1 Occlusion / Ablation</h3>

  <div class="method-card">
    <h4>Method: Token Ablation</h4>
    <p><strong>Idea:</strong> Remove (or mask) each token and measure output change.</p>
    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Run model on full input → get output probability \( p \)</li>
      <li>For each token \( i \):
        <ul>
          <li>Remove or mask token \( i \)</li>
          <li>Run model → get new probability \( p_i \)</li>
          <li>Attribution \( = p - p_i \) (drop in probability)</li>
        </ul>
      </li>
    </ol>
    <p><strong>Pros:</strong> Intuitive, model-agnostic, no gradient computation</p>
    <p><strong>Cons:</strong> Computationally expensive (\( O(n) \) forward passes for \( n \) tokens), may create
      unnatural inputs</p>
  </div>

  <h3>3.2 LIME (Local Interpretable Model-Agnostic Explanations)</h3>

  <div class="method-card">
    <h4>Method: LIME</h4>
    <p><strong>Idea:</strong> Fit a simple linear model <em>locally</em> around the input to approximate the model's
      behavior.</p>
    <p><strong>Procedure:</strong></p>
    <ol>
      <li>Generate perturbed inputs by randomly masking tokens</li>
      <li>Run model on perturbed inputs to get outputs</li>
      <li>Fit a weighted linear model: \( g(x') \approx f(x) \) where \( x' \) is in the perturbation neighborhood</li>
      <li>Linear coefficients = token importance</li>
    </ol>
    <p><strong>Pros:</strong> Model-agnostic, interpretable coefficients</p>
    <p><strong>Cons:</strong> Requires many model calls, local approximation may be inaccurate, random sampling
      variability</p>
  </div>

  <h3>3.3 SHAP (Shapley Additive Explanations)</h3>

  <div class="method-card">
    <h4>Method: SHAP</h4>
    <p><strong>Idea:</strong> Use game-theoretic Shapley values to assign fair credit to each token.</p>
    <p><strong>Formula:</strong> For each token \( i \), compute contribution by averaging over all possible subsets:
      \[
      \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! (|N| - |S| - 1)!}{|N|!} [f(S \cup \{i\}) - f(S)]
      \]
    </p>
    <p><strong>Pros:</strong> Theoretically grounded (unique solution satisfying fairness axioms), consistent</p>
    <p><strong>Cons:</strong> Exponentially expensive to compute exactly (approximations needed), requires defining
      "coalition" semantics for tokens</p>
  </div>

  <div class="warning-box">
    <h4>Critical Limitation: Additivity Assumption</h4>
    <p>
      Recent research (2024) shows that <strong>transformers cannot represent additive models</strong> due to their
      architecture (attention, LayerNorm). This casts doubt on LIME and SHAP's applicability, which assume local
      additivity.
    </p>
    <p>
      <strong>Implication:</strong> Use LIME/SHAP cautiously for transformers. Validate with other methods.
    </p>
  </div>

  <h3>3.4 ReAGent (Replace with Alternatives)</h3>

  <div class="method-card">
    <h4>Method: ReAGent</h4>
    <p><strong>Idea:</strong> Replace each token with plausible alternatives (e.g., from a masked LM) rather than just
      removing it.</p>
    <p><strong>Advantage:</strong> More natural perturbations than simple masking.</p>
    <p><strong>When to use:</strong> When you want to measure importance without creating out-of-distribution inputs.
    </p>
  </div>

  <h2>4. Attention-Based Attribution</h2>

  <p>
    Can we use <strong>attention weights</strong> as attribution? The answer is <em>complicated</em>.
  </p>

  <h3>4.1 Raw Attention Weights</h3>

  <div class="warning-box">
    <h4>Problem: Attention is Not Explanation</h4>
    <p>
      In transformers, <strong>information from different tokens gets increasingly mixed</strong> across layers through
      attention and residual connections. By the final layer, it's unclear which input tokens contributed to a
      representation.
    </p>
    <p>
      Raw attention weights show what the model <em>attends to</em> in a single layer, but they don't track information
      flow from input to output across the full network.
    </p>
  </div>

  <p>
    <strong>When raw attention is useful:</strong>
  </p>
  <ul>
    <li>Single-layer analysis (e.g., "does this head attend to previous token?")</li>
    <li>Qualitative exploration</li>
    <li>Sanity checks (e.g., "is the model attending to padding?")</li>
  </ul>

  <h3>4.2 Attention Rollout</h3>

  <div class="method-card">
    <h4>Method: Attention Rollout (Abnar & Zuidema, 2020)</h4>
    <p><strong>Idea:</strong> Propagate attention weights through layers, accounting for residual connections.</p>
    <p><strong>Assumption:</strong> Token identities are linearly combined based on attention weights.</p>
    <p><strong>Formula:</strong> Roll out attention from layer \( \ell \) to input:
      \[
      \tilde{A}^{(\ell)} = A^{(\ell)} \cdot \tilde{A}^{(\ell-1)}
      \]
      where \( A^{(\ell)} \) is attention at layer \( \ell \), adjusted for residual connections.
    </p>
    <p><strong>Pros:</strong> Accounts for multi-layer information flow</p>
    <p><strong>Cons:</strong> Linear approximation may be inaccurate for nonlinear models</p>
  </div>

  <h3>4.3 Attention Flow</h3>

  <div class="method-card">
    <h4>Method: Attention Flow (Abnar & Zuidema, 2020)</h4>
    <p><strong>Idea:</strong> Model the attention graph as a <em>flow network</em> and compute maximum flow from each
      input token to the output.</p>
    <p><strong>Algorithm:</strong> Use max-flow algorithms (e.g., Ford-Fulkerson) with attention weights as edge
      capacities.</p>
    <p><strong>Pros:</strong> Captures multi-path information flow</p>
    <p><strong>Cons:</strong> Computationally more expensive than rollout</p>
  </div>

  <div class="comparison-box">
    <h4>Attention Rollout vs Attention Flow</h4>
    <table>
      <tr>
        <th>Aspect</th>
        <th>Attention Rollout</th>
        <th>Attention Flow</th>
      </tr>
      <tr>
        <td><strong>Assumption</strong></td>
        <td>Linear mixing of identities</td>
        <td>Flow network</td>
      </tr>
      <tr>
        <td><strong>Computation</strong></td>
        <td>Matrix multiplication (fast)</td>
        <td>Max-flow algorithm (slower)</td>
      </tr>
      <tr>
        <td><strong>Multi-path</strong></td>
        <td>Averages paths</td>
        <td>Finds bottlenecks</td>
      </tr>
      <tr>
        <td><strong>Use case</strong></td>
        <td>Quick approximate attribution</td>
        <td>Detailed flow analysis</td>
      </tr>
    </table>
    <p><strong>Key finding:</strong> Both methods correlate better with ablation and gradient-based methods than raw
      attention.</p>
  </div>

  <h2>5. The Inseq Library: Practical Tool</h2>

  <p>
    <a href="https://inseq.org/" target="_blank">Inseq</a> is a Python library that makes attribution analysis
    accessible for sequence generation models.
  </p>

  <h3>5.1 What Inseq Provides</h3>

  <ul>
    <li><strong>Unified API</strong> for gradient, perturbation, and attention methods</li>
    <li><strong>Model support:</strong> GPT-2, GPT-NeoX, Llama, BLOOM, T5, BART, Marian MT (via HuggingFace)</li>
    <li><strong>Methods supported:</strong>
      <ul>
        <li>Gradient-based: Saliency, Input×Gradient, Integrated Gradients, DeepLIFT, GradientSHAP, UDIG</li>
        <li>Attention-based: Attention weights, value zeroing</li>
        <li>Perturbation-based: Occlusion, LIME, ReAGent</li>
      </ul>
    </li>
    <li><strong>High-level interface:</strong> <code>model.attribute(input_text, target_text, method="integrated_gradients")</code></li>
    <li><strong>Visualization tools</strong> for displaying attributions</li>
    <li><strong>Step-level attribution</strong> for generation (attribute each output token separately)</li>
  </ul>

  <h3>5.2 Example Usage</h3>

  <pre><code>import inseq

# Load model
model = inseq.load_model("gpt2", "integrated_gradients")

# Run attribution
result = model.attribute(
    "The cat sat on the",
    n_steps=5,  # Generate 5 tokens
    step_scores=["probability"]
)

# Visualize
result.show()
</code></pre>

  <h3>5.3 Key Pedagogical Examples from Inseq</h3>

  <div class="example-box">
    <p><strong>Example 1: Gender Bias in Machine Translation</strong></p>
    <p>
      <em>Task:</em> Translate "The doctor asked the nurse to help" from English to gendered language (e.g., Italian).
    </p>
    <p>
      <em>Question:</em> Does the model assign gendered pronouns based on stereotypes?
    </p>
    <p>
      <em>Attribution reveals:</em> Which input words drive gendered predictions ("doctor" vs "nurse").
    </p>
  </div>

  <div class="example-box">
    <p><strong>Example 2: Factual Knowledge Localization in GPT-2</strong></p>
    <p>
      <em>Input:</em> "The capital of France is"
    </p>
    <p>
      <em>Output:</em> "Paris"
    </p>
    <p>
      <em>Attribution reveals:</em> Model strongly attends to "France" (expected) but may also use "capital" heavily,
      showing it recognizes the prompt structure.
    </p>
  </div>

  <h2>6. Comparing Attribution Methods</h2>

  <p>
    Different methods can give different results. How do we know which to trust?
  </p>

  <h3>6.1 Validation Techniques</h3>

  <div class="info-box">
    <p><strong>1. Perturbation Test (Ground Truth)</strong></p>
    <p>
      Remove tokens with high attribution scores. The output should change more than removing low-attribution tokens.
    </p>
    <p><strong>Metric:</strong> Correlation between attribution scores and output change when tokens are removed.</p>
  </div>

  <div class="info-box">
    <p><strong>2. Sanity Checks</strong></p>
    <ul>
      <li><strong>Random model test:</strong> Attributions should differ between trained and random models</li>
      <li><strong>Random label test:</strong> Attributions should differ for different target labels</li>
      <li><strong>Completeness test:</strong> Sum of attributions should match output difference (for IG)</li>
    </ul>
  </div>

  <div class="info-box">
    <p><strong>3. Cross-Method Agreement</strong></p>
    <p>
      Check if multiple methods (e.g., IG, LIME, ablation) agree on which tokens are important. High agreement
      increases confidence.
    </p>
  </div>

  <h3>6.2 Method Comparison Summary</h3>

  <table>
    <tr>
      <th>Method</th>
      <th>Speed</th>
      <th>Accuracy</th>
      <th>Interpretability</th>
      <th>LLM-Specific Issues</th>
    </tr>
    <tr>
      <td><strong>Saliency</strong></td>
      <td>⚡ Fast</td>
      <td>⚠️ Saturation</td>
      <td>✓ Clear</td>
      <td>Unreliable in deep networks</td>
    </tr>
    <tr>
      <td><strong>Integrated Gradients</strong></td>
      <td>⚠️ Slow</td>
      <td>✓✓ Good</td>
      <td>✓ Clear</td>
      <td>Baseline-dependent, linear path</td>
    </tr>
    <tr>
      <td><strong>UDIG</strong></td>
      <td>⚠️ Slow</td>
      <td>✓✓✓ Best</td>
      <td>✓ Clear</td>
      <td>Designed for discrete inputs</td>
    </tr>
    <tr>
      <td><strong>Ablation</strong></td>
      <td>⚠️⚠️ Very Slow</td>
      <td>✓✓✓ Ground truth</td>
      <td>✓✓ Very clear</td>
      <td>Unnatural inputs (gaps)</td>
    </tr>
    <tr>
      <td><strong>LIME</strong></td>
      <td>⚠️⚠️ Very Slow</td>
      <td>⚠️ Variable</td>
      <td>✓✓ Linear model</td>
      <td>Additivity assumption violated</td>
    </tr>
    <tr>
      <td><strong>SHAP</strong></td>
      <td>⚠️⚠️⚠️ Extremely Slow</td>
      <td>✓ Theoretically sound</td>
      <td>✓✓ Game-theoretic</td>
      <td>Additivity assumption violated</td>
    </tr>
    <tr>
      <td><strong>Attention Rollout</strong></td>
      <td>⚡⚡ Very Fast</td>
      <td>⚠️ Approximate</td>
      <td>⚠️ Hard to interpret</td>
      <td>Linear approximation</td>
    </tr>
    <tr>
      <td><strong>Attention Flow</strong></td>
      <td>⚡ Fast</td>
      <td>✓ Better than rollout</td>
      <td>⚠️ Hard to interpret</td>
      <td>Flow network assumption</td>
    </tr>
  </table>

  <h3>6.3 Recommendations</h3>

  <ul>
    <li><strong>For quick exploration:</strong> Saliency or Attention Rollout</li>
    <li><strong>For reliable attribution:</strong> Integrated Gradients (or UDIG for NLP)</li>
    <li><strong>For validation:</strong> Ablation (ground truth)</li>
    <li><strong>For publication:</strong> Use multiple methods, report agreement</li>
    <li><strong>For transformers:</strong> Prefer gradient-based over LIME/SHAP (additivity issues)</li>
  </ul>

  <h2>7. Technical Challenges and Solutions</h2>

  <h3>7.1 Gradient Saturation</h3>
  <p><strong>Problem:</strong> Gradients near zero despite input importance.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use Integrated Gradients (accumulates over path)</li>
    <li>Try Input×Gradient (scales by input magnitude)</li>
    <li>For circuit discovery: Use EAP-GP (adaptive path avoids saturated regions)</li>
  </ul>

  <h3>7.2 Computational Cost</h3>
  <p><strong>Problem:</strong> IG requires ~50-300 forward passes; SHAP/LIME even more.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use fewer interpolation steps (trade accuracy for speed)</li>
    <li>Attribute only critical tokens (e.g., content words, not punctuation)</li>
    <li>Use faster methods (saliency, attention) for exploration, then validate with IG</li>
    <li>Parallelize across GPUs</li>
  </ul>

  <h3>7.3 Long Sequences</h3>
  <p><strong>Problem:</strong> Attributing 1000+ token inputs is expensive and overwhelming.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Aggregate attributions (e.g., by sentence, by entity)</li>
    <li>Focus on specific output tokens of interest</li>
    <li>Use sliding windows for local context</li>
  </ul>

  <h3>7.4 Discrete vs Continuous Inputs</h3>
  <p><strong>Problem:</strong> Text is discrete, but IG assumes continuous interpolation.</p>
  <p><strong>Solutions:</strong></p>
  <ul>
    <li>Use UDIG (nonlinear path respecting discrete structure)</li>
    <li>Apply IG to embeddings (continuous) but interpret results carefully</li>
    <li>Consider perturbation methods (naturally discrete)</li>
  </ul>

  <h2>8. Applications to Your Research</h2>

  <h3>8.1 Debugging Model Predictions</h3>

  <div class="example-box">
    <p><strong>Scenario:</strong> Your model incorrectly predicts subject-verb agreement.</p>
    <p><em>Input:</em> "The key to the cabinets <strong>are</strong>" (incorrect, should be "is")</p>
    <p><strong>Use attribution to investigate:</strong></p>
    <ol>
      <li>Run IG to see which input tokens influenced "are" prediction</li>
      <li>If attribution is high on "cabinets" (distractor), model is attending to wrong noun</li>
      <li>Guides intervention: Need to strengthen subject tracking (relate to Week 5 circuits)</li>
    </ol>
  </div>

  <h3>8.2 Analyzing Your Concept</h3>

  <p>
    For your project concept, attribution helps answer:
  </p>
  <ul>
    <li><strong>Which input features trigger your concept?</strong> (e.g., for "politeness," which words are most
      important?)</li>
    <li><strong>Are there spurious correlations?</strong> (e.g., does "musical key" attribution rely on irrelevant
      cues?)</li>
    <li><strong>How does context matter?</strong> (e.g., does the concept depend on distant words?)</li>
  </ul>

  <h3>8.3 Guiding Circuit Discovery</h3>

  <p>
    <strong>Workflow combining Week 5 and Week 9:</strong>
  </p>
  <ol>
    <li><strong>Attribution (Week 9):</strong> Identify which input tokens matter for your concept</li>
    <li><strong>Attention analysis:</strong> Which heads attend to those important tokens?</li>
    <li><strong>Path patching (Week 5):</strong> Test if those heads causally compute the concept</li>
    <li><strong>Validation (Week 8):</strong> Use IIA to confirm the circuit</li>
  </ol>

  <h2>9. Integration with Previous Weeks</h2>

  <h3>Connecting Attribution to Other Methods</h3>

  <table>
    <tr>
      <th>Previous Week</th>
      <th>How Attribution Helps</th>
    </tr>
    <tr>
      <td><strong>Week 5: Circuits</strong></td>
      <td>
        Use attribution to identify which input tokens activate your circuit.<br>
        Example: If a circuit computes induction, attribution should show high scores on the repeated token.
      </td>
    </tr>
    <tr>
      <td><strong>Week 6: Probes</strong></td>
      <td>
        Compare probe predictions with attribution. If a probe detects a feature, attribution should show which inputs
        provided that information.<br>
        Example: Probe detects "plural subject" → attribution should highlight the plural noun.
      </td>
    </tr>
    <tr>
      <td><strong>Week 7: SAEs</strong></td>
      <td>
        For an SAE feature representing your concept, use attribution to see which inputs activate it.<br>
        Example: "Politeness feature" activates → attribution reveals which politeness markers (e.g., "please," "thank
        you") trigger it.
      </td>
    </tr>
    <tr>
      <td><strong>Week 8: Causal Validation</strong></td>
      <td>
        Attribution ≠ causation! Validate high-attribution tokens with interventions.<br>
        Example: Attribution says "France" is important → intervene by changing "France" to "Italy" and verify output
        changes to "Rome."
      </td>
    </tr>
  </table>

  <h2>10. Limitations and Skepticism</h2>

  <div class="warning-box">
    <h4>Critical Limitations to Remember</h4>
    <ul>
      <li><strong>Correlation ≠ Causation:</strong> High attribution doesn't prove a token is <em>necessary</em>.
        Always validate with interventions (Week 8).</li>
      <li><strong>Additivity assumptions:</strong> LIME and SHAP assume local additivity, which transformers violate.
        Results may be misleading.</li>
      <li><strong>Baseline dependence:</strong> IG results change with baseline choice. Report your baseline and justify
        it.</li>
      <li><strong>Attention ≠ Explanation:</strong> Raw attention weights are insufficient for full attributions due to
        information mixing.</li>
      <li><strong>Out-of-distribution perturbations:</strong> Removing tokens creates unnatural inputs. Model behavior
        on these may not reflect normal operation.</li>
      <li><strong>Single-path assumptions:</strong> Standard IG uses a straight line, which may not capture the true
        path through representation space.</li>
    </ul>
  </div>

  <h3>When Attribution Fails</h3>

  <ul>
    <li><strong>Highly distributed concepts:</strong> If your concept depends on complex interactions of many tokens,
      attribution may not isolate individual contributions clearly.</li>
    <li><strong>Nonlinear interactions:</strong> Token A + Token B together matter, but individually they don't. Linear
      attribution methods will underestimate their importance.</li>
    <li><strong>Internal reasoning:</strong> If the model performs multi-step reasoning, input attribution alone won't
      reveal the internal steps (need circuit analysis).</li>
  </ul>

  <h2>11. Best Practices for Your Research</h2>

  <div class="info-box">
    <h3>Checklist for Using Attribution in Your Project</h3>
    <ul>
      <li>✓ <strong>Use multiple methods</strong> and report agreement</li>
      <li>✓ <strong>Validate with ablation</strong> (ground truth test)</li>
      <li>✓ <strong>Run sanity checks</strong> (random model, random label)</li>
      <li>✓ <strong>Justify baseline choice</strong> (for IG)</li>
      <li>✓ <strong>Report computational costs</strong></li>
      <li>✓ <strong>Visualize attributions</strong> clearly</li>
      <li>✓ <strong>Combine with causal validation</strong> (Week 8 IIA)</li>
      <li>✓ <strong>Aggregate for long sequences</strong></li>
      <li>✓ <strong>Don't over-interpret</strong> (attribution shows correlation, not causation)</li>
      <li>✓ <strong>Compare to human intuition</strong> (does attribution make sense?)</li>
    </ul>
  </div>


  <h2>Part 2: Skepticism and Attribution Validation (50% of content)</h2>

  <div class="warning-box">
    <h3>Critical Reality Check</h3>
    <p>
      Attribution methods produce compelling visualizations. Saliency maps highlight features. Attention weights show where
      the model "looks." But do these explanations actually reflect how the model makes decisions? This section examines
      evidence that attribution methods can be misleading—and how to validate them rigorously.
    </p>
  </div>

  <h2>1. Sanity Checks for Saliency Maps (Adebayo et al., 2018)</h2>

  <h3>The Problem: Visual Plausibility ≠ Correctness</h3>
  <p>
    Saliency maps often highlight seemingly meaningful regions—faces in images, important words in text. But are these
    explanations faithful to the model's computation?
  </p>

  <p>
    <strong>Adebayo et al. (2018)</strong> proposed simple <em>sanity checks</em> to test whether attribution methods are
    actually sensitive to the model and data they're supposed to explain.
  </p>

  <h3>Sanity Check 1: Model Parameter Randomization</h3>
  <div class="info-box">
    <p><strong>Test:</strong> Randomly re-initialize the model's weights.</p>
    <p><strong>Expected behavior:</strong> A good explanation method should produce <em>completely different</em>
      explanations for a trained model vs. a random model.</p>
    <p><strong>Why it matters:</strong> If explanations look the same for both, the method is just detecting network
      architecture or input patterns, not what the model learned.</p>
  </div>

  <div class="example-box">
    <p><strong>Example Application to LLMs:</strong></p>
    <pre><code># Test your attribution method
trained_model = GPT2LMHeadModel.from_pretrained("gpt2")
random_model = GPT2LMHeadModel(config)  # Random initialization

attribution_trained = compute_attribution(trained_model, text)
attribution_random = compute_attribution(random_model, text)

# They should be substantially different!
assert not np.allclose(attribution_trained, attribution_random)
</code></pre>
  </div>

  <h3>Sanity Check 2: Data Randomization</h3>
  <div class="info-box">
    <p><strong>Test:</strong> Train the model on data with random labels.</p>
    <p><strong>Expected behavior:</strong> Explanations should look different for a model trained on meaningful data vs.
      random noise.</p>
    <p><strong>Why it matters:</strong> A model that memorized noise has fundamentally different internal mechanisms than
      one that learned patterns.</p>
  </div>

  <h3>Key Findings from Adebayo et al.</h3>
  <ul>
    <li><strong>Guided Backpropagation FAILS:</strong> Produces similar-looking visualizations for trained and random
      models</li>
    <li><strong>Guided GradCAM FAILS:</strong> Primarily does edge detection, regardless of what the model learned</li>
    <li><strong>Simple Gradients PASS:</strong> Consistently distinguish trained from random models</li>
    <li><strong>Integrated Gradients PASS:</strong> Changes appropriately with model changes</li>
  </ul>

  <div class="warning-box">
    <p><strong>Implication for Your Project:</strong></p>
    <p>
      Before trusting any attribution visualization, run these sanity checks. If your method fails, you're seeing artifacts
      of the attribution algorithm, not insights into your model.
    </p>
  </div>

  <h2>2. Attention Is Not (Necessarily) Explanation (Jain & Wallace, 2019)</h2>

  <h3>The Intuition vs. The Evidence</h3>
  <p>
    Attention mechanisms are everywhere in modern NLP. It's tempting to interpret attention weights as showing which input
    tokens are "important" for predictions. But <strong>Jain & Wallace (2019)</strong> systematically tested this
    assumption.
  </p>

  <h3>Three Tests of Attention as Explanation</h3>

  <h4>Test 1: Correlation with Feature Importance</h4>
  <p>
    Compare attention weights with gradient-based measures of actual feature importance.
  </p>
  <p><strong>Result:</strong> Often <em>uncorrelated</em>. High attention ≠ high importance for prediction.</p>

  <h4>Test 2: Counterfactual Attention Distributions</h4>
  <p>
    Can you construct different attention patterns that produce identical outputs?
  </p>
  <p><strong>Result:</strong> Yes! Multiple contradictory attention distributions often yield the same prediction.</p>
  <p><strong>Implication:</strong> If multiple explanations are equally valid, which one is "true"?</p>

  <h4>Test 3: Adversarial Attention</h4>
  <p>
    Optimize attention weights to be maximally different while keeping outputs unchanged.
  </p>
  <p><strong>Result:</strong> Possible for many tasks. Attention can be manipulated without affecting predictions.</p>

  <h3>When Can You Trust Attention?</h3>
  <table>
    <tr>
      <th>Use Case</th>
      <th>Trustworthy?</th>
      <th>Reason</th>
    </tr>
    <tr>
      <td>Single-layer analysis</td>
      <td>✓ Moderate</td>
      <td>Shows what that layer attends to (but not why)</td>
    </tr>
    <tr>
      <td>Debugging attention patterns</td>
      <td>✓ Yes</td>
      <td>Useful for finding bugs (attending to padding, etc.)</td>
    </tr>
    <tr>
      <td>Input importance for output</td>
      <td>✗ No</td>
      <td>Use attribution methods instead</td>
    </tr>
    <tr>
      <td>Explanation for end users</td>
      <td>✗ Risky</td>
      <td>May be unfaithful; validate first</td>
    </tr>
  </table>

  <h3>Better Alternatives</h3>
  <ul>
    <li><strong>Attention Rollout:</strong> Propagate attention through layers (see Week 6 Part 1)</li>
    <li><strong>Attention Flow:</strong> Model as flow network</li>
    <li><strong>Integrated Gradients:</strong> More faithful to actual importance</li>
    <li><strong>Causal Interventions:</strong> Test by ablating attended tokens</li>
  </ul>

  <h2>3. The ROAR Benchmark: Quantifying Attribution Quality (Hooker et al., 2019)</h2>

  <h3>The Challenge</h3>
  <p>
    We have many attribution methods (gradients, IG, SHAP, LIME, attention). How do we know which ones actually identify
    important features? Visual inspection is subjective and prone to confirmation bias.
  </p>

  <h3>ROAR: RemOve And Retrain</h3>
  <p>
    <strong>Hooker et al. (2019)</strong> proposed a quantitative benchmark:
  </p>

  <div class="method-card">
    <h4>The ROAR Protocol</h4>
    <ol>
      <li>Train a model to convergence</li>
      <li>Use an attribution method to identify most important features</li>
      <li><strong>Remove those features</strong> from the training data</li>
      <li><strong>Retrain</strong> the model from scratch without those features</li>
      <li>Measure the drop in performance</li>
    </ol>

    <p><strong>Logic:</strong> If the method truly identifies important features, removing them should hurt performance
      more than removing random features.</p>
  </div>

  <h3>Shocking Results</h3>
  <div class="warning-box">
    <ul>
      <li>Many popular methods perform <strong>no better than random</strong> feature selection</li>
      <li>Even removing 90% of pixels (chosen by importance) still allows ~64% accuracy on ImageNet</li>
      <li>Only ensemble methods (VarGrad, SmoothGrad-Squared) consistently beat random</li>
    </ul>

    <p><strong>Conclusion:</strong> Just because an attribution method produces a plausible-looking heatmap doesn't mean
      it's identifying features that actually matter.</p>
  </div>

  <h3>Adapting ROAR for LLMs</h3>
  <p>
    For text models, adapt ROAR as follows:
  </p>

  <div class="example-box">
    <pre><code># ROAR for text attribution
1. Compute attribution scores for your dataset
2. Identify top-k most important tokens per example
3. Mask or remove those tokens from training data
4. Fine-tune (or retrain) model without important tokens
5. Measure performance drop

# Compare
drop_with_important_removed = baseline_acc - roar_acc
drop_with_random_removed = baseline_acc - random_acc

# Good attribution method:
assert drop_with_important_removed > drop_with_random_removed
</code></pre>
  </div>

  <h3>Cheaper Alternative: Perturbation Test</h3>
  <p>
    Full ROAR requires retraining (expensive). A faster proxy:
  </p>
  <ul>
    <li>Remove high-attribution tokens from test inputs</li>
    <li>Measure output change</li>
    <li>Compare to removing random tokens</li>
    <li>Not as rigorous as ROAR, but much faster</li>
  </ul>

  <h2>4. Validation Checklist for Attribution Methods</h2>

  <p>
    Before trusting attribution results, verify:
  </p>

  <div class="info-box">
    <h3>Mandatory Checks</h3>
    <ol>
      <li><strong>✓ Sanity checks (Adebayo):</strong>
        <ul>
          <li>Random model test: attributions differ from trained model</li>
          <li>Random label test: attributions differ from properly trained model</li>
        </ul>
      </li>

      <li><strong>✓ Perturbation validation:</strong>
        <ul>
          <li>Removing high-attribution features hurts performance</li>
          <li>More than removing random features</li>
          <li>Effect size is substantial</li>
        </ul>
      </li>

      <li><strong>✓ Method agreement:</strong>
        <ul>
          <li>Compare at least 2-3 independent attribution methods</li>
          <li>If they disagree, investigate why</li>
          <li>Don't cherry-pick the method that gives desired results</li>
        </ul>
      </li>

      <li><strong>✓ Baseline comparison:</strong>
        <ul>
          <li>Beats random feature selection</li>
          <li>Beats frequency-based heuristics</li>
          <li>Compare simple (gradients) vs complex (IG, SHAP) methods</li>
        </ul>
      </li>

      <li><strong>✓ Completeness test (for IG):</strong>
        <ul>
          <li>Sum of attributions ≈ f(x) - f(baseline)</li>
          <li>If not, something is wrong with implementation or baseline</li>
        </ul>
      </li>
    </ol>
  </div>

  <h2>5. Common Pitfalls and How to Avoid Them</h2>

  <h3>Pitfall 1: Baseline Dependence (Integrated Gradients)</h3>
  <p><strong>Problem:</strong> IG results can change dramatically with baseline choice.</p>
  <p><strong>Solution:</strong></p>
  <ul>
    <li>Test multiple baselines (zero embeddings, padding, average embeddings)</li>
    <li>Report which baseline you used and why</li>
    <li>If results flip with baseline change, interpretation is fragile</li>
  </ul>

  <h3>Pitfall 2: Additivity Assumptions (LIME, SHAP)</h3>
  <p><strong>Problem:</strong> Transformers violate local additivity due to attention and LayerNorm.</p>
  <p><strong>Solution:</strong></p>
  <ul>
    <li>Use LIME/SHAP cautiously for transformers</li>
    <li>Prefer gradient-based methods</li>
    <li>Validate LIME/SHAP results with ablation</li>
  </ul>

  <h3>Pitfall 3: Out-of-Distribution Perturbations</h3>
  <p><strong>Problem:</strong> Removing tokens creates unnatural inputs. Model behavior may not reflect normal operation.
  </p>
  <p><strong>Solution:</strong></p>
  <ul>
    <li>Use ReAGent (replace with alternatives) instead of masking</li>
    <li>Consider in-distribution perturbations</li>
    <li>Report that perturbations are artificial</li>
  </ul>

  <h3>Pitfall 4: Confirmation Bias</h3>
  <p><strong>Problem:</strong> Seeing what you expect in attribution maps.</p>
  <p><strong>Solution:</strong></p>
  <ul>
    <li>Pre-register predictions before running attribution</li>
    <li>Show examples where attribution gives unexpected results</li>
    <li>Have collaborators blind-evaluate attributions</li>
  </ul>

  <h2>6. Best Practices for Rigorous Attribution Research</h2>

  <div class="comparison-box">
    <h3>Do's</h3>
    <ul>
      <li>✓ Run sanity checks before trusting any method</li>
      <li>✓ Use multiple independent attribution methods</li>
      <li>✓ Validate with perturbation or ROAR tests</li>
      <li>✓ Report baseline choice and justify it</li>
      <li>✓ Show when attribution fails or gives unexpected results</li>
      <li>✓ Combine attribution with causal intervention (Week 4)</li>
      <li>✓ Report computational cost</li>
    </ul>

    <h3>Don'ts</h3>
    <ul>
      <li>✗ Trust visualizations without validation</li>
      <li>✗ Use only one attribution method</li>
      <li>✗ Rely solely on attention weights</li>
      <li>✗ Cherry-pick examples that look good</li>
      <li>✗ Ignore baseline dependence</li>
      <li>✗ Skip sanity checks to save time</li>
      <li>✗ Over-interpret correlation as causation</li>
    </ul>
  </div>

  <h2>7. Integration with Your Research Project</h2>

  <h3>Validating Your Concept Attribution</h3>
  <p>
    When attributing input importance for your concept:
  </p>

  <div class="assignment-box">
    <h4>Week 6 Validation Protocol</h4>
    <ol>
      <li><strong>Choose methods:</strong> Pick 2-3 attribution methods (e.g., IG, gradients, ablation)</li>

      <li><strong>Run sanity checks:</strong>
        <ul>
          <li>Test on random model</li>
          <li>Verify completeness (for IG)</li>
        </ul>
      </li>

      <li><strong>Compute attributions:</strong> For your concept-relevant predictions</li>

      <li><strong>Validate:</strong>
        <ul>
          <li>Do methods agree on top features?</li>
          <li>Perturbation test: removing features changes output</li>
          <li>Compare to random baseline</li>
        </ul>
      </li>

      <li><strong>Causal integration:</strong>
        <ul>
          <li>Combine with Week 4 patching results</li>
          <li>Do high-attribution tokens align with causally important components?</li>
        </ul>
      </li>

      <li><strong>Report honestly:</strong>
        <ul>
          <li>Show agreement and disagreement between methods</li>
          <li>Document failure cases</li>
          <li>Acknowledge limitations</li>
        </ul>
      </li>
    </ol>
  </div>

  <h2>8. Looking Ahead: More Skepticism in Week 10</h2>

  <p>
    This week introduced validation for attribution methods. But interpretability illusions go deeper:
  </p>

  <ul>
    <li><strong>Week 7 (SAEs):</strong> Can sparse features be adversarially fragile?</li>
    <li><strong>Week 8 (Circuits):</strong> How sensitive are circuit findings to methodological choices?</li>
    <li><strong>Week 10 (Full Skepticism):</strong> Comprehensive study of when interpretability methods fail</li>
  </ul>

  <p>
    The validation framework from Week 4, combined with attribution skepticism from this week, prepares you to critically
    evaluate any interpretability claim.
  </p>

  <h2>Summary: Attribution + Skepticism</h2>

  <h3>Part 1 Takeaways (Attribution Methods)</h3>
  <ul>
    <li>Input attribution identifies which inputs matter for outputs</li>
    <li>Gradient-based methods (IG, UDIG) are reliable when validated</li>
    <li>Perturbation methods (ablation) provide ground truth</li>
    <li>Attention-based methods are fast but approximate</li>
    <li>Inseq library makes attribution accessible</li>
  </ul>

  <h3>Part 2 Takeaways (Skepticism)</h3>
  <ul>
    <li>Visual plausibility ≠ correctness (Adebayo sanity checks)</li>
    <li>Attention weights are not reliable explanations (Jain & Wallace)</li>
    <li>Many methods perform no better than random (ROAR benchmark)</li>
    <li>Always validate with multiple independent tests</li>
    <li>Combine attribution with causal validation</li>
  </ul>

  <h3>For Your Project</h3>
  <ul>
    <li>Use attribution to identify important inputs for your concept</li>
    <li>Run full validation protocol (sanity checks + perturbation + agreement)</li>
    <li>Integrate with Week 4 causal intervention results</li>
    <li>Report failures and limitations honestly</li>
    <li>Build toward rigorous, validated interpretability</li>
  </ul>

  <h2>References for Part 2 (Skepticism)</h2>

  <h3>Core Papers</h3>
  <ul>
    <li><strong>Adebayo et al. (2018):</strong> "Sanity Checks for Saliency Maps." NeurIPS. <a
        href="https://arxiv.org/abs/1810.03292" target="_blank">arXiv:1810.03292</a></li>
    <li><strong>Jain & Wallace (2019):</strong> "Attention is not Explanation." NAACL. <a
        href="https://arxiv.org/abs/1902.10186" target="_blank">arXiv:1902.10186</a></li>
    <li><strong>Hooker et al. (2019):</strong> "A Benchmark for Interpretability Methods in Deep Neural Networks."
      NeurIPS. <a href="https://arxiv.org/abs/1806.10758" target="_blank">arXiv:1806.10758</a></li>
  </ul>

  <h3>Nuance and Follow-ups</h3>
  <ul>
    <li>Wiegreffe & Pinter (2019). "Attention is not not Explanation." EMNLP. <a href="https://arxiv.org/abs/1908.04626"
        target="_blank">arXiv:1908.04626</a> [Defense of attention under certain conditions]</li>
    <li>Jacovi & Goldberg (2020). "Towards Faithfully Interpretable NLP Systems." ACL. [Framework for faithfulness]</li>
  </ul>
  <h2>12. Summary and Next Steps</h2>

  <h3>Key Takeaways</h3>

  <ul>
    <li><strong>Input attribution</strong> identifies which inputs matter for outputs—complementary to circuit/probe/SAE
      analysis</li>
    <li><strong>Gradient-based methods</strong> (IG, UDIG) are reliable for LLMs when used carefully</li>
    <li><strong>Perturbation-based methods</strong> (ablation) provide ground truth but are computationally expensive
    </li>
    <li><strong>Attention-based methods</strong> (rollout, flow) are fast but approximate</li>
    <li><strong>Inseq library</strong> makes attribution accessible for sequence generation models</li>
    <li><strong>Always validate</strong> attributions with multiple methods and causal interventions</li>
    <li><strong>Critical limitations:</strong> Baseline dependence, additivity assumptions, correlation ≠ causation</li>
  </ul>

  <h3>For Your Research Project</h3>

  <ol>
    <li>Use attribution to identify <strong>which input features activate your concept</strong></li>
    <li>Combine with <strong>circuit analysis</strong> (Week 5) to understand <em>how</em> those inputs are processed
    </li>
    <li>Validate findings with <strong>causal interventions</strong> (Week 8)</li>
    <li>Report attribution results in your paper with appropriate caveats</li>
  </ol>

  <h3>Looking Ahead</h3>

  <p>
    <strong>Week 10:</strong> Skepticism and interpretability illusions—when interpretability methods mislead us, and
    how to be rigorous.
  </p>

  <h2>References & Resources</h2>

  <h3>Core Papers</h3>
  <ul>
    <li><strong>Inseq Library:</strong> Sarti et al. (2023). "Inseq: An Interpretability Toolkit for Sequence
      Generation Models." <a href="https://arxiv.org/abs/2302.13942" target="_blank">arXiv:2302.13942</a></li>
    <li><strong>Integrated Gradients:</strong> Sundararajan et al. (2017). "Axiomatic Attribution for Deep Networks."
      ICML. <a href="https://arxiv.org/abs/1703.01365" target="_blank">arXiv:1703.01365</a></li>
    <li><strong>UDIG:</strong> Recent (2024). "Uniform Discretized Integrated Gradients." <a
        href="https://arxiv.org/abs/2412.03886" target="_blank">arXiv:2412.03886</a></li>
    <li><strong>Attention Flow:</strong> Abnar & Zuidema (2020). "Quantifying Attention Flow in Transformers." ACL. <a
        href="https://aclanthology.org/2020.acl-main.385/" target="_blank">ACL Anthology</a></li>
    <li><strong>LIME:</strong> Ribeiro et al. (2016). "Why Should I Trust You?" KDD. <a
        href="https://arxiv.org/abs/1602.04938" target="_blank">arXiv:1602.04938</a></li>
    <li><strong>SHAP:</strong> Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions." NIPS. <a
        href="https://arxiv.org/abs/1705.07874" target="_blank">arXiv:1705.07874</a></li>
    <li><strong>Baseline Selection:</strong> Sturmfels et al. (2020). "Visualizing the Impact of Feature Attribution
      Baselines." Distill. <a href="https://distill.pub/2020/attribution-baselines/"
        target="_blank">distill.pub</a></li>
  </ul>

  <h3>Tools & Libraries</h3>
  <ul>
    <li><strong>Inseq:</strong> <a href="https://inseq.org/" target="_blank">inseq.org</a> | <a
        href="https://github.com/inseq-team/inseq" target="_blank">GitHub</a></li>
    <li><strong>Captum (PyTorch):</strong> <a href="https://captum.ai/" target="_blank">captum.ai</a> | <a
        href="https://captum.ai/tutorials/Llama2_LLM_Attribution" target="_blank">LLM Tutorial</a></li>
    <li><strong>Transformer Interpret:</strong> <a href="https://github.com/cdpierse/transformers-interpret"
        target="_blank">GitHub</a></li>
  </ul>

  <h3>Supplementary Reading</h3>
  <ul>
    <li>Attention is not Explanation: Jain & Wallace (2019). <a href="https://arxiv.org/abs/1902.10186"
        target="_blank">arXiv:1902.10186</a></li>
    <li>Attention is not not Explanation: Wiegreffe & Pinter (2019). <a href="https://arxiv.org/abs/1908.04626"
        target="_blank">arXiv:1908.04626</a></li>
    <li>Sanity Checks for Saliency Maps: Adebayo et al. (2018). NIPS. <a href="https://arxiv.org/abs/1810.03292"
        target="_blank">arXiv:1810.03292</a></li>
    <li>Transformers Can't Represent Additive Models: Recent research on LIME/SHAP limitations for transformers (2024).
    </li>
  </ul>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 6</strong></p>
      <p>
        Apply attribution methods to identify which input tokens matter most for your concept.
        Use sanity checks to validate that your attribution methods are meaningful and not artifacts.
      </p>

      <h4>Attribution Analysis</h4>
      <ul>
        <li><strong>Apply multiple attribution methods:</strong>
          <ul>
            <li>Gradient-based: Input gradients, integrated gradients, or gradient × input</li>
            <li>Perturbation-based: Ablation or occlusion</li>
            <li>Attention-based: Attention rollout or attention flow (with caution)</li>
          </ul>
        </li>
        <li><strong>Identify important tokens:</strong>
          <ul>
            <li>Which input tokens have highest attribution scores for your concept?</li>
            <li>Do different methods agree on important tokens?</li>
            <li>Visualize attributions across multiple examples</li>
          </ul>
        </li>
        <li><strong>Run sanity checks:</strong>
          <ul>
            <li>Random model baseline: do attributions disappear with random weights?</li>
            <li>Random input baseline: do attributions change appropriately with random inputs?</li>
            <li>Cascading randomization: test layers systematically</li>
          </ul>
        </li>
        <li><strong>Validate with interventions:</strong>
          <ul>
            <li>Ablate high-attribution tokens: does model behavior change as predicted?</li>
            <li>Ablate low-attribution tokens: behavior should remain stable</li>
            <li>Compare attribution rankings with intervention effect sizes</li>
          </ul>
        </li>
      </ul>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Attribution visualizations:</strong>
          <ul>
            <li>Heatmaps showing token attributions for 10-15 examples</li>
            <li>Comparison across different attribution methods</li>
            <li>Summary of which tokens are consistently important</li>
          </ul>
        </li>
        <li><strong>Sanity check results:</strong>
          <ul>
            <li>Results from all sanity check tests</li>
            <li>Assessment: do your attributions pass sanity checks?</li>
          </ul>
        </li>
        <li><strong>Validation results:</strong>
          <ul>
            <li>Correlation between attribution scores and ablation effects</li>
            <li>Examples where attribution predictions match/mismatch interventions</li>
          </ul>
        </li>
        <li><strong>Interpretation:</strong>
          <ul>
            <li>Which tokens matter most for your concept?</li>
            <li>Are attributions trustworthy for your concept?</li>
            <li>What does this reveal about how models process your concept?</li>
          </ul>
        </li>
        <li><strong>Code:</strong> Notebook with attribution methods, sanity checks, and validation</li>
      </ul>

      <p><em>
        Attribution methods can be unreliable—always validate with sanity checks and interventions before
        drawing conclusions about which inputs matter.
      </em></p>
    </div>
  </section>

</body>

</html>
