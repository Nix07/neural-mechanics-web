{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Exercise: Benchmarking Language Models\n",
    "\n",
    "In this exercise, you'll gain hands-on experience with the fundamental concepts of benchmarking LLMs:\n",
    "- Loading and using a language model\n",
    "- Examining token probabilities\n",
    "- Testing different sampling strategies\n",
    "- Implementing three prompting approaches\n",
    "- Computing evaluation metrics\n",
    "- Measuring statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch numpy scipy sklearn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading a Language Model\n",
    "\n",
    "We'll use GPT-2, a small but capable autoregressive language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small (124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Token Probabilities\n",
    "\n",
    "Let's examine what the model actually computes: a probability distribution over the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token_probabilities(text):\n",
    "    \"\"\"Get probability distribution over next token given input text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Get logits for the last token position\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        # Convert to probabilities\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "    \n",
    "    return probs.cpu()\n",
    "\n",
    "# Example: What comes after \"The capital of France is\"?\n",
    "prompt = \"The capital of France is\"\n",
    "probs = get_next_token_probabilities(prompt)\n",
    "\n",
    "# Show top 10 most likely tokens\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nTop {top_k} most likely next tokens:\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  '{token}' → {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Cloze Evaluation\n",
    "\n",
    "Use token probabilities to evaluate factual knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probability(prompt, target_token):\n",
    "    \"\"\"Get the probability of a specific token following the prompt.\"\"\"\n",
    "    probs = get_next_token_probabilities(prompt)\n",
    "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    return probs[target_id].item()\n",
    "\n",
    "# Test factual knowledge\n",
    "test_cases = [\n",
    "    (\"The capital of France is\", \" Paris\", \" London\"),\n",
    "    (\"The largest planet in our solar system is\", \" Jupiter\", \" Mars\"),\n",
    "    (\"Water freezes at\", \" 0\", \" 100\"),\n",
    "]\n",
    "\n",
    "print(\"Cloze Evaluation Results:\")\n",
    "for prompt, correct, incorrect in test_cases:\n",
    "    prob_correct = get_token_probability(prompt, correct)\n",
    "    prob_incorrect = get_token_probability(prompt, incorrect)\n",
    "    \n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"  P('{correct}') = {prob_correct:.4f}\")\n",
    "    print(f\"  P('{incorrect}') = {prob_incorrect:.4f}\")\n",
    "    print(f\"  Correct answer favored: {prob_correct > prob_incorrect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:** Add 3 more test cases for a concept relevant to your project. Test whether the model assigns higher probability to correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your test cases here\n",
    "my_test_cases = [\n",
    "    # (\"prompt\", \" correct_answer\", \" incorrect_answer\"),\n",
    "]\n",
    "\n",
    "# Test your cases\n",
    "# for prompt, correct, incorrect in my_test_cases:\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Autoregressive Sampling Strategies\n",
    "\n",
    "Different sampling methods produce different text characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, strategy=\"greedy\", max_length=50, **kwargs):\n",
    "    \"\"\"Generate text using different sampling strategies.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    if strategy == \"greedy\":\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elif strategy == \"temperature\":\n",
    "        temp = kwargs.get(\"temperature\", 1.0)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elif strategy == \"top_k\":\n",
    "        k = kwargs.get(\"k\", 50)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=k,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elif strategy == \"top_p\":\n",
    "        p = kwargs.get(\"p\", 0.9)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=p,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different strategies\n",
    "prompt = \"Once upon a time, in a distant galaxy,\"\n",
    "\n",
    "print(\"Comparing Sampling Strategies:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"GREEDY:\\n{generate_text(prompt, 'greedy')}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"TEMPERATURE (0.7):\\n{generate_text(prompt, 'temperature', temperature=0.7)}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP-K (k=50):\\n{generate_text(prompt, 'top_k', k=50)}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP-P (p=0.9):\\n{generate_text(prompt, 'top_p', p=0.9)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Sampling Reproducibility\n",
    "\n",
    "For benchmarking, we need reproducible results. Test which strategies are deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run each sampling strategy multiple times\n",
    "# Which ones produce identical outputs? Why?\n",
    "\n",
    "test_prompt = \"The meaning of life is\"\n",
    "\n",
    "# Test greedy\n",
    "print(\"Greedy (run 1):\", generate_text(test_prompt, \"greedy\", max_length=20))\n",
    "print(\"Greedy (run 2):\", generate_text(test_prompt, \"greedy\", max_length=20))\n",
    "\n",
    "# TODO: Test temperature sampling with same temperature\n",
    "# TODO: Why do results differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Three Prompting Strategies\n",
    "\n",
    "Test instruction-following, cloze, and in-context learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Instruction-Following\n",
    "\n",
    "Note: GPT-2 is a base model, not instruction-tuned, so this may not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"Translate to French: Hello, how are you?\\nTranslation:\"\n",
    "result = generate_text(instruction_prompt, \"greedy\", max_length=30)\n",
    "print(f\"Instruction prompt:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Cloze Prompts\n",
    "\n",
    "Already covered in Part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: In-Context Learning\n",
    "\n",
    "Provide examples to teach the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_prompt = \"\"\"Classify the sentiment of each review:\n",
    "\n",
    "Review: This movie was amazing! I loved every minute.\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: Terrible waste of time. Very disappointing.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: It was okay, nothing special.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Review: Absolutely brilliant performances and story!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "result = generate_text(icl_prompt, \"greedy\", max_length=len(tokenizer.encode(icl_prompt)) + 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Design ICL for Your Concept\n",
    "\n",
    "Create a few-shot prompt for a concept relevant to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design a few-shot prompt for your concept\n",
    "my_icl_prompt = \"\"\"\n",
    "# Your examples here\n",
    "\"\"\"\n",
    "\n",
    "# Test it\n",
    "# result = generate_text(my_icl_prompt, \"greedy\", max_length=...)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation Metrics\n",
    "\n",
    "Compute precision, recall, F1, and perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Binary classification task\n",
    "# 1 = positive, 0 = negative\n",
    "\n",
    "true_labels = np.array([1, 1, 0, 1, 0, 0, 1, 1, 0, 1])\n",
    "predicted_labels = np.array([1, 1, 0, 0, 0, 1, 1, 1, 0, 1])\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "accuracy = (true_labels == predicted_labels).mean()\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"  Precision: {precision:.3f}\")\n",
    "print(f\"  Recall:    {recall:.3f}\")\n",
    "print(f\"  F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix breakdown\n",
    "tp = ((true_labels == 1) & (predicted_labels == 1)).sum()\n",
    "fp = ((true_labels == 0) & (predicted_labels == 1)).sum()\n",
    "fn = ((true_labels == 1) & (predicted_labels == 0)).sum()\n",
    "tn = ((true_labels == 0) & (predicted_labels == 0)).sum()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives:  {tp}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Negatives:  {tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Perplexity\n",
    "\n",
    "Measures how \"surprised\" the model is by a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text):\n",
    "    \"\"\"Calculate perplexity of text under the model.\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "        # Negative log likelihood\n",
    "        nll = outputs.loss.item()\n",
    "    \n",
    "    # Perplexity = exp(average negative log likelihood)\n",
    "    perplexity = np.exp(nll)\n",
    "    return perplexity\n",
    "\n",
    "# Test on different texts\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Colorless green ideas sleep furiously.\",  # Grammatical but nonsensical\n",
    "    \"asdf qwer zxcv hjkl\",  # Random characters\n",
    "]\n",
    "\n",
    "print(\"Perplexity Comparison:\")\n",
    "for text in texts:\n",
    "    ppl = calculate_perplexity(text)\n",
    "    print(f\"  PPL = {ppl:8.2f} | '{text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Compare Perplexity\n",
    "\n",
    "Calculate perplexity for text from your domain vs. out-of-domain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add examples from your concept domain\n",
    "in_domain_text = \"...\"\n",
    "out_domain_text = \"...\"\n",
    "\n",
    "# Calculate and compare perplexities\n",
    "# What does this tell you about the model's knowledge of your domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Statistical Significance\n",
    "\n",
    "Determine if differences in performance are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(accuracies):\n",
    "    \"\"\"Calculate 95% confidence interval for accuracy.\"\"\"\n",
    "    mean = np.mean(accuracies)\n",
    "    se = stats.sem(accuracies)  # Standard error\n",
    "    ci = stats.t.interval(0.95, len(accuracies)-1, loc=mean, scale=se)\n",
    "    return mean, ci\n",
    "\n",
    "def bootstrap_ci(scores, n_bootstrap=1000):\n",
    "    \"\"\"Bootstrap confidence interval.\"\"\"\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(scores, size=len(scores), replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "    return np.mean(scores), (ci_lower, ci_upper)\n",
    "\n",
    "# Example: Two models on a benchmark\n",
    "# Each entry is 1 (correct) or 0 (incorrect)\n",
    "model_a_results = np.random.binomial(1, 0.85, 100)  # 85% accuracy\n",
    "model_b_results = np.random.binomial(1, 0.82, 100)  # 82% accuracy\n",
    "\n",
    "# Bootstrap CIs\n",
    "mean_a, ci_a = bootstrap_ci(model_a_results)\n",
    "mean_b, ci_b = bootstrap_ci(model_b_results)\n",
    "\n",
    "print(\"Model A:\")\n",
    "print(f\"  Accuracy: {mean_a:.3f}\")\n",
    "print(f\"  95% CI: [{ci_a[0]:.3f}, {ci_a[1]:.3f}]\")\n",
    "print(f\"\\nModel B:\")\n",
    "print(f\"  Accuracy: {mean_b:.3f}\")\n",
    "print(f\"  95% CI: [{ci_b[0]:.3f}, {ci_b[1]:.3f}]\")\n",
    "\n",
    "# Do confidence intervals overlap?\n",
    "overlap = not (ci_a[1] < ci_b[0] or ci_b[1] < ci_a[0])\n",
    "print(f\"\\nConfidence intervals overlap: {overlap}\")\n",
    "if overlap:\n",
    "    print(\"⚠️  Difference may not be statistically significant\")\n",
    "else:\n",
    "    print(\"✓ Difference appears statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Statistical Testing\n",
    "\n",
    "Use McNemar's test to compare two models on paired examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Create contingency table\n",
    "# model_a_correct, model_b_correct (both arrays of 1s and 0s)\n",
    "model_a_correct = np.random.binomial(1, 0.85, 100)\n",
    "model_b_correct = np.random.binomial(1, 0.82, 100)\n",
    "\n",
    "# Build 2x2 table\n",
    "both_correct = ((model_a_correct == 1) & (model_b_correct == 1)).sum()\n",
    "a_correct_b_wrong = ((model_a_correct == 1) & (model_b_correct == 0)).sum()\n",
    "a_wrong_b_correct = ((model_a_correct == 0) & (model_b_correct == 1)).sum()\n",
    "both_wrong = ((model_a_correct == 0) & (model_b_correct == 0)).sum()\n",
    "\n",
    "table = [[both_correct, a_correct_b_wrong],\n",
    "         [a_wrong_b_correct, both_wrong]]\n",
    "\n",
    "print(\"Contingency Table:\")\n",
    "print(f\"  Both correct: {both_correct}\")\n",
    "print(f\"  A correct, B wrong: {a_correct_b_wrong}\")\n",
    "print(f\"  A wrong, B correct: {a_wrong_b_correct}\")\n",
    "print(f\"  Both wrong: {both_wrong}\")\n",
    "\n",
    "# McNemar's test\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f\"\\nMcNemar's test p-value: {result.pvalue:.4f}\")\n",
    "\n",
    "if result.pvalue < 0.05:\n",
    "    print(\"✓ Statistically significant difference (p < 0.05)\")\n",
    "else:\n",
    "    print(\"⚠️  No statistically significant difference (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Memorization vs. Generalization\n",
    "\n",
    "Test whether the model has memorized vs. learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test on exact phrasing vs. paraphrased\n",
    "\n",
    "exact_prompt = \"To be or not to be, that is the\"\n",
    "paraphrase_prompt = \"The question is whether to exist or not exist, that is the\"\n",
    "\n",
    "# Get next token probabilities\n",
    "probs_exact = get_next_token_probabilities(exact_prompt)\n",
    "probs_para = get_next_token_probabilities(paraphrase_prompt)\n",
    "\n",
    "# Expected continuation: \"question\"\n",
    "target_token = \" question\"\n",
    "prob_exact = get_token_probability(exact_prompt, target_token)\n",
    "prob_para = get_token_probability(paraphrase_prompt, target_token)\n",
    "\n",
    "print(\"Memorization Test:\")\n",
    "print(f\"  Exact phrase: P('{target_token}') = {prob_exact:.4f}\")\n",
    "print(f\"  Paraphrased:  P('{target_token}') = {prob_para:.4f}\")\n",
    "print(f\"\\n  Probability ratio: {prob_exact / (prob_para + 1e-10):.2f}x\")\n",
    "\n",
    "if prob_exact > 10 * prob_para:\n",
    "    print(\"  ⚠️ Likely memorized (much higher prob for exact phrasing)\")\n",
    "else:\n",
    "    print(\"  ✓ May have generalized (similar probs for paraphrase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1: Design Generalization Tests\n",
    "\n",
    "Create pairs of prompts to test memorization vs. understanding for your concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create test pairs for your concept\n",
    "# Compare:\n",
    "# 1. Common phrasing vs. novel phrasing\n",
    "# 2. Standard examples vs. counterfactual examples\n",
    "# 3. In-distribution vs. out-of-distribution cases\n",
    "\n",
    "test_pairs = [\n",
    "    # (\"common_phrasing\", \"novel_phrasing\", \"expected_answer\"),\n",
    "]\n",
    "\n",
    "# Test and compare probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Answer these in your project writeup:\n",
    "\n",
    "1. **Prompting Strategy**: Which prompting strategy (instruction, cloze, ICL) works best for your concept? Why?\n",
    "\n",
    "2. **Metrics**: Which evaluation metrics (accuracy, precision/recall, perplexity) are most appropriate for your concept? Why?\n",
    "\n",
    "3. **Sample Size**: How many test examples do you need for statistical significance? Use the confidence interval calculations to justify your answer.\n",
    "\n",
    "4. **Memorization**: How will you distinguish memorization from true understanding? What makes a prompt unlikely to have been in training data?\n",
    "\n",
    "5. **Generalization**: What variations of your prompts will test for genuine concept understanding rather than surface pattern matching?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Use this notebook as a foundation for your Week 1 assignment:\n",
    "- Design your benchmark prompts\n",
    "- Test them on GPT-2 or another model\n",
    "- Calculate evaluation metrics\n",
    "- Assess statistical significance\n",
    "- Create generalization tests\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
