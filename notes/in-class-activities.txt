I am thinking through in-class activities, and we may need some logistical setup for these.

Week 0: theme is "what is mech interp research" and "what makes a good research question" in-class activity is forming teams for the projects, and actually creating gdrives and gdocs listing the team members.  I would like the teams to be set by the end of the class!  There are one or two students who cannot attend this day so we should make sure to include them/assign them on teams using email head of time.

Week 1: here we teach necessary foundations, i.e., what is a transformer language model, and some review of machine learning/NLP basics.  Then we teach what logit lens is  in-class activity is to use ndif's logit lens to try the wendler thinking in english experiment and then we introduce the model problem of "understanding puns" - and we use logit lens to examine a few models' internal processing [or lack of awareness] of puns.  Homework will be to use logit lens on your own project concept [and to do other project pitch writing].

Week 2: we teach "a vector is a concept", discussing the piantadosi paper and also the superposition hypothesis, and we also discuss steering.  In-class activity is to use neuronpedia to look for pun-awareness vectors in different models.  The problem here is that neuronpeida is not actually strong enough to do this yet, so I have emailed johnny to ask for some help here (todo we need to follow up with johnny).  Homework will be to use neuronpeida to look for signs of life of your own project concepts [and to create slides and present your first exploratory experiment results]

Week 3: we teach "AI-assisted evaluation", discussing the perez paper as well as some other evaluation issues.  In-class activity will be to prompt an AI agent such as claude code to create evaluation datasets for the "pun" problem, and then to turn around and evaluate several models for pun capabilities, either on the outside (just using model inference APIs) or on the inside (using ndif logit lens or neuronpeida at the API level - to do these in week 1 and 2 we should probably also provide code scaffolding for both logit lens and neuronpedia beside the UI).  Homework for this week will be to set up the github for the project and do the same sort of thing for your own project. 

Week 4: we revision the problem of representation geometry, discussing semantic vector arithmetic and methods for visualizing concept space.  As a classroom exercise we use nnsight to gather activations of the "pun" example and use colab or raw local python with a claude-code agent to create a PCA visualization of the activations and push it to an interactive website.    Homework will be to similarly visualize your own concepts.

Week 5: we introduce causal mediation analysis, discussing a set of experiments that use it.  Then the in-class activity use ndif's CMA tool to use CMA to pin down the representation of pun knowledge (probably in ICL prompts).  Then we write some nsight CMA code to get a layer or a narrower neural representation of puns that has causal effects.

Week 6: is about training probes.  Here we need some small amount of training computation available for the course.  Todo: do we have funding for running this course @aruna ?  Maybe we should spend it on runpod or some other computation resource and explain to students how to use it.  Or @arnab / @Nikhil Prakash maybe we should write a brief proposal to ACCESS/Delta so that students can have accounts on NSF resources.  They won't need much.  or @JadenFK is it feasible for them to train probes using nnsight on ndif yet?  As a classroom exercise I'd like them to train probes to detect the model's awareness of presence of punny predictions as oopposed to straight predictions.  I'd love ideas from @Sheridan  or @Grace Proebsting who have been training tons of probes what is practical here for the class, and what resources we need to set up ahead of time. 

Week 7: is about attribution analysis.  Here we need to figure out how to enable students to use inseq.  @Gabriele Sarti - maybe you can help me think through what a good in-class exercise is.  I'd like to relate it to the "pun" exercise - e.g., can we visualize or evaluate what parts of the input are necessary for the model to attribute a punny predictiioin?  In the zero-shot case?  In the CIL case?

Week 8: is about circuit-finding, and I would like to try running michael Hanna's circuit finding tool on the pun problem as an in-class exercise.  What is the right way to do this?  Neuronpedia advertises support for his tool and I know @JadenFK has been working on supporting his tool via nnsight, but - what is the right way for us to run it as part of the class?

Week 9: training dynamics.  Here the natural thing to do is to get all the teams to join the ndif hot swapping pilot to give them access to snapshots of Olmo.  @Emma Bortz any issues with setting that up for class members?  As a classroom exercise, I'd like them to see what is involved in testing the "pun" capabilities acros training iterations.  I'd love them to see the training iteration evolution plots that @Eric Todd had in the algebra paper (I will add it as a reading).

Week 10: In lecture I'll talk about both self-description and the bridging the human-AI knowledge gap research.  The class activity will focus on the easier one of those which is self-description - As a class activity we should try a neologism setup or patchscopes to analyze the models's ability to self-articulate knowledge of pun representations.   I think we could aim to do this as a colab notebook, but this will require a bit of prep.

And then that's it for in-class lab activites.  I think if we can actually pull off all these it will be an awesome course.

