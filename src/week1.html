<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 1: Foundations - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 1: Foundations</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">&larr; Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      How can we peer inside a running language model to see what it's "thinking"? The key insight is that transformer
      intermediate layers encode evolving predictions that we can decode and inspect. This week introduces the conceptual
      vocabulary and core techniques for mechanistic interpretability, including the logit lens&mdash;a simple but powerful
      idea that opened a window into the progressive refinement of representations through the network.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Explain what a transformer language model is and how it processes sequences</li>
      <li>Describe the residual stream view of transformers</li>
      <li>Identify the main components: token embeddings, attention layers, MLP layers, and unembedding</li>
      <li>Apply the logit lens to decode intermediate layer activations into vocabulary space</li>
      <li>Interpret what the model is "predicting" at each layer</li>
      <li>Understand how predictions evolve across layers</li>
      <li>Explain the difference between base logit lens and tuned lens approaches</li>
      <li>Discuss what latent language reveals about multilingual model internals</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2405.00208" target="_blank">A Primer on the Inner Workings of Transformer-based Language Models</a>
        </div>
        <div class="reading-description">Ferrando, Sarti, Bisazza &amp; Costa-juss&agrave; (2024). Accessible pedagogical overview of interpretability techniques, establishing shared vocabulary for the course.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank">Interpreting GPT: The Logit Lens</a>
        </div>
        <div class="reading-description">nostalgebraist (2020). The foundational blog post introducing logit lens&mdash;decoding intermediate layer activations directly into vocabulary space.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2402.10588" target="_blank">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</a>
        </div>
        <div class="reading-description">Wendler et al. (2024). Applies logit lens to multilingual models, revealing that models often pivot through English in intermediate representations.</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2303.08112" target="_blank">Eliciting Latent Predictions from Transformers with the Tuned Lens</a>
        </div>
        <div class="reading-description">Belrose et al. (2023). Refined version of logit lens with learned probes for more accurate intermediate decoding.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">A Mathematical Framework for Transformer Circuits</a>
        </div>
        <div class="reading-description">Elhage et al. (2021). Deeper mathematical treatment of the residual stream view. Dense but foundational.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Understanding Transformer Internals</h2>

    <h3>1. The Transformer as a Grid of States</h3>
    <p>
      A transformer language model processes text through three phases, creating a grid of internal states:
    </p>

    <img src="imgs/transformer-state-grid-3-phases.png" alt="Transformer state grid showing three phases: encoder, layers, and decoder" style="max-width: 100%; margin: 20px 0;">

    <ol>
      <li><strong>Encoder:</strong> First, the encoder turns each input token into a vector of neural activations. The word "the" becomes a high-dimensional vector.</li>
      <li><strong>Layers:</strong> Then a series of neural layers mixes and transforms the vectors for each token. Information flows both along each token's "column" and across tokens via attention.</li>
      <li><strong>Decoder:</strong> Finally, the decoder turns each vector into a prediction for the next word. After "Miles Davis plays the", the model predicts "trumpet".</li>
    </ol>

    <h3>2. The Residual Stream View</h3>
    <p>
      A transformer can be understood as a series of operations that read from and write to a shared "residual stream."
      At each layer, attention heads and MLP modules add information to this stream rather than replacing it entirely.
    </p>

    <div class="diagram">
      x<sub>0</sub> = embed(token)<br>
      x<sub>1</sub> = x<sub>0</sub> + attention<sub>1</sub>(x<sub>0</sub>) + MLP<sub>1</sub>(x<sub>0</sub>)<br>
      x<sub>2</sub> = x<sub>1</sub> + attention<sub>2</sub>(x<sub>1</sub>) + MLP<sub>2</sub>(x<sub>1</sub>)<br>
      ...<br>
      output = unembed(x<sub>final</sub>)
    </div>

    <p>
      This view enables a key insight: we can peek at intermediate states x<sub>L</sub> to see what the model
      "knows" at layer L.
    </p>

    <h3>3. The Logit Lens: Early Exit Decoding</h3>
    <p>
      What if we don't wait until the final layer to decode? The <strong>logit lens</strong> applies the decoder (unembedding matrix) directly to intermediate activations:
    </p>

    <img src="imgs/transformer-early-exit-logit-lens.png" alt="Early exit decoding showing how predictions evolve: thing → jazz → horn → horn → trumpet" style="max-width: 100%; margin: 20px 0;">

    <p>
      If you decode each vector early, you can see how the prediction evolves. In this example, the model's prediction for "Miles Davis plays the ___" progresses through: "thing" &rarr; "jazz" &rarr; "horn" &rarr; "horn" &rarr; "trumpet". The correct answer emerges gradually as information is refined through the layers.
    </p>

    <div class="diagram">
      intermediate_logits = W<sub>U</sub> &middot; x<sub>L</sub><br>
      probs = softmax(intermediate_logits)
    </div>

    <p>
      Key observations from logit lens analysis:
    </p>
    <ul>
      <li>Early layers often predict generic or input-related tokens</li>
      <li>Middle layers begin to show task-relevant predictions</li>
      <li>Late layers refine toward the final answer</li>
      <li>The "correct" prediction often emerges gradually, not suddenly</li>
    </ul>

    <h3>4. What Does Latent Language Reveal?</h3>
    <p>
      When applying logit lens to multilingual models processing non-English text, a striking pattern emerges:
      regardless of input language, intermediate representations often decode to English tokens.
    </p>

    <p>
      <strong>Wendler et al. (2024)</strong> studied this systematically with translation tasks. When prompting a model to translate French to Chinese (e.g., <code>Fran&ccedil;ais: "fleur" - 中文:</code>), the intermediate layers decode to... English! The model appears to pivot through English internally, even when English is neither the input nor output language.
    </p>

    <p>
      <strong>Possible implications:</strong>
    </p>
    <ul>
      <li>Interpretability tools developed on English may apply to other languages</li>
      <li>Models may not process all languages equally</li>
      <li>Internal representations are not simple copies of input/output</li>
    </ul>

    <p>
      <strong>Caveats:</strong> These observations are intriguing but require careful interpretation. The logit lens decoding shows English words, but this does not prove that the model "thinks in English" or that English serves as an internal lingua franca. Alternative explanations include: the unembedding matrix may be biased toward English tokens (since English dominates training data); the pattern may not generalize across all word types or model sizes; and crucially, we have no causal evidence that these English representations are <em>used</em> by the model rather than being epiphenomenal. The Wendler results are best viewed as a surprising observation that motivates further investigation, not a settled conclusion about how multilingual models work.
    </p>

    <h3>5. Tuned Lens: Beyond Raw Projection</h3>
    <p>
      The basic logit lens assumes we can directly project intermediate activations with the unembedding matrix.
      But representations at early layers may not be in the same "format" as final-layer representations.
    </p>

    <p>
      The <strong>tuned lens</strong> learns a small affine transformation at each layer:
    </p>

    <div class="diagram">
      tuned_logits = W<sub>U</sub> &middot; (A<sub>L</sub> x<sub>L</sub> + b<sub>L</sub>)
    </div>

    <p>
      This accounts for layer-specific representation formats, often yielding more interpretable intermediate predictions.
    </p>

    <h3>6. What Logit Lens Does NOT Tell You</h3>
    <p>
      Logit lens is a powerful exploratory tool, but it has important limitations. Understanding what it <em>cannot</em> tell you is essential for interpreting results correctly.
    </p>

    <ul>
      <li><strong>No causal power:</strong> Logit lens is purely observational. Seeing a word decoded at layer 15 does not mean that layer 15 "caused" or "computed" that prediction. The model might have computed the answer earlier, or the decoded word might be a side effect of unrelated processing. To establish causation, you need intervention experiments (coming in Week 5).</li>

      <li><strong>No generalization claims:</strong> Observing a pattern on one prompt tells you nothing about whether the pattern holds in general. The model might decode "flower" in intermediate layers for this particular French word, but behave completely differently for other words, phrasings, or contexts.</li>

      <li><strong>No explanation of "why":</strong> Even when logit lens shows an interesting pattern&mdash;like English appearing in intermediate layers during translation&mdash;it doesn't explain <em>why</em> this happens. Is English a computational lingua franca? An artifact of training data? A consequence of tokenization? Logit lens alone cannot answer these questions.</li>

      <li><strong>No guarantee of capability:</strong> Seeing the "right" answer appear at some layer doesn't mean the model actually has the capability you're testing. The decoded token might be a lucky correlation, or the model might fail on slight variations of the same task. Behavioral evaluation (coming in Week 3) is needed to establish what a model can actually do.</li>
    </ul>

    <p>
      <strong>Bottom line:</strong> Logit lens is best understood as a <em>hypothesis generator</em>, not a hypothesis tester. It can reveal surprising patterns that suggest where to look deeper, but confirming those patterns requires additional methods&mdash;causal interventions, systematic evaluation, and careful controls.
    </p>
  </section>

  <section id="in-class-exercise">
    <h2>In-Class Exercise: Logit Lens Workbench</h2>

    <p>
      In this hands-on exercise, we'll use the <a href="https://workbench.ndif.us" target="_blank">NDIF Logit Lens Workbench</a>&mdash;a code-free tool for exploring transformer internals. No programming required!
    </p>

    <a href="https://workbench.ndif.us" target="_blank" class="colab-button">
      Open NDIF Logit Lens Workbench
    </a>

    <h3>Part 1: Reproduce the Wendler Results</h3>
    <p>
      First, we'll replicate the "latent language" finding from Wendler et al. (2024). Does the model really pivot through English when translating between non-English languages?
    </p>

    <h4>Instructions:</h4>
    <ol>
      <li>Open the workbench and select a multilingual model (e.g., Llama-2)</li>
      <li>Enter a French-to-Chinese translation prompt:
        <div class="diagram" style="text-align: left;">
          Fran&ccedil;ais: "cinq" - 中文: "五"<br>
          Fran&ccedil;ais: "coeur" - 中文: "心"<br>
          Fran&ccedil;ais: "trois" - 中文: "三"<br>
          Fran&ccedil;ais: "nuage" - 中文:
        </div>
      </li>
      <li>Examine the logit lens heatmap for the final token position</li>
      <li>At which layers do you see English words appearing? At which layers does Chinese emerge?</li>
    </ol>

    <h4>Discussion Questions:</h4>
    <ul>
      <li>What English words appear in the intermediate layers?</li>
      <li>Is the pattern consistent across different French words?</li>
      <li>Try reversing the direction (Chinese &rarr; German). Do you still see English?</li>
    </ul>

    <h3>Part 2: Exploring a Research Question&mdash;Do Models Understand Puns?</h3>
    <p>
      Now let's apply logit lens to investigate a research question we'll revisit throughout the course: <strong>how do language models process puns?</strong>
    </p>

    <p>
      Puns are interesting because they require recognizing that a word has multiple meanings simultaneously. If a model "gets" a pun, we might expect to see both meanings active in its intermediate representations.
    </p>

    <h4>Example Puns to Explore:</h4>
    <div class="diagram" style="text-align: left;">
      "I used to be a banker, but I lost interest."<br><br>
      "The past, present, and future walked into a bar. It was tense."<br><br>
      "I'm reading a book about anti-gravity. It's impossible to put down."
    </div>

    <h4>Instructions:</h4>
    <ol>
      <li>Enter a pun-setup prompt that leads to a punchline word</li>
      <li>Look at the logit lens output for the position just before the punchline</li>
      <li>Do you see evidence of both meanings of the pun word?</li>
      <li>At what layer does the "correct" (pun) completion emerge?</li>
      <li>Compare with a non-pun sentence using the same words&mdash;does the pattern differ?</li>
    </ol>

    <h4>Research Questions to Consider:</h4>
    <ul>
      <li>Can you find evidence that the model represents both meanings of the pun word?</li>
      <li>Does the "wrong" meaning appear at early layers before being suppressed?</li>
      <li>How does the model's behavior differ between good puns and bad puns?</li>
      <li>What would it mean to "localize" pun understanding in the model?</li>
    </ul>

    <p>
      <em>We'll return to the pun research question throughout the semester as we learn new interpretability methods. Each technique will give us a different lens on the same phenomenon.</em>
    </p>
  </section>

  <section id="code-exercise">
    <h2>Optional: Code Exercise</h2>
    <p>
      For those who want to dive deeper into the implementation, this exercise provides hands-on coding experience:
    </p>
    <ul>
      <li>Load a transformer language model and examine its architecture</li>
      <li>Extract activation vectors at different layers</li>
      <li>Implement the logit lens to decode intermediate predictions</li>
      <li>Visualize how predictions evolve across layers</li>
      <li>Compare logit lens results across different prompts</li>
    </ul>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER" target="_blank" class="colab-button">
      Open Exercise in Google Colab
    </a>

    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be linked here.</em>
    </p>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 1</strong></p>
      <p>
        This week focuses on getting oriented with the tools and selecting your concept for the semester project.
      </p>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Concept pitches:</strong> Prepare 2-3 candidate concepts from your domain expertise</li>
        <li><strong>Literature review:</strong> Find 3-5 papers relevant to your concepts (not necessarily interpretability papers)</li>
        <li><strong>Initial exploration:</strong> Apply logit lens to prompts related to your concepts
          <ul>
            <li>Do the intermediate predictions seem related to your concept?</li>
            <li>At which layers does relevant information appear?</li>
          </ul>
        </li>
      </ul>

      <p><em>
        Tip: Choose concepts that can be expressed in language and that the model might reasonably "know about."
        Abstract concepts from your field (legal reasoning, medical diagnosis, literary themes) are great candidates.
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">&larr; Back to Course Home</a></p>
  </footer>

</body>

</html>
