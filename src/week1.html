<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 1: Foundations - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 1: Foundations</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">&larr; Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      How can we peer inside a running language model to see what it's "thinking"? The key insight is that transformer
      intermediate layers encode evolving predictions that we can decode and inspect. This week introduces the conceptual
      vocabulary and core techniques for mechanistic interpretability, including the logit lens&mdash;a simple but powerful
      idea that opened a window into the progressive refinement of representations through the network.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Explain what a transformer language model is and how it processes sequences</li>
      <li>Describe the residual stream view of transformers</li>
      <li>Identify the main components: token embeddings, attention layers, MLP layers, and unembedding</li>
      <li>Apply the logit lens to decode intermediate layer activations into vocabulary space</li>
      <li>Interpret what the model is "predicting" at each layer</li>
      <li>Understand how predictions evolve across layers</li>
      <li>Explain the difference between base logit lens and tuned lens approaches</li>
      <li>Discuss what latent language reveals about multilingual model internals</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2405.00208" target="_blank">A Primer on the Inner Workings of Transformer-based Language Models</a>
        </div>
        <div class="reading-description">Ferrando, Sarti, Bisazza &amp; Costa-juss&agrave; (2024). Accessible pedagogical overview of interpretability techniques, establishing shared vocabulary for the course.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank">Interpreting GPT: The Logit Lens</a>
        </div>
        <div class="reading-description">nostalgebraist (2020). The foundational blog post introducing logit lens&mdash;decoding intermediate layer activations directly into vocabulary space.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2402.10588" target="_blank">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</a>
        </div>
        <div class="reading-description">Wendler et al. (2024). Applies logit lens to multilingual models, revealing that models often pivot through English in intermediate representations.</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2303.08112" target="_blank">Eliciting Latent Predictions from Transformers with the Tuned Lens</a>
        </div>
        <div class="reading-description">Belrose et al. (2023). Refined version of logit lens with learned probes for more accurate intermediate decoding.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">A Mathematical Framework for Transformer Circuits</a>
        </div>
        <div class="reading-description">Elhage et al. (2021). Deeper mathematical treatment of the residual stream view. Dense but foundational.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://www.youtube.com/watch?v=e9-0BxyKG10" target="_blank">How do Transformers Work?</a>
        </div>
        <div class="reading-description">Professor Bryce. Visual explanation of transformer architecture. Helpful background for understanding the models we're interpreting.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Understanding Transformer Internals</h2>

    <h3>1. The Transformer as a Grid of States</h3>
    <p>
      A transformer language model processes text through three phases, creating a grid of internal states:
    </p>

    <img src="imgs/transformer-state-grid-3-phases.png" alt="Transformer state grid showing three phases: encoder, layers, and decoder" style="max-width: 100%; margin: 20px 0;">

    <ol>
      <li><strong>Encoder:</strong> First, the encoder turns each input token into a vector of neural activations. The word "the" becomes a high-dimensional vector.</li>
      <li><strong>Layers:</strong> Then a series of neural layers mixes and transforms the vectors for each token. Information flows both along each token's "column" and across tokens via attention.</li>
      <li><strong>Decoder:</strong> Finally, the decoder turns each vector into a prediction for the next word. After "Miles Davis plays the", the model predicts "trumpet".</li>
    </ol>

    <h3>2. The Residual Stream View</h3>
    <p>
      A transformer can be understood as a series of operations that read from and write to a shared "residual stream."
      At each layer, attention heads and MLP modules add information to this stream rather than replacing it entirely.
    </p>

    <div class="diagram">
      x<sub>0</sub> = embed(token)<br>
      x<sub>1</sub> = x<sub>0</sub> + attention<sub>1</sub>(x<sub>0</sub>) + MLP<sub>1</sub>(x<sub>0</sub>)<br>
      x<sub>2</sub> = x<sub>1</sub> + attention<sub>2</sub>(x<sub>1</sub>) + MLP<sub>2</sub>(x<sub>1</sub>)<br>
      ...<br>
      output = unembed(x<sub>final</sub>)
    </div>

    <p>
      This view enables a key insight: we can peek at intermediate states x<sub>L</sub> to see what the model
      "knows" at layer L.
    </p>

    <h3>3. The Logit Lens: Early Exit Decoding</h3>
    <p>
      What if we don't wait until the final layer to decode? The <strong>logit lens</strong> applies the decoder (unembedding matrix) directly to intermediate activations:
    </p>

    <img src="imgs/transformer-early-exit-logit-lens.png" alt="Early exit decoding showing how predictions evolve: thing → jazz → horn → horn → trumpet" style="max-width: 100%; margin: 20px 0;">

    <p>
      If you decode each vector early, you can see how the prediction evolves. In this example, the model's prediction for "Miles Davis plays the ___" progresses through: "thing" &rarr; "jazz" &rarr; "horn" &rarr; "horn" &rarr; "trumpet". The correct answer emerges gradually as information is refined through the layers.
    </p>

    <div class="diagram">
      intermediate_logits = W<sub>U</sub> &middot; x<sub>L</sub><br>
      probs = softmax(intermediate_logits)
    </div>

    <p>
      Key observations from logit lens analysis:
    </p>
    <ul>
      <li>Early layers often predict generic or input-related tokens</li>
      <li>Middle layers begin to show task-relevant predictions</li>
      <li>Late layers refine toward the final answer</li>
      <li>The "correct" prediction often emerges gradually, not suddenly</li>
    </ul>

    <h3>4. What Does Latent Language Reveal?</h3>
    <p>
      When applying logit lens to multilingual models processing non-English text, a striking pattern emerges:
      regardless of input language, intermediate representations often decode to English tokens.
    </p>

    <p>
      <strong>Wendler et al. (2024)</strong> studied this systematically with translation tasks. When prompting a model to translate French to Chinese (e.g., <code>Fran&ccedil;ais: "fleur" - 中文:</code>), the intermediate layers decode to... English! The model appears to pivot through English internally, even when English is neither the input nor output language.
    </p>

    <p>
      <strong>Possible implications:</strong>
    </p>
    <ul>
      <li>Perhaps the model has an internal "concept language" that decodes as English regardless of input and output languages</li>
      <li>This suggests models might have a <em>language-independent</em> way of representing and processing concepts</li>
      <li>It strongly suggests that concepts are not just words&mdash;the internal representations are something different from the surface tokens</li>
    </ul>

    <p>
      <strong>But what does this actually show?</strong> The observation that intermediate layers decode to English is striking, but its interpretation is far from settled. Consider: the unembedding matrix was trained predominantly on English data&mdash;perhaps it simply projects <em>any</em> high-dimensional vector more readily onto English tokens. Or perhaps the pattern reflects tokenization artifacts rather than genuine linguistic processing. And even if the model does "pivot through English," we have no causal evidence that these intermediate English representations are actually <em>used</em> in the computation, rather than being epiphenomenal byproducts. We'll return to further experiments along these lines later in the course, but it's a reasonable debate: what does this really prove?
    </p>

    <h3>5. Variations on Logit Lens</h3>
    <p>
      The basic logit lens assumes we can directly project intermediate activations through the unembedding matrix.
      But representations at early layers may not be in the same "format" as final-layer representations, and researchers have developed many variations:
    </p>
    <ul>
      <li><strong>Tuned Lens</strong> (Belrose et al., 2023): Learns a small affine transformation at each layer to account for layer-specific representation formats</li>
      <li><strong>Future Lens</strong> (Pal et al., 2023): Predicts not just the next token but future tokens from intermediate states</li>
      <li><strong>Concept Lens</strong> (Feucht et al., 2024): Projects onto concept-specific directions rather than vocabulary tokens</li>
    </ul>
    <p>
      Each variation asks a slightly different question about what information is present in intermediate representations. The proliferation of "lenses" reflects both the fertility of the basic idea and the difficulty of interpreting what these projections really mean.
    </p>

    <h3>6. From Observation to Understanding: What Experiments Are Needed?</h3>
    <p>
      Logit lens generates striking observations. But observations are not explanations. The central challenge is: <strong>what additional experiments would you need to turn a logit lens observation into a scientific claim?</strong>
    </p>

    <div class="discussion-box" style="background-color: #fff9e6; border-left: 4px solid #f39c12; padding: 15px; margin: 20px 0;">
      <h4>Class Discussion: Designing Follow-up Experiments</h4>
      <p>Consider the Wendler observation: intermediate layers decode to English during French&rarr;Chinese translation.</p>
      <ol>
        <li><strong>Hypothesis:</strong> "The model uses English as an internal lingua franca."</li>
        <li><strong>Alternative hypothesis:</strong> "The decoder matrix is biased toward English tokens."</li>
        <li><strong>Another alternative:</strong> "This is a tokenization artifact."</li>
      </ol>
      <p><strong>Your task:</strong> For each alternative, design an experiment that would distinguish it from the main hypothesis. What would you measure? What controls would you need? What result would be convincing?</p>
    </div>

    <p>
      Similar questions arise for any logit lens observation:
    </p>
    <ul>
      <li>If you see a prediction "emerge" across layers (thing &rarr; jazz &rarr; horn &rarr; trumpet), how would you test whether those intermediate states are <em>causally</em> involved in the final prediction, versus being epiphenomenal?</li>
      <li>If a pattern appears in one example, what systematic evaluation would establish generalization? How many examples? What variations?</li>
      <li>If logit lens shows the "right answer" at layer 20, how would you verify that the model actually has the capability you're testing, rather than exploiting a spurious correlation?</li>
    </ul>

    <p>
      We'll develop tools to address these questions throughout the course: causal interventions (Week 5), systematic evaluation methodology (Week 3), and validation techniques (Weeks 6-7). For now, the goal is to recognize the gap between observation and understanding&mdash;and to start thinking about how to bridge it.
    </p>
  </section>

  <section id="in-class-exercise">
    <h2>In-Class Exercise: Logit Lens Workbench</h2>

    <p>
      In this hands-on exercise, we'll use the <a href="https://workbench.ndif.us" target="_blank">NDIF Logit Lens Workbench</a>&mdash;a code-free tool for exploring transformer internals. No programming required!
    </p>

    <a href="https://workbench.ndif.us" target="_blank" class="colab-button">
      Open NDIF Logit Lens Workbench
    </a>

    <h3>Part 1: Reproduce the Wendler Results</h3>
    <p>
      First, we'll replicate the "latent language" finding from Wendler et al. (2024). Does the model really pivot through English when translating between non-English languages?
    </p>

    <h4>Instructions:</h4>
    <ol>
      <li>Open the workbench and select a multilingual model (e.g., Llama-2)</li>
      <li>Enter a French-to-Chinese translation prompt:
        <div class="diagram" style="text-align: left;">
          Fran&ccedil;ais: "cinq" - 中文: "五"<br>
          Fran&ccedil;ais: "coeur" - 中文: "心"<br>
          Fran&ccedil;ais: "trois" - 中文: "三"<br>
          Fran&ccedil;ais: "nuage" - 中文:
        </div>
      </li>
      <li>Examine the logit lens heatmap for the final token position</li>
      <li>At which layers do you see English words appearing? At which layers does Chinese emerge?</li>
    </ol>

    <h4>Discussion Questions:</h4>
    <ul>
      <li>What English words appear in the intermediate layers?</li>
      <li>Is the pattern consistent across different French words?</li>
      <li>Try reversing the direction (Chinese &rarr; German). Do you still see English?</li>
    </ul>

    <h3>Part 2: Exploring a Research Question&mdash;Do Models Understand Puns?</h3>
    <p>
      Now let's apply logit lens to investigate a research question we'll revisit throughout the course: <strong>how do language models process puns?</strong>
    </p>

    <p>
      Puns are interesting because they require recognizing that a word has multiple meanings simultaneously. If a model "gets" a pun, we might expect to see both meanings active in its intermediate representations.
    </p>

    <h4>Example Puns to Explore:</h4>
    <div class="diagram" style="text-align: left;">
      "I used to be a banker, but I lost interest."<br><br>
      "The past, present, and future walked into a bar. It was tense."<br><br>
      "I'm reading a book about anti-gravity. It's impossible to put down."
    </div>

    <h4>Instructions:</h4>
    <ol>
      <li>Enter a pun-setup prompt that leads to a punchline word</li>
      <li>Look at the logit lens output for the position just before the punchline</li>
      <li>Do you see evidence of both meanings of the pun word?</li>
      <li>At what layer does the "correct" (pun) completion emerge?</li>
      <li>Compare with a non-pun sentence using the same words&mdash;does the pattern differ?</li>
    </ol>

    <h4>In-Context Learning of Puns</h4>
    <p>
      Here's a more sophisticated experiment: can we use <em>in-context learning</em> to teach the model the "pun pattern"?
    </p>
    <p>
      Try comparing the prediction for an incomplete pun <strong>alone</strong> versus <strong>prefixed with other puns</strong>:
    </p>
    <div class="diagram" style="text-align: left;">
      <strong>Without context:</strong><br>
      "I used to be a banker, but I lost ___"<br>
      <em>(Model might predict: "my job", "everything", "money")</em><br><br>
      <strong>With pun context:</strong><br>
      "I used to be a tailor, but I wasn't suited for it.<br>
      I used to be a train conductor, but I switched tracks.<br>
      I used to be a banker, but I lost ___"<br>
      <em>(Model might now predict: "interest")</em>
    </div>
    <p>
      Use logit lens to examine: at which layers does "interest" emerge? Does the pun context change what appears in intermediate layers, or only at the final layers? This connects back to the function vectors idea from Week 0&mdash;is there a "pun function" that gets activated by the context?
    </p>

    <h4>Comparing Model Sizes</h4>
    <p>
      Try the same pun experiments on models of different sizes. At what scale does pun understanding emerge? A small model might never predict "interest" even with pun context, while a larger model might get it immediately. Use this to explore: <em>how large does a model need to be to "get" puns?</em> Does the pattern in intermediate layers look different in models that succeed versus those that fail?
    </p>

    <h4>Using SOTA Models as a Reference</h4>
    <p>
      State-of-the-art models like ChatGPT or Claude can help in two ways:
    </p>
    <ul>
      <li><strong>Generate test cases:</strong> Ask for puns with a specific structure, e.g., "Generate 10 puns that follow the pattern 'I used to be a [profession], but [punchline using profession-related word with double meaning].'" This gives you a systematic dataset to explore.</li>
      <li><strong>Check pun understanding:</strong> Give a SOTA model an incomplete pun and see if it can complete it correctly. This establishes a baseline&mdash;if even the best models struggle with certain puns, that's informative. If they succeed easily, you can then ask: what's different about the internal representations in models that "get it" versus those that don't?</li>
    </ul>

    <h4>Research Questions to Consider:</h4>
    <ul>
      <li>Can you find evidence that the model represents both meanings of the pun word?</li>
      <li>Does the "wrong" meaning appear at early layers before being suppressed?</li>
      <li>How does in-context pun examples change the intermediate representations?</li>
      <li>At what model size does pun understanding emerge?</li>
      <li>What would it mean to "localize" pun understanding in the model?</li>
    </ul>

    <p>
      <em>We'll return to the pun research question throughout the semester as we learn new interpretability methods. Each technique will give us a different lens on the same phenomenon.</em>
    </p>
  </section>

  <section id="code-exercise">
    <h2>Optional: Code Exercise</h2>
    <p>
      For those who want to dive deeper into the implementation, this exercise provides hands-on coding experience:
    </p>
    <ul>
      <li>Load a transformer language model and examine its architecture</li>
      <li>Extract activation vectors at different layers</li>
      <li>Implement the logit lens to decode intermediate predictions</li>
      <li>Visualize how predictions evolve across layers</li>
      <li>Compare logit lens results across different prompts</li>
    </ul>

    <a href="https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week1/logit_lens.ipynb" target="_blank" class="colab-button">
      Open Logit Lens Exercise in Colab
    </a>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 1</strong></p>

      <h4>Deliverable: Initial Pitch</h4>
      <ul>
        <li><strong>Pitch document:</strong> 1-2 page writeup describing your team's proposed concept, written as a Google Doc in your team's project Google Drive. Submit the link (make sure it's readable).</li>
        <li><strong>Elevator pitch:</strong> Prepare a 5-minute presentation of your idea for class.</li>
      </ul>

      <h4>Thursday In-Class Activity</h4>
      <p>
        Each team will present their five-minute elevator pitch. As a class, we will read each other's proposals and discuss them using the <strong>FINER framework</strong>:
      </p>
      <ul>
        <li>Is this <strong>Feasible</strong>? Can we find signs of life in the model?</li>
        <li>Is it <strong>Interesting</strong>? Would experts in the domain care?</li>
        <li>Is it <strong>Novel</strong>? Has this been studied before?</li>
        <li>Is it <strong>Ethical</strong>? Could this research enable harm?</li>
        <li>Is it <strong>Relevant</strong>? What changes if we answer this question?</li>
      </ul>

      <h4>Next Assignment (Due: Week 2)</h4>
      <p>
        After receiving feedback, begin <strong>exploratory analysis</strong> to test the "F" in FINER: <em>Is this feasible? Are there signs of life?</em>
      </p>
      <ul>
        <li>Use the <a href="https://workbench.ndif.us" target="_blank">Logit Lens Workbench</a> or <a href="https://www.neuronpedia.org" target="_blank">Neuronpedia</a> to explore your concept</li>
        <li>Look for preliminary evidence that the model represents or processes your concept</li>
        <li>Document interesting observations, even negative results</li>
      </ul>
      <p>
        <strong>Deliverable for Week 2:</strong> Start a running <strong>project slide deck</strong> in your team's Google Drive. Include your preliminary observations and be ready to present them in class.
      </p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">&larr; Back to Course Home</a></p>
  </footer>

</body>

</html>
