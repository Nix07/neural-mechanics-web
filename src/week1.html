<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Week 1: Foundations - Neural Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }

    header img {
      height: 60px;
      margin-right: 20px;
    }

    h1 {
      margin: 0;
      font-size: 1.8em;
    }

    section {
      margin-bottom: 40px;
    }

    h2 {
      font-size: 1.4em;
      margin-bottom: 10px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 1.2em;
      margin-top: 20px;
      margin-bottom: 10px;
    }

    h4 {
      font-size: 1.05em;
      margin-top: 15px;
      margin-bottom: 8px;
    }

    a {
      color: #0055a4;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .reading-list {
      margin-left: 20px;
    }

    .reading-item {
      margin-bottom: 15px;
    }

    .reading-title {
      font-weight: bold;
    }

    .reading-description {
      color: #555;
      font-style: italic;
    }

    ul {
      margin-left: 20px;
    }

    li {
      margin-bottom: 8px;
    }

    .assignment-box {
      background-color: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      margin: 20px 0;
    }

    .colab-button {
      display: inline-block;
      background-color: #0055a4;
      color: white;
      padding: 10px 20px;
      border-radius: 4px;
      margin: 10px 0;
      font-weight: bold;
    }

    .colab-button:hover {
      background-color: #003d7a;
      text-decoration: none;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    .diagram {
      background-color: #f8f8f8;
      border: 1px solid #ddd;
      padding: 15px;
      margin: 15px 0;
      font-family: monospace;
      text-align: center;
    }
  </style>
</head>

<body>

  <header>
    <img src="imgs/ne.png" alt="Northeastern Logo">
    <h1>Week 1: Foundations</h1>
  </header>

  <nav style="margin-bottom: 20px;">
    <a href="index.html">&larr; Back to Course Home</a>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      How can we peer inside a running language model to see what it's "thinking"? The key insight is that transformer
      intermediate layers encode evolving predictions that we can decode and inspect. This week introduces the conceptual
      vocabulary and core techniques for mechanistic interpretability, including the logit lens&mdash;a simple but powerful
      idea that opened a window into the progressive refinement of representations through the network.
    </p>
  </section>

  <section id="learning-objectives">
    <h2>Learning Objectives</h2>
    <p>By the end of this week, you should be able to:</p>
    <ul>
      <li>Explain what a transformer language model is and how it processes sequences</li>
      <li>Describe the residual stream view of transformers</li>
      <li>Identify the main components: token embeddings, attention layers, MLP layers, and unembedding</li>
      <li>Apply the logit lens to decode intermediate layer activations into vocabulary space</li>
      <li>Interpret what the model is "predicting" at each layer</li>
      <li>Understand how predictions evolve across layers</li>
      <li>Explain the difference between base logit lens and tuned lens approaches</li>
      <li>Discuss what latent language reveals about multilingual model internals</li>
    </ul>
  </section>

  <section id="readings">
    <h2>Required Readings</h2>

    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2405.00208" target="_blank">A Primer on the Inner Workings of Transformer-based Language Models</a>
        </div>
        <div class="reading-description">Ferrando, Sarti, Bisazza &amp; Costa-juss&agrave; (2024). Accessible pedagogical overview of interpretability techniques, establishing shared vocabulary for the course.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank">Interpreting GPT: The Logit Lens</a>
        </div>
        <div class="reading-description">nostalgebraist (2020). The foundational blog post introducing logit lens&mdash;decoding intermediate layer activations directly into vocabulary space.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2402.10588" target="_blank">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</a>
        </div>
        <div class="reading-description">Wendler et al. (2024). Applies logit lens to multilingual models, revealing that models often pivot through English in intermediate representations.</div>
      </div>
    </div>

    <h3>Supplementary Readings</h3>
    <div class="reading-list">
      <div class="reading-item">
        <div class="reading-title">
          <a href="https://arxiv.org/abs/2303.08112" target="_blank">Eliciting Latent Predictions from Transformers with the Tuned Lens</a>
        </div>
        <div class="reading-description">Belrose et al. (2023). Refined version of logit lens with learned probes for more accurate intermediate decoding.</div>
      </div>

      <div class="reading-item">
        <div class="reading-title">
          <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">A Mathematical Framework for Transformer Circuits</a>
        </div>
        <div class="reading-description">Elhage et al. (2021). Deeper mathematical treatment of the residual stream view. Dense but foundational.</div>
      </div>
    </div>
  </section>

  <section id="tutorial">
    <h2>Tutorial: Understanding Transformer Internals</h2>

    <h3>1. The Residual Stream View</h3>
    <p>
      A transformer can be understood as a series of operations that read from and write to a shared "residual stream."
      At each layer, attention heads and MLP modules add information to this stream rather than replacing it entirely.
    </p>

    <div class="diagram">
      x<sub>0</sub> = embed(token)<br>
      x<sub>1</sub> = x<sub>0</sub> + attention<sub>1</sub>(x<sub>0</sub>) + MLP<sub>1</sub>(x<sub>0</sub>)<br>
      x<sub>2</sub> = x<sub>1</sub> + attention<sub>2</sub>(x<sub>1</sub>) + MLP<sub>2</sub>(x<sub>1</sub>)<br>
      ...<br>
      output = unembed(x<sub>final</sub>)
    </div>

    <p>
      This view enables a key insight: we can peek at intermediate states x<sub>L</sub> to see what the model
      "knows" at layer L.
    </p>

    <h3>2. The Logit Lens</h3>
    <p>
      The <strong>logit lens</strong> applies the unembedding matrix directly to intermediate activations:
    </p>

    <div class="diagram">
      intermediate_logits = W<sub>U</sub> &middot; x<sub>L</sub><br>
      probs = softmax(intermediate_logits)
    </div>

    <p>
      This lets us see what the model would "predict" if we stopped processing at layer L. Key observations:
    </p>
    <ul>
      <li>Early layers often predict the input token or simple continuations</li>
      <li>Middle layers begin to show task-relevant predictions</li>
      <li>Late layers refine toward the final answer</li>
      <li>The "correct" prediction often emerges gradually, not suddenly</li>
    </ul>

    <h3>3. What Does Latent Language Reveal?</h3>
    <p>
      When applying logit lens to multilingual models processing non-English text, a striking pattern emerges:
      regardless of input language, intermediate representations often decode to English tokens.
    </p>

    <p>
      This suggests models may use English as an internal "lingua franca"&mdash;even when input and output are in
      another language, the intermediate computation happens in an English-like representation space.
    </p>

    <p>
      <strong>Implications:</strong>
    </p>
    <ul>
      <li>Interpretability tools developed on English may apply to other languages</li>
      <li>Models may not process all languages equally</li>
      <li>Internal representations are not simple copies of input/output</li>
    </ul>

    <h3>4. Tuned Lens: Beyond Raw Projection</h3>
    <p>
      The basic logit lens assumes we can directly project intermediate activations with the unembedding matrix.
      But representations at early layers may not be in the same "format" as final-layer representations.
    </p>

    <p>
      The <strong>tuned lens</strong> learns a small affine transformation at each layer:
    </p>

    <div class="diagram">
      tuned_logits = W<sub>U</sub> &middot; (A<sub>L</sub> x<sub>L</sub> + b<sub>L</sub>)
    </div>

    <p>
      This accounts for layer-specific representation formats, often yielding more interpretable intermediate predictions.
    </p>
  </section>

  <section id="code-exercise">
    <h2>Code Exercise</h2>
    <p>
      This week's exercise provides hands-on experience with the core concepts:
    </p>
    <ul>
      <li>Load a transformer language model and examine its architecture</li>
      <li>Extract activation vectors at different layers</li>
      <li>Implement the logit lens to decode intermediate predictions</li>
      <li>Visualize how predictions evolve across layers</li>
      <li>Compare logit lens results across different prompts</li>
    </ul>

    <a href="https://colab.research.google.com/drive/PLACEHOLDER" target="_blank" class="colab-button">
      Open Exercise in Google Colab
    </a>

    <p style="margin-top: 10px; color: #666; font-size: 0.9em;">
      <em>Note: The Colab notebook will be created and linked here.</em>
    </p>
  </section>

  <section id="milestone">
    <h2>Project Milestone</h2>

    <div class="assignment-box">
      <p><strong>Due: Thursday of Week 1</strong></p>
      <p>
        This week focuses on getting oriented with the tools and selecting your concept for the semester project.
      </p>

      <h4>Deliverables:</h4>
      <ul>
        <li><strong>Concept pitches:</strong> Prepare 2-3 candidate concepts from your domain expertise</li>
        <li><strong>Literature review:</strong> Find 3-5 papers relevant to your concepts (not necessarily interpretability papers)</li>
        <li><strong>Initial exploration:</strong> Apply logit lens to prompts related to your concepts
          <ul>
            <li>Do the intermediate predictions seem related to your concept?</li>
            <li>At which layers does relevant information appear?</li>
          </ul>
        </li>
      </ul>

      <p><em>
        Tip: Choose concepts that can be expressed in language and that the model might reasonably "know about."
        Abstract concepts from your field (legal reasoning, medical diagnosis, literary themes) are great candidates.
      </em></p>
    </div>
  </section>

  <footer style="margin-top: 60px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">
    <p><a href="index.html">&larr; Back to Course Home</a></p>
  </footer>

</body>

</html>
