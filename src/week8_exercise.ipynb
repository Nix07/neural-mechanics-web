{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Causal Abstraction and Interchange Intervention Analysis\n",
    "\n",
    "This notebook provides hands-on exercises for understanding and applying causal abstraction, Interchange Intervention Analysis (IIA), and Distributed Alignment Search (DAS) to validate interpretability findings.\n",
    "\n",
    "**Learning objectives:**\n",
    "1. Build causal models for algorithmic and linguistic tasks\n",
    "2. Implement Interchange Intervention Analysis (IIA)\n",
    "3. Compare IIA with simple activation patching\n",
    "4. Apply IIA to validate circuit hypotheses\n",
    "5. Implement Distributed Alignment Search (DAS)\n",
    "6. Validate SAE features using causal abstraction\n",
    "7. Test probe findings with intervention analysis\n",
    "\n",
    "We'll work through two main examples:\n",
    "- **Addition** (Parts 1-3): Simple algorithmic task to build intuition\n",
    "- **Subject-verb agreement** (Parts 4-7): Real NLP task demonstrating full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Causal Model for Addition\n",
    "\n",
    "We'll start with a simple task: computing `(a + b) + c` for three numbers.\n",
    "\n",
    "**High-level causal model:**\n",
    "```\n",
    "Input₁, Input₂ → S₁ = Input₁ + Input₂\n",
    "S₁, Input₃ → S₂ = S₁ + Input₃\n",
    "S₂ → Output\n",
    "```\n",
    "\n",
    "**Exercise 1.1:** Implement the high-level causal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AdditionCausalModel:\n",
    "    \"\"\"High-level causal model for (a + b) + c.\"\"\"\n",
    "    input1: int\n",
    "    input2: int\n",
    "    input3: int\n",
    "    \n",
    "    def compute_s1(self) -> int:\n",
    "        \"\"\"First intermediate sum: S₁ = Input₁ + Input₂.\"\"\"\n",
    "        # TODO: Implement\n",
    "        raise NotImplementedError(\"Implement computation of S₁\")\n",
    "    \n",
    "    def compute_s2(self, s1: int) -> int:\n",
    "        \"\"\"Second intermediate sum: S₂ = S₁ + Input₃.\"\"\"\n",
    "        # TODO: Implement\n",
    "        raise NotImplementedError(\"Implement computation of S₂\")\n",
    "    \n",
    "    def forward(self) -> Dict[str, int]:\n",
    "        \"\"\"Run the full causal model, returning all intermediate variables.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # Should return {'S1': ..., 'S2': ..., 'Output': ...}\n",
    "        raise NotImplementedError(\"Implement forward pass\")\n",
    "\n",
    "# Test your implementation\n",
    "model = AdditionCausalModel(input1=5, input2=3, input3=2)\n",
    "result = model.forward()\n",
    "print(f\"Causal model result: {result}\")\n",
    "assert result['Output'] == 10, \"Expected (5 + 3) + 2 = 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training a Neural Network for Addition\n",
    "\n",
    "Now we'll train a small neural network to perform this addition task, and identify which layer might correspond to the intermediate variable S₁.\n",
    "\n",
    "**Exercise 2.1:** Implement and train a simple feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionNetwork(nn.Module):\n",
    "    \"\"\"Neural network for computing (a + b) + c.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # Input: 3 numbers, Output: 1 number\n",
    "        self.layer1 = nn.Linear(3, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)  # Candidate for S₁\n",
    "        self.layer4 = nn.Linear(hidden_size, 1)\n",
    "        self.activations = {}  # Store intermediate activations\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.layer1(x))\n",
    "        self.activations['L1'] = h1\n",
    "        \n",
    "        h2 = torch.relu(self.layer2(h1))\n",
    "        self.activations['L2'] = h2\n",
    "        \n",
    "        h3 = torch.relu(self.layer3(h2))\n",
    "        self.activations['L3'] = h3  # Hypothesize this represents S₁\n",
    "        \n",
    "        output = self.layer4(h3)\n",
    "        self.activations['Output'] = output\n",
    "        \n",
    "        return output\n",
    "\n",
    "def generate_addition_dataset(n_samples=1000, max_val=20):\n",
    "    \"\"\"Generate random addition problems.\"\"\"\n",
    "    # TODO: Generate random (a, b, c) and compute (a + b) + c\n",
    "    raise NotImplementedError(\"Implement dataset generation\")\n",
    "\n",
    "def train_addition_network(model, X_train, y_train, epochs=100, lr=0.001):\n",
    "    \"\"\"Train the addition network.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Implement training loop\n",
    "        raise NotImplementedError(\"Implement training loop\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Generate data and train\n",
    "# X_train, y_train, X_test, y_test = generate_addition_dataset()\n",
    "# net = AdditionNetwork()\n",
    "# net = train_addition_network(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing Interchange Intervention Analysis (IIA)\n",
    "\n",
    "Now we test whether layer L3 truly represents the intermediate sum S₁ using IIA.\n",
    "\n",
    "**IIA Procedure:**\n",
    "1. Run base input through high-level model → get S₁(base)\n",
    "2. Run source input through high-level model → get S₁(source)\n",
    "3. Run base input through neural network → get h₃(base)\n",
    "4. Run source input through neural network → get h₃(source)\n",
    "5. **Intervene:** Replace h₃(base) with h₃(source) in base run\n",
    "6. **Check:** Does the output match what the high-level model predicts when S₁ is intervened?\n",
    "\n",
    "**Exercise 3.1:** Implement IIA for the addition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interchange_intervention(\n",
    "    neural_net: nn.Module,\n",
    "    causal_model_class: type,\n",
    "    base_input: Tuple[int, int, int],\n",
    "    source_input: Tuple[int, int, int],\n",
    "    intervention_var: str = 'S1',  # Variable to intervene on\n",
    "    alignment_layer: str = 'L3'     # Neural layer aligned to that variable\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Perform Interchange Intervention Analysis.\n",
    "    \n",
    "    Returns:\n",
    "        - 'iia_output': Neural network output after intervention\n",
    "        - 'expected_output': What causal model predicts\n",
    "        - 'error': Absolute difference\n",
    "        - 'base_output': Original neural network output (no intervention)\n",
    "    \"\"\"\n",
    "    # TODO: Implement IIA\n",
    "    # 1. Get S₁(source) from high-level causal model\n",
    "    # 2. Get h₃(base) and h₃(source) from neural network\n",
    "    # 3. Intervene: replace h₃(base) with h₃(source), continue forward pass\n",
    "    # 4. Compare with expected output from causal model with intervened S₁\n",
    "    raise NotImplementedError(\"Implement IIA\")\n",
    "\n",
    "# Test IIA\n",
    "# base = (5, 3, 2)    # (5+3)+2 = 10\n",
    "# source = (7, 1, 2)  # (7+1)+2 = 10, but S₁ differs: 8 vs 8\n",
    "# result = interchange_intervention(net, AdditionCausalModel, base, source)\n",
    "# print(f\"IIA result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:** Compare IIA with simple activation patching.\n",
    "\n",
    "Simple patching just replaces activations without checking if the intervention matches a causal model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_activation_patching(\n",
    "    neural_net: nn.Module,\n",
    "    base_input: Tuple[int, int, int],\n",
    "    source_input: Tuple[int, int, int],\n",
    "    patch_layer: str = 'L3'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Simple activation patching (no causal model).\n",
    "    Just patches and sees what happens.\n",
    "    \"\"\"\n",
    "    # TODO: Implement simple patching\n",
    "    # 1. Run base input, store activations\n",
    "    # 2. Run source input, get activation at patch_layer\n",
    "    # 3. Re-run base with patched activation\n",
    "    # 4. Return output change\n",
    "    raise NotImplementedError(\"Implement simple patching\")\n",
    "\n",
    "# Compare both methods\n",
    "# iia_res = interchange_intervention(net, AdditionCausalModel, (5,3,2), (7,1,2))\n",
    "# patch_res = simple_activation_patching(net, (5,3,2), (7,1,2))\n",
    "# print(f\"IIA error: {iia_res['error']:.4f}\")\n",
    "# print(f\"Patch output change: {patch_res['output_change']:.4f}\")\n",
    "# print(\"IIA provides interpretable error w.r.t. causal model, patching just shows effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Subject-Verb Agreement - Building the Causal Model\n",
    "\n",
    "Now we move to a real NLP task: subject-verb agreement in sentences like:\n",
    "- \"The **key** to the cabinets **is** on the table.\"\n",
    "\n",
    "**High-level causal model:**\n",
    "```\n",
    "Subject_Number (singular/plural) → Verb_Prediction (is/are)\n",
    "```\n",
    "\n",
    "Distractors (\"cabinets\") should NOT affect the prediction.\n",
    "\n",
    "**Exercise 4.1:** Implement the linguistic causal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SubjectVerbCausalModel:\n",
    "    \"\"\"High-level causal model for subject-verb agreement.\"\"\"\n",
    "    subject_number: str  # 'singular' or 'plural'\n",
    "    \n",
    "    def predict_verb(self) -> str:\n",
    "        \"\"\"Predict correct verb form based on subject number.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # 'singular' → 'is', 'plural' → 'are'\n",
    "        raise NotImplementedError(\"Implement verb prediction\")\n",
    "\n",
    "def generate_agreement_sentences(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate subject-verb agreement sentences.\n",
    "    \n",
    "    Template: \"The [SUBJECT] to the [DISTRACTOR] [VERB] on the table.\"\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with keys: 'sentence', 'subject_number', 'correct_verb'\n",
    "    \"\"\"\n",
    "    subjects = {\n",
    "        'singular': ['key', 'book', 'cat', 'student', 'teacher'],\n",
    "        'plural': ['keys', 'books', 'cats', 'students', 'teachers']\n",
    "    }\n",
    "    distractors = {\n",
    "        'singular': ['cabinet', 'shelf', 'box', 'room', 'desk'],\n",
    "        'plural': ['cabinets', 'shelves', 'boxes', 'rooms', 'desks']\n",
    "    }\n",
    "    \n",
    "    # TODO: Generate sentences with varying subject/distractor combinations\n",
    "    # Critical: Include cases where distractor has OPPOSITE number from subject\n",
    "    raise NotImplementedError(\"Implement sentence generation\")\n",
    "\n",
    "# Generate dataset\n",
    "# sentences = generate_agreement_sentences()\n",
    "# print(f\"Example sentences:\")\n",
    "# for s in sentences[:5]:\n",
    "#     print(f\"  {s['sentence']} → {s['correct_verb']} (subject: {s['subject_number']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Loading GPT-2 and Extracting Activations\n",
    "\n",
    "We'll use a pre-trained GPT-2 model and extract activations from specific layers and attention heads.\n",
    "\n",
    "**Exercise 5.1:** Implement activation extraction with hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"Extract and store activations from specific layers/heads.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def register_hooks(self, layer_idx: int, head_indices: List[int] = None):\n",
    "        \"\"\"\n",
    "        Register hooks to extract activations from specified layer and heads.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Which transformer layer (0-11 for GPT-2)\n",
    "            head_indices: Which attention heads (None = all heads)\n",
    "        \"\"\"\n",
    "        def hook_fn(module, input, output):\n",
    "            # TODO: Store attention outputs\n",
    "            # output[0] has shape (batch, seq_len, n_heads, head_dim)\n",
    "            raise NotImplementedError(\"Implement hook function\")\n",
    "        \n",
    "        # Register hook on attention layer\n",
    "        layer = self.model.transformer.h[layer_idx].attn\n",
    "        hook = layer.register_forward_hook(hook_fn)\n",
    "        self.hooks.append(hook)\n",
    "    \n",
    "    def get_activation(self, sentence: str, position: int, layer: int, head: int):\n",
    "        \"\"\"\n",
    "        Get activation for a specific position/layer/head.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Input sentence\n",
    "            position: Token position (e.g., subject position)\n",
    "            layer: Layer index\n",
    "            head: Head index\n",
    "        \"\"\"\n",
    "        # TODO: Tokenize, run model, extract activation\n",
    "        raise NotImplementedError(\"Implement activation extraction\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "# Test extraction\n",
    "# extractor = ActivationExtractor(model)\n",
    "# extractor.register_hooks(layer_idx=5, head_indices=[2, 7, 11])\n",
    "# act = extractor.get_activation(\"The key to the cabinets is\", position=1, layer=5, head=2)\n",
    "# print(f\"Activation shape: {act.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Implementing IIA for Subject-Verb Agreement\n",
    "\n",
    "Now we test whether specific attention heads (e.g., Layer 5, Heads {2, 7, 11}) track subject number.\n",
    "\n",
    "**IIA for NLP:**\n",
    "1. Base sentence: \"The **key** to the cabinets is\"\n",
    "2. Source sentence: \"The **keys** to the cabinet are\" (swapped number)\n",
    "3. Extract activations from hypothesized heads\n",
    "4. Intervene: Replace base head activations with source head activations\n",
    "5. Check: Does the model now predict \"are\" instead of \"is\"?\n",
    "\n",
    "**Exercise 6.1:** Implement IIA for linguistic task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_iia(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    base_sentence: str,\n",
    "    source_sentence: str,\n",
    "    intervention_layer: int,\n",
    "    intervention_heads: List[int],\n",
    "    subject_position: int\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Perform IIA for subject-verb agreement.\n",
    "    \n",
    "    Returns:\n",
    "        - 'base_prediction': Verb predicted without intervention\n",
    "        - 'intervened_prediction': Verb predicted after intervention\n",
    "        - 'expected_change': Whether intervention should change prediction\n",
    "        - 'intervention_success': Whether prediction changed as expected\n",
    "    \"\"\"\n",
    "    # TODO: Implement linguistic IIA\n",
    "    # 1. Get activations from base and source sentences\n",
    "    # 2. Intervene on specified heads at subject position\n",
    "    # 3. Check if verb prediction changes appropriately\n",
    "    raise NotImplementedError(\"Implement linguistic IIA\")\n",
    "\n",
    "# Test IIA on example sentence pair\n",
    "# base = \"The key to the cabinets\"\n",
    "# source = \"The keys to the cabinet\"  # Subject number flipped\n",
    "# result = linguistic_iia(\n",
    "#     model, tokenizer, base, source,\n",
    "#     intervention_layer=5,\n",
    "#     intervention_heads=[2, 7, 11],\n",
    "#     subject_position=1  # Position of \"key\"/\"keys\"\n",
    "# )\n",
    "# print(f\"IIA result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.2:** Compute IIA accuracy across many sentence pairs.\n",
    "\n",
    "A good alignment should have high IIA accuracy: interventions should consistently produce the expected changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_iia_accuracy(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    sentence_pairs: List[Tuple[str, str]],\n",
    "    layer: int,\n",
    "    heads: List[int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate IIA accuracy over many intervention pairs.\n",
    "    \n",
    "    Returns:\n",
    "        Fraction of interventions that produce expected change\n",
    "    \"\"\"\n",
    "    # TODO: Run IIA on all sentence pairs, compute accuracy\n",
    "    raise NotImplementedError(\"Implement IIA evaluation\")\n",
    "\n",
    "# Generate pairs and evaluate\n",
    "# pairs = [(s1['sentence'], s2['sentence']) \n",
    "#          for s1, s2 in zip(sentences[::2], sentences[1::2])\n",
    "#          if s1['subject_number'] != s2['subject_number']]\n",
    "# accuracy = evaluate_iia_accuracy(model, tokenizer, pairs, layer=5, heads=[2,7,11])\n",
    "# print(f\"IIA accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Distributed Alignment Search (DAS)\n",
    "\n",
    "Instead of manually hypothesizing which heads track subject number, we can search automatically.\n",
    "\n",
    "**DAS Algorithm:**\n",
    "1. Start with empty set of components\n",
    "2. For each component (layer/head):\n",
    "   - Add it to the set\n",
    "   - Compute IIA accuracy\n",
    "   - Keep it if accuracy improves\n",
    "3. Return best-performing set\n",
    "\n",
    "**Exercise 7.1:** Implement greedy DAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_alignment_search(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    sentence_pairs: List[Tuple[str, str]],\n",
    "    max_components: int = 5,\n",
    "    n_layers: int = 12,\n",
    "    n_heads: int = 12\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Greedy search for components aligned to subject number.\n",
    "    \n",
    "    Returns:\n",
    "        - 'components': List of (layer, head) tuples\n",
    "        - 'accuracy': IIA accuracy with these components\n",
    "        - 'search_history': Accuracy at each step\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    best_accuracy = 0.0\n",
    "    history = []\n",
    "    \n",
    "    # TODO: Implement greedy search\n",
    "    # For each potential component:\n",
    "    #   1. Try adding it to selected set\n",
    "    #   2. Compute IIA accuracy\n",
    "    #   3. Keep if it improves accuracy\n",
    "    # Stop after max_components or when no improvement\n",
    "    raise NotImplementedError(\"Implement DAS\")\n",
    "    \n",
    "    return {\n",
    "        'components': selected,\n",
    "        'accuracy': best_accuracy,\n",
    "        'search_history': history\n",
    "    }\n",
    "\n",
    "# Run DAS\n",
    "# das_result = distributed_alignment_search(model, tokenizer, pairs, max_components=5)\n",
    "# print(f\"DAS found components: {das_result['components']}\")\n",
    "# print(f\"Final IIA accuracy: {das_result['accuracy']:.2%}\")\n",
    "# \n",
    "# # Plot search progress\n",
    "# plt.plot(das_result['search_history'])\n",
    "# plt.xlabel('Component added')\n",
    "# plt.ylabel('IIA accuracy')\n",
    "# plt.title('DAS Search Progress')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Validating Circuit Findings from Week 5\n",
    "\n",
    "Now let's use IIA to validate a circuit hypothesis from Week 5 (e.g., induction head circuit).\n",
    "\n",
    "**Exercise 8.1:** Test whether identified induction heads truly perform the induction operation using causal abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_induction_circuit(\n",
    "    model: GPT2LMHeadModel,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    induction_heads: List[Tuple[int, int]],  # (layer, head) pairs\n",
    "    n_tests: int = 20\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Validate induction head circuit using IIA.\n",
    "    \n",
    "    Induction task: \"A B ... A\" → predict B\n",
    "    \n",
    "    Causal model:\n",
    "        Previous_Token_Match → Next_Token_Prediction\n",
    "    \n",
    "    Returns:\n",
    "        IIA accuracy for induction heads\n",
    "    \"\"\"\n",
    "    # TODO: Implement IIA for induction\n",
    "    # 1. Generate base: \"cat dog ... cat\"\n",
    "    # 2. Generate source: \"bird fish ... bird\"\n",
    "    # 3. Intervene on induction heads\n",
    "    # 4. Check if prediction changes from \"dog\" to \"fish\"\n",
    "    raise NotImplementedError(\"Implement induction circuit validation\")\n",
    "\n",
    "# Test induction heads (example coordinates)\n",
    "# induction_heads = [(5, 1), (5, 5), (6, 9)]  # Hypothetical\n",
    "# result = validate_induction_circuit(model, tokenizer, induction_heads)\n",
    "# print(f\"Induction circuit IIA accuracy: {result['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Validating SAE Features from Week 7\n",
    "\n",
    "We can use causal abstraction to test whether an SAE feature causally represents a concept.\n",
    "\n",
    "**Exercise 9.1:** Validate an SAE feature using intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sae_feature(\n",
    "    model: GPT2LMHeadModel,\n",
    "    sae_encoder: nn.Module,  # Trained SAE from Week 7\n",
    "    sae_decoder: nn.Module,\n",
    "    feature_idx: int,\n",
    "    concept_examples: List[Tuple[str, str]],  # (has_concept, lacks_concept) pairs\n",
    "    layer: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test if SAE feature causally represents a concept.\n",
    "    \n",
    "    Causal model:\n",
    "        Concept_Present → Feature_Active → Behavior_Change\n",
    "    \n",
    "    Procedure:\n",
    "        1. Find examples where feature activates (concept present)\n",
    "        2. Find examples where feature doesn't activate (concept absent)\n",
    "        3. Intervene: Set feature activation to opposite value\n",
    "        4. Check if behavior changes as expected\n",
    "    \n",
    "    Returns:\n",
    "        Causal effect strength (how much intervention changes output)\n",
    "    \"\"\"\n",
    "    # TODO: Implement SAE feature validation\n",
    "    # 1. Encode activations to get SAE features\n",
    "    # 2. Intervene on specific feature (set to 0 or mean value)\n",
    "    # 3. Decode back to activation space\n",
    "    # 4. Continue model forward pass\n",
    "    # 5. Measure output change\n",
    "    raise NotImplementedError(\"Implement SAE feature validation\")\n",
    "\n",
    "# Example: Validate a \"plural subject\" feature\n",
    "# sae_encoder = ...  # Load from Week 7\n",
    "# sae_decoder = ...\n",
    "# plural_examples = [\n",
    "#     (\"The keys are\", \"The key is\"),  # plural vs singular\n",
    "#     (\"The books are\", \"The book is\"),\n",
    "# ]\n",
    "# result = validate_sae_feature(model, sae_encoder, sae_decoder, \n",
    "#                               feature_idx=42, concept_examples=plural_examples, layer=5)\n",
    "# print(f\"Causal effect of feature 42: {result['causal_effect']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Project Template - Validating Your Concept\n",
    "\n",
    "Now apply these techniques to validate your own research project from previous weeks.\n",
    "\n",
    "**Project Workflow:**\n",
    "1. Define your concept's causal model\n",
    "2. Identify candidate neural components (circuits, SAE features, or probe-identified layers)\n",
    "3. Implement IIA to test alignment\n",
    "4. Use DAS if you need to search for components\n",
    "5. Report IIA accuracy and causal effect sizes\n",
    "6. Make rigorous claims backed by intervention evidence\n",
    "\n",
    "**Exercise 10.1:** Fill in this template for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PROJECT TEMPLATE ==========\n",
    "\n",
    "# 1. Define your concept and causal model\n",
    "MY_CONCEPT = \"[Your concept here, e.g., 'temporal reasoning', 'politeness', 'causality']\"\n",
    "\n",
    "@dataclass\n",
    "class MyConceptCausalModel:\n",
    "    \"\"\"High-level causal model for [YOUR CONCEPT].\"\"\"\n",
    "    # TODO: Define variables in your causal model\n",
    "    pass\n",
    "\n",
    "# 2. Generate test examples\n",
    "def generate_concept_examples():\n",
    "    \"\"\"Generate sentence pairs that differ in your concept.\"\"\"\n",
    "    # TODO: Create examples where your concept varies\n",
    "    # Return list of (base, source) pairs\n",
    "    raise NotImplementedError(\"Generate examples for your concept\")\n",
    "\n",
    "# 3. Identify candidate components\n",
    "# From Week 5 (circuits), Week 6 (probes), or Week 7 (SAE features)\n",
    "CANDIDATE_COMPONENTS = [\n",
    "    # TODO: List (layer, head) or (layer, neuron) or (layer, sae_feature)\n",
    "]\n",
    "\n",
    "# 4. Run IIA validation\n",
    "def validate_my_concept(\n",
    "    model: GPT2LMHeadModel,\n",
    "    components: List[Tuple[int, int]],\n",
    "    examples: List[Tuple[str, str]]\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"Validate that components represent your concept.\"\"\"\n",
    "    # TODO: Implement IIA for your concept\n",
    "    # 1. For each example pair:\n",
    "    #    - Extract activations from components\n",
    "    #    - Intervene on components\n",
    "    #    - Check if output changes as causal model predicts\n",
    "    # 2. Compute IIA accuracy\n",
    "    # 3. Compute effect sizes\n",
    "    raise NotImplementedError(\"Implement validation for your concept\")\n",
    "\n",
    "# 5. Run DAS if needed\n",
    "def search_for_concept_components(model, examples):\n",
    "    \"\"\"Search for components aligned to your concept.\"\"\"\n",
    "    # TODO: Run DAS to find best components\n",
    "    raise NotImplementedError(\"Run DAS for your concept\")\n",
    "\n",
    "# 6. Report results\n",
    "# examples = generate_concept_examples()\n",
    "# validation_results = validate_my_concept(model, CANDIDATE_COMPONENTS, examples)\n",
    "# \n",
    "# print(f\"Concept: {MY_CONCEPT}\")\n",
    "# print(f\"IIA Accuracy: {validation_results['accuracy']:.2%}\")\n",
    "# print(f\"Average Causal Effect: {validation_results['avg_effect']:.3f}\")\n",
    "# print(f\"\\nConclusion: These components {'DO' if validation_results['accuracy'] > 0.8 else 'DO NOT'} \")\n",
    "# print(f\"            causally represent {MY_CONCEPT} (IIA accuracy: {validation_results['accuracy']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Research Guidelines\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Causal abstraction** formalizes the relationship between high-level concepts and neural mechanisms\n",
    "2. **IIA** provides quantitative validation of interpretability claims (vs. qualitative inspection)\n",
    "3. **DAS** automates the search for components aligned to concepts\n",
    "4. **Integration:** Use IIA to validate findings from circuits (Week 5), probes (Week 6), and SAEs (Week 7)\n",
    "\n",
    "**For Your Research Paper:**\n",
    "\n",
    "✅ **DO:**\n",
    "- Define explicit causal models for your concept\n",
    "- Report IIA accuracy and effect sizes\n",
    "- Compare multiple alignment hypotheses\n",
    "- Use intervention controls (e.g., unrelated components)\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Make causal claims based only on correlation (e.g., probe accuracy)\n",
    "- Cherry-pick examples without systematic evaluation\n",
    "- Ignore cases where interventions fail\n",
    "\n",
    "**Next Steps:**\n",
    "1. Apply these techniques to your project\n",
    "2. Iterate between exploratory methods (Week 5-7) and validation (Week 8)\n",
    "3. Build rigorous evidence for NeurIPS submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
