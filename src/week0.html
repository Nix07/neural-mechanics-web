<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Week 0: Introduction & Course Overview</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      border-bottom: 3px solid #3498db;
      padding-bottom: 10px;
    }

    h2 {
      color: #34495e;
      margin-top: 30px;
      border-bottom: 2px solid #bdc3c7;
      padding-bottom: 5px;
    }

    h3 {
      color: #555;
      margin-top: 20px;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .info-box {
      background-color: #e8f4f8;
      border-left: 4px solid #3498db;
      padding: 15px;
      margin: 20px 0;
    }

    .example-box {
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      border-radius: 5px;
      padding: 15px;
      margin: 20px 0;
    }

    .discussion-box {
      background-color: #fff9e6;
      border-left: 4px solid #f39c12;
      padding: 15px;
      margin: 20px 0;
    }

    .assignment-box {
      background-color: #e8f8f5;
      border-left: 4px solid #27ae60;
      padding: 15px;
      margin: 20px 0;
    }

    ul {
      margin: 10px 0;
    }

    li {
      margin: 8px 0;
    }

    code {
      background-color: #f4f4f4;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
    }

    .objectives {
      background-color: #f0f7ff;
      padding: 15px;
      border-radius: 5px;
      margin: 20px 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }

    th,
    td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }
  </style>
</head>

<body>
  <h1>Week 0: Introduction & Course Overview</h1>

  <div class="info-box">
    <p><strong>Start here:</strong> <a href="week0_welcome.html">Prologue: At the Edge of Understanding</a></p>
    <p>An essay on why understanding AI matters and what neural mechanics can teach us about the future of human knowledge.</p>
    <p><strong>Also see:</strong> <a href="week0_notes.html">Lecture Notes</a> | <a href="week0_research.html">Finding a Good Research Question</a></p>
  </div>

  <div class="objectives">
    <h3>Learning Objectives</h3>
    <ul>
      <li>Understand course structure, expectations, and grading</li>
      <li>Grasp basic neural network concepts (accessible to non-ML students)</li>
      <li>Appreciate the historical surprise and significance of in-context learning</li>
      <li>See concrete examples of interpretability discoveries from early work</li>
      <li>Understand function vector results at a high level (methods come later)</li>
      <li>Identify characteristics of successful interdisciplinary interpretability projects</li>
      <li>Form teams and propose a concept to study</li>
    </ul>
  </div>

  <h2>1. Course Logistics</h2>

  <h3>Course Overview</h3>
  <p>
    <strong>Goal:</strong> Study how large language models encode non-CS concepts through interpretability research,
    culminating in papers suitable for submission to NeurIPS or similar venues.
  </p>

  <p>
    <strong>Team Structure:</strong> Interdisciplinary teams of ~3 students:
  </p>
  <ul>
    <li><strong>Non-CS PhD student:</strong> Domain expertise in your field (biology, linguistics, psychology, physics,
      law, etc.)</li>
    <li><strong>CS/ML PhD student:</strong> Technical ML background, PyTorch experience</li>
    <li><strong>BauLab member:</strong> Expert in interpretability methods, will guide technical implementation</li>
  </ul>

  <h3>Course Structure</h3>
  <table>
    <tr>
      <th>Weeks</th>
      <th>Focus</th>
    </tr>
    <tr>
      <td>0-1</td>
      <td>Foundations: Course setup, benchmarking concepts</td>
    </tr>
    <tr>
      <td>2-8</td>
      <td>Methods: Core interpretability techniques (steering, circuits, probes, SAEs, validation)</td>
    </tr>
    <tr>
      <td>9-11</td>
      <td>Advanced topics: Attribution, skepticism, and research best practices</td>
    </tr>
    <tr>
      <td>12</td>
      <td>Final presentations</td>
    </tr>
  </table>

  <h3>Grading & Expectations</h3>
  <div class="info-box">
    <p><strong>This is a research course, not a traditional class.</strong></p>
    <p>Grading will be based on:</p>
    <ul>
      <li><strong>Weekly exercises:</strong> Hands-on implementation of interpretability methods (30%)</li>
      <li><strong>Project milestones:</strong> Proposal, preliminary results, draft figures (30%)</li>
      <li><strong>Final deliverable:</strong> Research paper draft + presentation (40%)</li>
    </ul>
    <p><em>Success = rigorous investigation of your concept, not necessarily positive results. Negative results with
        careful validation are publishable!</em></p>
  </div>

  <h2>2. The Central Mystery: Are Concepts Different from Words?</h2>

  <p>
    Large language models process text, but do they think in words? This course begins with a provocation: the internal representations that drive model behavior may be fundamentally different from the tokens that flow in and out.
  </p>

  <p>
    Words are surface phenomena. They vary across languages, shift meaning with context, and often fail to capture the abstractions that underlie coherent reasoning. <strong>Concepts</strong>, by contrast, are the invariants&mdash;the stable structures that persist across linguistic transformations. This course asks: can we find these invariants inside neural networks?
  </p>

  <h3>The Gap Between Knowledge and Expression</h3>
  <p>
    What an AI knows is not always what it says. This gap between internal representation and external behavior has become starkly visible in recent work.
  </p>

  <p>
    <strong>Rager et al. (2025)</strong> study DeepSeek models, revealing a striking case of censorship mechanics. The models demonstrably possess knowledge about sensitive topics&mdash;their internal representations encode the relevant concepts&mdash;yet they refuse to express this knowledge in their outputs. The information exists inside the model; the suppression is a separate mechanism layered on top.
  </p>

  <p>
    This dissociation between knowing and saying points to a general phenomenon: models may have internal states that diverge systematically from their outputs. If we want to understand what models actually believe, know, or intend, we cannot rely solely on their words. <strong>We must look inside.</strong>
  </p>

  <h3>Concepts as Invariants</h3>
  <p>
    The emerging picture suggests that concepts are invariants of the system&mdash;internal structures whose functional roles remain unchanged under many transformations. Just as a physical law remains valid regardless of the coordinate system used to express it, a neural concept may persist across languages, phrasings, and contexts.
  </p>

  <p>Examples of concepts that appear to have this invariant character:</p>
  <ul>
    <li><strong>Truthfulness concepts:</strong> Directions in activation space corresponding to whether a statement is true or false, independent of topic or phrasing</li>
    <li><strong>Task concepts:</strong> The abstract notion of "translate" or "summarize" as distinct from any particular instance</li>
    <li><strong>User concepts:</strong> Representations of who the model is talking to, tracking properties across a conversation</li>
    <li><strong>Domain concepts:</strong> Abstract structures from law, medicine, physics, music&mdash;the focus of this course</li>
  </ul>

  <p>
    These are not isolated neurons but distributed patterns of activation. They are not explicitly labeled in the training data but emerge from the structure of the task. And they are not words&mdash;they are what the words point to.
  </p>

  <h2>3. The Emergence of Metareasoning</h2>

  <h3>In-Context Learning: A Form of Reasoning About Reasoning</h3>
  <p>
    In 2020, GPT-3 demonstrated <strong>in-context learning</strong> (ICL): the ability to learn
    new tasks from a few examples in the prompt, <em>without any gradient updates</em>.
  </p>

  <div class="example-box">
    <p><strong>Example: Translation without training</strong></p>
    <p>
      <strong>Prompt:</strong><br>
      English: Hello &rarr; French: Bonjour<br>
      English: Goodbye &rarr; French: Au revoir<br>
      English: Thank you &rarr; French: <em>[model generates: Merci]</em>
    </p>
  </div>

  <p>
    This is a form of <strong>metareasoning</strong>&mdash;the model is not just processing text but reasoning about how to reason. Where does this capability come from? It is not explicitly programmed. It emerges from training on next-token prediction, yet manifests as something that looks remarkably like flexible cognition.
  </p>

  <h3>Induction Heads: A Circuit That Implements ICL</h3>
  <p>
    Among the most thoroughly analyzed circuits in transformers are <strong>induction heads</strong>: attention patterns that implement a simple but powerful operation of matching previous tokens and copying what came next. When you see "Harry Potter" in context and later encounter "Harry," an induction head helps the model predict "Potter."
  </p>

  <p>
    This mechanism underlies much of in-context learning. Research has characterized induction heads in detail, revealing that they operate across multiple pathways and&mdash;crucially&mdash;that the relevant internal representations are often <strong>language-independent</strong>.
  </p>

  <div class="info-box">
    <p><strong>A Key Finding:</strong> A concept that emerges in English contexts may be the <em>same internal structure</em> that handles the analogous German or Chinese context. The representations remain stable under transformations that change everything about the tokens while preserving the underlying meaning.</p>
  </div>

  <p>
    This reinforces the central thesis: concepts inside LLMs are not words. They are abstract structures&mdash;stable under linguistic transformations&mdash;that we can study, characterize, and manipulate.
  </p>

  <h3>Why This Matters</h3>
  <p>
    Understanding how emergent capabilities like ICL are implemented&mdash;not just that they exist, but how they work mechanistically&mdash;is one of the core challenges that motivates interpretability research. If we can identify the internal representations that causally mediate specific behaviors, we gain both scientific understanding and practical control.
  </p>

  <h2>4. Causal Mediation: Finding the Neurons That Matter</h2>

  <p>
    How do we identify which internal components are responsible for specific behaviors? The methodology is called <strong>causal mediation analysis</strong>: intervening on internal components to establish their causal role in the model's outputs.
  </p>

  <h3>Early Success: The Lamp-Controlling Neuron</h3>
  <p>
    Early work on GANs (Generative Adversarial Networks) provides an instructive template. Researchers found individual neurons that controlled specific scene properties: a single neuron might control whether lights are on or off.
  </p>

  <div class="example-box">
    <p><strong>Experiment (Bau et al., 2019):</strong></p>
    <ol>
      <li>Generate a bedroom image (no lamps)</li>
      <li>Manually increase activation of "lamp neuron"</li>
      <li>Result: Lamps appear in the image!</li>
      <li>Decrease activation: Lamps disappear</li>
    </ol>
    <p><em>This is causal mediation: intervening on internal components establishes their causal role in the outputs.</em></p>
  </div>

  <h3>Key Research Threads in LLMs</h3>
  <p>
    The methodology generalizes from GANs to language models. Three lines of work illustrate the progress that has been made in localizing and characterizing neural concepts:
  </p>

  <p>
    <strong>Locating Factual Knowledge (Meng et al., 2022):</strong> The ROME paper demonstrated that factual associations in GPT-style models are localized in specific MLP layers. By applying <em>causal tracing</em>&mdash;corrupting inputs and restoring activations at targeted locations&mdash;researchers identified where facts like "The Eiffel Tower is in Paris" are stored. This established that factual knowledge has a discernible address inside the network.
  </p>

  <p>
    <strong>Locating Functions (Todd et al., 2024):</strong> This work extended localization from facts to functions. When a model performs in-context learning of an abstract task&mdash;translating to French, answering with antonyms, continuing a pattern&mdash;where is that function encoded? <em>Function vectors</em> show that task-level abstractions are localizable and, remarkably, transferable: a vector extracted from one context can induce the same function in another.
  </p>

  <p>
    <strong>Locating Mental State Representations (Prakash et al., 2025):</strong> This work examined how models represent and track the mental states of agents described in text&mdash;theory of mind. It shows that interpretability methods can reach beyond simple factual recall into the representation of genuinely abstract, cognitively rich concepts.
  </p>

  <div class="info-box">
    <p><strong>The Pattern:</strong> From facts to functions to mental states&mdash;each study finds that abstract concepts have neural correlates that can be identified, validated, and manipulated. The methods you will learn in this course are the tools for this kind of discovery.</p>
  </div>

  <h2>5. Toward an Atlas of Neural Concepts</h2>

  <p>
    If concepts inside LLMs are real, discoverable, and causally important, then we should map them systematically. The vision motivating this course is the creation of an <strong>atlas of neural concepts</strong>: a comprehensive characterization of the internal structures that drive model behavior across domains.
  </p>

  <h3>Why Build an Atlas?</h3>
  <p>
    Such an atlas would serve multiple purposes:
  </p>
  <ul>
    <li><strong>Scientific understanding:</strong> What are the basic building blocks of machine cognition?</li>
    <li><strong>Safety and alignment:</strong> Which concepts relate to honesty, deception, harm, and helpfulness?</li>
    <li><strong>Knowledge extraction:</strong> What has the model learned that humans do not yet know?</li>
    <li><strong>Control and steering:</strong> How can we intervene on concepts to shape behavior?</li>
  </ul>

  <p>
    Building this atlas requires methods. The works surveyed above&mdash;and the techniques you will learn throughout this course&mdash;constitute the toolkit for this cartographic project.
  </p>

  <h3>What Should the Atlas Contain?</h3>
  <p>
    This is an open question. Possibilities include:
  </p>
  <ul>
    <li><strong>Individual neurons:</strong> The simplest units, but concepts are rarely confined to single neurons</li>
    <li><strong>Directions in activation space:</strong> Linear combinations of neurons that represent concepts</li>
    <li><strong>Circuits:</strong> Collections of components that work together to implement functions</li>
    <li><strong>Functional roles:</strong> Abstract descriptions of what computations are performed where</li>
  </ul>

  <p>
    The right level of description likely depends on the question being asked. Your projects will contribute to answering this.
  </p>

  <div class="discussion-box">
    <h3>Discussion Questions</h3>
    <ol>
      <li><strong>What do interpretability methods have in common?</strong> Causal tracing, function vectors, circuit analysis&mdash;each uses different techniques. What makes something a "mechanistic interpretability" approach?</li>
      <li><strong>How do we know when we have found a concept?</strong> What criteria distinguish a genuine neural concept from a spurious correlation or an artifact of our analysis methods?</li>
      <li><strong>What concepts matter most?</strong> Given limited research resources, which concepts should we prioritize? What makes a concept important from a scientific perspective? From a safety perspective?</li>
      <li><strong>Can models describe their own concepts?</strong> If concepts are not words, can language models ever articulate what they internally represent? Or is there a fundamental gap between neural representation and linguistic expression?</li>
    </ol>
  </div>

  <h2>6. What Makes a Good Research Question?</h2>

  <p>
    Before you invest months of effort into a research project, you should ask: is this the right question? The FINER framework&mdash;adapted from clinical research methodology&mdash;offers useful criteria: a good research question should be <strong>Feasible, Interesting, Novel, Ethical, and Relevant</strong>.
  </p>

  <p>For a detailed discussion, see <a href="week0_research.html">Finding a Good Research Question</a>.</p>

  <h3>The FINER Framework for Interpretability</h3>

  <table>
    <tr>
      <th>Criterion</th>
      <th>Key Questions</th>
    </tr>
    <tr>
      <td><strong>Feasible</strong></td>
      <td>Can you find a model where the phenomenon appears and where you can see inside? Are there signs of life in the internal representations?</td>
    </tr>
    <tr>
      <td><strong>Interesting</strong></td>
      <td>Does this question excite you? Would your colleagues (in ML and in your domain) consider it important?</td>
    </tr>
    <tr>
      <td><strong>Novel</strong></td>
      <td>Has this been done before? Domain-specific concepts are vastly understudied&mdash;this is your opportunity.</td>
    </tr>
    <tr>
      <td><strong>Ethical</strong></td>
      <td>Would this work primarily enable harm? Interpretability tools can be misused.</td>
    </tr>
    <tr>
      <td><strong>Relevant</strong></td>
      <td>If you answer this question, what changes for the field? For the domains where models are deployed?</td>
    </tr>
  </table>

  <h3>The Interdisciplinary Advantage</h3>

  <p>
    LLMs are being deployed in medicine, law, education, scientific research, policy analysis. The highest-stakes applications are almost entirely outside computer science. Yet interpretability research remains focused on questions that matter primarily to ML researchers.
  </p>

  <p>
    This is the gap. This course is forming interdisciplinary teams precisely to address it. The interpretability community has strong intuitions about which questions matter for AI safety and for understanding deep learning. But we have weak intuitions about which questions matter for medicine, for law, for scientific discovery. <strong>Your non-CS collaborators have those intuitions.</strong>
  </p>

  <div class="info-box">
    <p>When the legal scholar on your team gets excited about a research direction, that signal contains information you could not generate yourself. When the biologist identifies a concept that would matter for scientific discovery, that is the kind of question no one else is asking.</p>
  </div>

  <h3>Example Project Ideas</h3>

  <div class="example-box">
    <p><strong>Biology:</strong> Evolutionary relationships, protein structure encoding, ecological relationships</p>
    <p><strong>Linguistics:</strong> Politeness strategies, evidentiality, discourse structure</p>
    <p><strong>Psychology:</strong> Theory of mind, temporal reasoning, causal attribution</p>
    <p><strong>Physics/Chemistry:</strong> Conservation laws, reaction mechanisms, spatial reasoning</p>
    <p><strong>Law:</strong> Legal precedent, burden of proof, jurisdiction</p>
    <p><strong>Music:</strong> Harmonic relationships, rhythmic patterns, musical form</p>
  </div>

  <h3>Group Discussion</h3>

  <div class="discussion-box">
    <p><strong>In your teams, discuss:</strong></p>
    <ol>
      <li>What is a fundamental concept in your domain that non-experts often misunderstand?</li>
      <li>How would you test if an LLM understands this concept?</li>
      <li>What would it mean to "localize" this concept in a neural network?</li>
      <li>What failure modes might be interesting? (When does the model get it wrong?)</li>
      <li>How could this research benefit your field or AI safety?</li>
    </ol>
  </div>

  <h2>7. Assignment: Team Formation & Project Proposal</h2>

  <div class="assignment-box">
    <h3>Assignment (Due: End of Week 1)</h3>

    <p><strong>Part 1: Form Teams</strong></p>
    <ul>
      <li>Groups of ~3 students (ideally: 1 non-CS PhD, 1 CS/ML PhD, 1 BauLab member)</li>
      <li>Use Canvas discussion board or class Slack to coordinate</li>
      <li>Consider complementary expertise (e.g., linguist + NLP researcher + interpretability expert)</li>
    </ul>

    <p><strong>Part 2: Write a Short Proposal (1-2 pages)</strong></p>

    <p>Your proposal should include:</p>

    <ol>
      <li><strong>Team Members & Expertise</strong>
        <ul>
          <li>Names, departments, relevant background</li>
        </ul>
      </li>

      <li><strong>Concept Description</strong>
        <ul>
          <li>What concept will you study? (1 paragraph)</li>
          <li>Why is it important in your domain?</li>
          <li>Why is it interesting for LLM interpretability?</li>
        </ul>
      </li>

      <li><strong>Testable Questions</strong>
        <ul>
          <li>3-5 specific questions you'll investigate</li>
          <li>Examples: "Where is this concept represented?", "Is it computed locally or distributed?", "Does it align
            with human understanding?"</li>
        </ul>
      </li>

      <li><strong>Preliminary Examples</strong>
        <ul>
          <li>5-10 example sentences/prompts where the concept is present vs absent</li>
          <li>These will form the basis of your evaluation dataset</li>
        </ul>
      </li>

      <li><strong>Success Criteria</strong>
        <ul>
          <li>What would constitute a successful project?</li>
          <li>Remember: Null results with good methodology are publishable!</li>
        </ul>
      </li>

      <li><strong>Potential Challenges</strong>
        <ul>
          <li>What might be difficult about this project?</li>
          <li>How will you mitigate risks?</li>
        </ul>
      </li>
    </ol>

    <p><strong>Submission:</strong> Upload to Canvas by [DATE]. We'll provide feedback within one week.</p>

    <p><strong>Template:</strong> Download the <a href="project_proposal_template.pdf">project proposal template</a> to
      help structure your submission.</p>
  </div>

  <h2>8. Looking Ahead</h2>

  <p><strong>Week 1:</strong> Benchmarking—how to create evaluation datasets for your concept</p>
  <p><strong>Week 2:</strong> Steering—manipulating model behavior by adding activation vectors</p>
  <p><strong>Week 3:</strong> Representation visualization—seeing what models encode</p>
  <p><strong>Weeks 4-8:</strong> Advanced methods (circuits, probes, SAEs, validation)</p>

  <p>
    <strong>By the end:</strong> You'll have a complete research pipeline for characterizing YOUR concept in LLMs.
  </p>

  <div class="info-box">
    <h3>Questions?</h3>
    <p>Reach out to the teaching team:</p>
    <ul>
      <li>Prof. David Bau: <a href="mailto:davidbau@northeastern.edu">davidbau@northeastern.edu</a></li>
      <li>Nikhil Prakash (TA): <a href="mailto:prakash.nik@northeastern.edu">prakash.nik@northeastern.edu</a></li>
    </ul>
    <p>Office hours: [See course website]</p>
  </div>

  <h2>References & Further Reading</h2>

  <h3>On Finding Good Research Questions</h3>
  <ul>
    <li>Hamming, R. W. (1986). <a href="papers/hamming-1986-you-and-your-research.pdf">"You and Your Research"</a>. Bell Communications Research colloquium. <em>The classic essay on choosing important problems.</em></li>
    <li>Nielsen, M. A. (2004). <a href="papers/nielsen-2004-principles-of-effective-research.pdf">"Principles of Effective Research"</a>. University of Queensland Technical Note. <em>On problem-solvers vs. problem-creators.</em></li>
    <li>Cummings, S. R., Browner, W. S., &amp; Hulley, S. B. (2013). <a href="papers/cummings-2013-conceiving-the-research-question.pdf">"Conceiving the Research Question"</a>. In <em>Designing Clinical Research</em>. <em>The FINER framework.</em></li>
    <li>Bau, D. (2024). <a href="https://davidbau.com/archives/2024/12/05/in_defense_of_curiosity.html">"In Defense of Curiosity"</a>. <em>On Venetian glassmaking, the value of basic scientific research, and understanding for its own sake.</em></li>
  </ul>

  <h3>Interpretability Papers Mentioned</h3>
  <ul>
    <li>Meng et al. (2022). <a href="papers/meng-2022-rome-2202.05262.pdf">"Locating and Editing Factual Associations in GPT"</a> (ROME)</li>
    <li>Todd et al. (2024). <a href="papers/todd-2023-function-vectors-2310.15213.pdf">"Function Vectors in Large Language Models"</a></li>
    <li>Prakash et al. (2025). <a href="papers/prakash-2024-entity-tracking-2402.14811.pdf">"Fine-tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking"</a></li>
    <li>Olsson et al. (2022). <a href="papers/olsson-2022-induction-heads-2209.11895.pdf">"In-context Learning and Induction Heads"</a></li>
    <li>Bau et al. (2019). <a href="papers/bau-2018-gan-dissection-1811.10597.pdf">"GAN Dissection"</a> | <a href="https://gandissect.csail.mit.edu/" target="_blank">Interactive demo</a></li>
  </ul>

  <h3>Accessible Introductions</h3>
  <ul>
    <li><a href="https://transformer-circuits.pub/" target="_blank">Transformer Circuits Thread</a> (Anthropic) —
      Visual explanations of interpretability concepts</li>
    <li><a href="https://distill.pub/" target="_blank">Distill.pub</a> — Interactive ML explanations (now archived but
      excellent)</li>
    <li><a href="https://pair.withgoogle.com/explorables/" target="_blank">Google PAIR Explorables</a> — Interactive ML
      concept demos</li>
  </ul>

  <h3>Supplementary Reading</h3>
  <ul>
    <li>Olah et al. (2020). "Zoom In: An Introduction to Circuits". <a href="https://distill.pub/2020/circuits/zoom-in/"
        target="_blank">Distill</a></li>
    <li>Elhage et al. (2021). "A Mathematical Framework for Transformer Circuits". <a
        href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">Transformer Circuits</a></li>
  </ul>

  <h2>Project Milestone</h2>
  <div class="assignment-box">
    <p><strong>Due: Thursday of Week 0</strong></p>
    <p>
      Form interdisciplinary teams of approximately 3 members: one non-CS PhD student (bringing domain expertise),
      one CS/ML PhD student (bringing technical ML background), and one BauLab member (bringing interpretability expertise).
    </p>
    <p>
      Begin brainstorming concepts from your domain that might be interesting to study in LLMs. The goal is to identify
      2-3 candidate concepts that are:
    </p>
    <ul>
      <li><strong>Specific enough</strong> to operationalize (not too abstract)</li>
      <li><strong>Interesting</strong> to your domain (relevant to real research questions)</li>
      <li><strong>Testable</strong> in language models (can be expressed through text)</li>
      <li><strong>Novel</strong> (ideally not already thoroughly studied in interpretability literature)</li>
    </ul>

    <h4>Deliverables:</h4>
    <ul>
      <li>Team formation: Submit team member names and roles</li>
      <li>Concept proposals: Brief description (1 paragraph each) of 2-3 candidate concepts</li>
      <li>Domain context: Why these concepts matter in your field</li>
      <li>Initial thoughts: What would successful "understanding" of this concept look like?</li>
    </ul>

    <p><em>
      You don't need to commit to a final concept yet—Week 1's steering exercise will help you explore
      which concept is most promising for deeper investigation.
    </em></p>
  </div>

</body>

</html>