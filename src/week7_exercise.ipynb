{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 7 Exercise: Unsupervised Feature Discovery and Superposition\n",
        "\n",
        "In this exercise, you'll gain hands-on experience with:\n",
        "- Building toy models of superposition\n",
        "- Training sparse autoencoders\n",
        "- Interpreting discovered features\n",
        "- Validating feature quality\n",
        "- Comparing SAE features with other methods\n",
        "- Understanding feature splitting and capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy matplotlib einops -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Toy Model of Superposition\n",
        "\n",
        "Build a simple model to understand how features can exist in superposition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyModel(nn.Module):\n",
        "    \"\"\"Simple autoencoder to demonstrate superposition.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_features=5, n_hidden=2):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "        # Encoder and decoder\n",
        "        self.W_enc = nn.Parameter(torch.randn(n_hidden, n_features) * 0.1)\n",
        "        self.W_dec = nn.Parameter(torch.randn(n_features, n_hidden) * 0.1)\n",
        "        self.b_enc = nn.Parameter(torch.zeros(n_hidden))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(n_features))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encode to bottleneck\n",
        "        hidden = torch.matmul(x, self.W_enc.t()) + self.b_enc\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        # Decode back\n",
        "        reconstructed = torch.matmul(hidden, self.W_dec.t()) + self.b_dec\n",
        "        \n",
        "        return reconstructed, hidden\n",
        "\n",
        "\n",
        "def generate_sparse_data(n_features, n_samples, sparsity=0.1):\n",
        "    \"\"\"\n",
        "    Generate sparse data where most features are zero.\n",
        "    \n",
        "    Args:\n",
        "        n_features: Number of features\n",
        "        n_samples: Number of samples\n",
        "        sparsity: Probability that each feature is non-zero\n",
        "    \"\"\"\n",
        "    # Random binary mask for sparsity\n",
        "    mask = (torch.rand(n_samples, n_features) < sparsity).float()\n",
        "    \n",
        "    # Feature values when active\n",
        "    values = torch.randn(n_samples, n_features)\n",
        "    \n",
        "    # Sparse data\n",
        "    data = mask * values\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "# Create toy model with more features than dimensions\n",
        "n_features = 5\n",
        "n_hidden = 2\n",
        "n_samples = 1000\n",
        "sparsity = 0.1\n",
        "\n",
        "print(f\"Toy Model Setup:\")\n",
        "print(f\"  Features: {n_features}\")\n",
        "print(f\"  Hidden dimensions: {n_hidden}\")\n",
        "print(f\"  Compression ratio: {n_features/n_hidden:.1f}x\")\n",
        "print(f\"  Sparsity: {sparsity:.1%}\")\n",
        "print(f\"\\nThis creates conditions for superposition:\")\n",
        "print(f\"  - More features than dimensions\")\n",
        "print(f\"  - Sparse activations (low interference)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train toy model\n",
        "model = ToyModel(n_features, n_hidden).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(500):\n",
        "    data = generate_sparse_data(n_features, n_samples, sparsity).to(device)\n",
        "    \n",
        "    reconstructed, hidden = model(data)\n",
        "    loss = ((reconstructed - data) ** 2).mean()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# Plot training curve\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Reconstruction Loss')\n",
        "plt.title('Toy Model Training: Learning Superposition')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize learned representations\n",
        "# The columns of W_dec show how features are represented in 2D space\n",
        "W_dec_np = model.W_dec.detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "\n",
        "# Plot each feature as a vector\n",
        "for i in range(n_features):\n",
        "    vector = W_dec_np[i, :]\n",
        "    plt.arrow(0, 0, vector[0], vector[1], \n",
        "             head_width=0.05, head_length=0.05, \n",
        "             fc=f'C{i}', ec=f'C{i}',\n",
        "             label=f'Feature {i}')\n",
        "\n",
        "plt.xlabel('Hidden Dimension 1')\n",
        "plt.ylabel('Hidden Dimension 2')\n",
        "plt.title(f'Feature Representations in 2D Space\\n({n_features} features in {n_hidden} dimensions)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Each arrow represents how a feature is encoded in 2D space\")\n",
        "print(\"  - Notice: 5 features packed into 2 dimensions (superposition!)\")\n",
        "print(\"  - Features can interfere when multiple are active simultaneously\")\n",
        "print(\"  - But with sparsity, interference is rare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test interference\n",
        "# When only one feature active → good reconstruction\n",
        "# When multiple features active → interference\n",
        "\n",
        "# Single feature active\n",
        "single_input = torch.zeros(1, n_features).to(device)\n",
        "single_input[0, 0] = 1.0\n",
        "single_recon, _ = model(single_input)\n",
        "\n",
        "print(\"Single feature active:\")\n",
        "print(f\"  Input:  {single_input[0].cpu().numpy()}\")\n",
        "print(f\"  Recon:  {single_recon[0].detach().cpu().numpy()}\")\n",
        "print(f\"  Error:  {((single_recon - single_input)**2).sum().item():.4f}\")\n",
        "\n",
        "# Multiple features active (interference)\n",
        "multi_input = torch.zeros(1, n_features).to(device)\n",
        "multi_input[0, 0] = 1.0\n",
        "multi_input[0, 1] = 1.0\n",
        "multi_input[0, 2] = 1.0\n",
        "multi_recon, _ = model(multi_input)\n",
        "\n",
        "print(\"\\nMultiple features active:\")\n",
        "print(f\"  Input:  {multi_input[0].cpu().numpy()}\")\n",
        "print(f\"  Recon:  {multi_recon[0].detach().cpu().numpy()}\")\n",
        "print(f\"  Error:  {((multi_recon - multi_input)**2).sum().item():.4f}\")\n",
        "\n",
        "print(\"\\nConclusion: More interference when multiple features active!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Training a Sparse Autoencoder\n",
        "\n",
        "Now let's train an SAE to decompose real model activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Hidden size: {model.config.n_embd}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SparseAutoencoder(nn.Module):\n",
        "    \"\"\"Sparse autoencoder for decomposing model activations.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, d_hidden, l1_coeff=1e-3):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.l1_coeff = l1_coeff\n",
        "        \n",
        "        # Encoder and decoder\n",
        "        self.W_enc = nn.Parameter(torch.randn(d_hidden, d_model) / np.sqrt(d_model))\n",
        "        self.W_dec = nn.Parameter(torch.randn(d_model, d_hidden) / np.sqrt(d_hidden))\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        pre_activation = torch.matmul(x, self.W_enc.t()) + self.b_enc\n",
        "        features = torch.relu(pre_activation)\n",
        "        \n",
        "        # Decode\n",
        "        reconstructed = torch.matmul(features, self.W_dec.t()) + self.b_dec\n",
        "        \n",
        "        return reconstructed, features\n",
        "    \n",
        "    def loss(self, x):\n",
        "        reconstructed, features = self.forward(x)\n",
        "        \n",
        "        # Reconstruction loss\n",
        "        recon_loss = ((reconstructed - x) ** 2).mean()\n",
        "        \n",
        "        # Sparsity penalty (L1)\n",
        "        sparsity_loss = self.l1_coeff * features.abs().mean()\n",
        "        \n",
        "        return recon_loss + sparsity_loss, recon_loss, sparsity_loss\n",
        "\n",
        "\n",
        "# Create SAE\n",
        "d_model = model.config.n_embd  # 768 for GPT-2 small\n",
        "d_hidden = d_model * 4  # 4x overcomplete (3072 features)\n",
        "\n",
        "sae = SparseAutoencoder(d_model, d_hidden, l1_coeff=1e-3).to(device)\n",
        "\n",
        "print(f\"SAE Architecture:\")\n",
        "print(f\"  Input dimensions: {d_model}\")\n",
        "print(f\"  Hidden features: {d_hidden}\")\n",
        "print(f\"  Expansion factor: {d_hidden/d_model:.1f}x\")\n",
        "print(f\"  L1 coefficient: {sae.l1_coeff}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect activation data\n",
        "def collect_activations(texts, layer_idx=-1):\n",
        "    \"\"\"Extract activations from a specific layer.\"\"\"\n",
        "    activations = []\n",
        "    \n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states[layer_idx]\n",
        "            \n",
        "            # Collect all token activations\n",
        "            for pos in range(hidden_states.shape[1]):\n",
        "                activations.append(hidden_states[0, pos, :].cpu())\n",
        "    \n",
        "    return torch.stack(activations)\n",
        "\n",
        "\n",
        "# Example texts\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"Neural networks consist of layers of interconnected nodes.\",\n",
        "    \"Coffee is a popular beverage consumed worldwide.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"Quantum computing could revolutionize computation.\",\n",
        "    \"The human brain contains billions of neurons.\",\n",
        "    \"Mathematics is the language of science.\"\n",
        "]\n",
        "\n",
        "print(\"Collecting activations...\")\n",
        "activations = collect_activations(texts, layer_idx=6)\n",
        "print(f\"Collected {activations.shape[0]} activation vectors\")\n",
        "print(f\"Shape: {activations.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SAE\n",
        "optimizer = optim.Adam(sae.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 100\n",
        "\n",
        "train_losses = []\n",
        "recon_losses = []\n",
        "sparsity_losses = []\n",
        "l0_sparsities = []\n",
        "\n",
        "print(\"Training SAE...\\n\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Shuffle data\n",
        "    indices = torch.randperm(activations.shape[0])\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_recon = 0\n",
        "    epoch_sparsity = 0\n",
        "    epoch_l0 = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for i in range(0, activations.shape[0], batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        batch = activations[batch_indices].to(device)\n",
        "        \n",
        "        loss, recon_loss, sparsity_loss = sae.loss(batch)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics\n",
        "        with torch.no_grad():\n",
        "            _, features = sae(batch)\n",
        "            l0 = (features > 0).float().sum(dim=1).mean().item()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_recon += recon_loss.item()\n",
        "        epoch_sparsity += sparsity_loss.item()\n",
        "        epoch_l0 += l0\n",
        "        n_batches += 1\n",
        "    \n",
        "    # Average over batches\n",
        "    train_losses.append(epoch_loss / n_batches)\n",
        "    recon_losses.append(epoch_recon / n_batches)\n",
        "    sparsity_losses.append(epoch_sparsity / n_batches)\n",
        "    l0_sparsities.append(epoch_l0 / n_batches)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        print(f\"  Total loss: {train_losses[-1]:.4f}\")\n",
        "        print(f\"  Recon loss: {recon_losses[-1]:.4f}\")\n",
        "        print(f\"  L0 sparsity: {l0_sparsities[-1]:.1f}/{d_hidden} ({100*l0_sparsities[-1]/d_hidden:.1f}%)\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].plot(recon_losses)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Reconstruction Loss')\n",
        "axes[0].set_title('Reconstruction Quality')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(sparsity_losses)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('L1 Penalty')\n",
        "axes[1].set_title('Sparsity Penalty')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(l0_sparsities)\n",
        "axes[2].axhline(y=d_hidden, color='r', linestyle='--', label='All features')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Active Features (L0)')\n",
        "axes[2].set_title('Feature Sparsity')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal metrics:\")\n",
        "print(f\"  Reconstruction loss: {recon_losses[-1]:.4f}\")\n",
        "print(f\"  Active features: {l0_sparsities[-1]:.1f}/{d_hidden} ({100*l0_sparsities[-1]/d_hidden:.1f}%)\")\n",
        "print(f\"  Explained variance: {1 - recon_losses[-1]/activations.var():.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Feature Interpretation\n",
        "\n",
        "Interpret what features the SAE discovered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find most active features\n",
        "with torch.no_grad():\n",
        "    _, all_features = sae(activations.to(device))\n",
        "    \n",
        "    # Feature activation frequencies\n",
        "    feature_counts = (all_features > 0).float().sum(dim=0).cpu()\n",
        "    \n",
        "    # Feature activation magnitudes\n",
        "    feature_magnitudes = all_features.mean(dim=0).cpu()\n",
        "\n",
        "# Find top features\n",
        "top_k = 10\n",
        "top_indices = torch.argsort(feature_counts, descending=True)[:top_k]\n",
        "\n",
        "print(f\"Top {top_k} most frequently active features:\\n\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    freq = feature_counts[idx] / activations.shape[0]\n",
        "    mag = feature_magnitudes[idx]\n",
        "    print(f\"{i+1}. Feature {idx.item()}:\")\n",
        "    print(f\"   Activation frequency: {freq:.1%}\")\n",
        "    print(f\"   Average magnitude: {mag:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find max-activating examples for a feature\n",
        "def find_max_activating_examples(feature_idx, activations, texts, tokenizer, k=5):\n",
        "    \"\"\"\n",
        "    Find texts/tokens where a feature activates most strongly.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        _, features = sae(activations.to(device))\n",
        "        feature_activations = features[:, feature_idx].cpu()\n",
        "    \n",
        "    # Find top k activations\n",
        "    top_k_values, top_k_indices = torch.topk(feature_activations, k)\n",
        "    \n",
        "    # Map back to texts (approximate)\n",
        "    examples = []\n",
        "    token_idx = 0\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token_idx in top_k_indices:\n",
        "                activation = feature_activations[token_idx].item()\n",
        "                examples.append((text, token, activation))\n",
        "            token_idx += 1\n",
        "    \n",
        "    return examples\n",
        "\n",
        "\n",
        "# Analyze a specific feature\n",
        "feature_to_analyze = top_indices[0].item()\n",
        "examples = find_max_activating_examples(feature_to_analyze, activations, texts, tokenizer, k=5)\n",
        "\n",
        "print(f\"\\nMax-activating examples for Feature {feature_to_analyze}:\\n\")\n",
        "for i, (text, token, activation) in enumerate(examples[:5]):\n",
        "    print(f\"{i+1}. Token: '{token}' (activation: {activation:.4f})\")\n",
        "    print(f\"   Context: {text}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple automated interpretation\n",
        "# In practice, you'd use GPT-4 API for this\n",
        "# Here we'll just show the pattern\n",
        "\n",
        "def interpret_feature_simple(examples):\n",
        "    \"\"\"\n",
        "    Simple heuristic interpretation based on max-activating examples.\n",
        "    In practice, use LLM for this.\n",
        "    \"\"\"\n",
        "    tokens = [ex[1] for ex in examples]\n",
        "    \n",
        "    # Simple heuristics\n",
        "    if all(t.startswith('Ġ') for t in tokens):  # Space-prefixed tokens\n",
        "        return \"Likely: word boundaries or start of tokens\"\n",
        "    elif all(t.isupper() for t in tokens):\n",
        "        return \"Likely: uppercase/acronyms\"\n",
        "    elif all(t.isdigit() for t in tokens):\n",
        "        return \"Likely: numbers\"\n",
        "    else:\n",
        "        return f\"Tokens: {', '.join(tokens[:3])}...\"\n",
        "\n",
        "\n",
        "interpretation = interpret_feature_simple(examples)\n",
        "print(f\"Simple interpretation: {interpretation}\")\n",
        "print(\"\\nNote: In practice, use GPT-4 API for automated interpretation.\")\n",
        "print(\"Send max-activating examples to GPT-4 and ask 'What concept do these share?'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Feature Validation\n",
        "\n",
        "Test feature quality: monosemanticity and causality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test monosemanticity: diversity of activating examples\n",
        "def test_monosemanticity(feature_idx, activations, texts, tokenizer, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Check if feature responds to coherent concept.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        _, features = sae(activations.to(device))\n",
        "        feature_acts = features[:, feature_idx].cpu()\n",
        "    \n",
        "    # Get all activations above threshold\n",
        "    active_indices = (feature_acts > threshold).nonzero(as_tuple=True)[0]\n",
        "    \n",
        "    # Sample of activating contexts\n",
        "    n_samples = min(10, len(active_indices))\n",
        "    sample_indices = active_indices[torch.randperm(len(active_indices))[:n_samples]]\n",
        "    \n",
        "    print(f\"Feature {feature_idx} activates on {len(active_indices)} examples\")\n",
        "    print(f\"\\nSample of activating contexts:\")\n",
        "    \n",
        "    token_idx = 0\n",
        "    shown = 0\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        for token in tokens:\n",
        "            if token_idx in sample_indices and shown < 5:\n",
        "                print(f\"  '{token}' in: {text}\")\n",
        "                shown += 1\n",
        "            token_idx += 1\n",
        "    \n",
        "    return len(active_indices)\n",
        "\n",
        "\n",
        "# Test a feature\n",
        "n_active = test_monosemanticity(feature_to_analyze, activations, texts, tokenizer)\n",
        "print(f\"\\nMonosemanticity check: Do these contexts share a common concept?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test causality: steering with SAE features\n",
        "def steer_with_sae_feature(model, sae, text, feature_idx, layer_idx, alpha=5.0):\n",
        "    \"\"\"\n",
        "    Test if amplifying an SAE feature affects model output.\n",
        "    This is a simplified version - full implementation needs hooks.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Baseline\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        baseline_logits = outputs.logits[0, -1, :]\n",
        "        baseline_probs = torch.softmax(baseline_logits, dim=-1)\n",
        "        baseline_top = torch.topk(baseline_probs, 5)\n",
        "    \n",
        "    print(f\"Baseline predictions for '{text}':\")\n",
        "    for prob, idx in zip(baseline_top.values, baseline_top.indices):\n",
        "        token = tokenizer.decode([idx])\n",
        "        print(f\"  {token}: {prob:.4f}\")\n",
        "    \n",
        "    # In practice, you would:\n",
        "    # 1. Get activations at layer_idx\n",
        "    # 2. Pass through SAE encoder to get features\n",
        "    # 3. Multiply feature_idx by alpha\n",
        "    # 4. Decode back through SAE\n",
        "    # 5. Continue model forward pass\n",
        "    # 6. Compare outputs\n",
        "    \n",
        "    print(\"\\nNote: Full steering implementation requires activation hooks.\")\n",
        "    print(\"See Week 2 exercises for complete steering code.\")\n",
        "\n",
        "\n",
        "# Test steering\n",
        "test_text = \"The capital of France is\"\n",
        "steer_with_sae_feature(model, sae, test_text, feature_to_analyze, layer_idx=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Comparing with Other Methods\n",
        "\n",
        "Compare SAE features with steering vectors and probes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare SAE feature with steering vector (from Week 2)\n",
        "# For a concept, compute steering vector and SAE features\n",
        "\n",
        "def compare_sae_with_steering(sae, steering_vector, activations):\n",
        "    \"\"\"\n",
        "    Compare SAE decomposition with steering vector.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Encode steering vector through SAE\n",
        "        steering_encoded, steering_features = sae(steering_vector.unsqueeze(0).to(device))\n",
        "        \n",
        "        # Find which SAE features activate\n",
        "        active_features = (steering_features[0] > 0.1).nonzero(as_tuple=True)[0]\n",
        "        \n",
        "        print(f\"Steering vector decomposition:\")\n",
        "        print(f\"  Active SAE features: {len(active_features)}\")\n",
        "        \n",
        "        # Top contributing features\n",
        "        top_k = min(5, len(active_features))\n",
        "        top_values, top_indices = torch.topk(steering_features[0], top_k)\n",
        "        \n",
        "        print(f\"\\n  Top {top_k} features:\")\n",
        "        for i, (val, idx) in enumerate(zip(top_values, top_indices)):\n",
        "            print(f\"    {i+1}. Feature {idx.item()}: {val.item():.4f}\")\n",
        "        \n",
        "        # Reconstruction quality\n",
        "        recon_error = ((steering_encoded[0] - steering_vector.to(device))**2).sum().item()\n",
        "        print(f\"\\n  Reconstruction error: {recon_error:.4f}\")\n",
        "\n",
        "\n",
        "# Example: create a simple \"steering vector\" (random for demo)\n",
        "demo_steering_vector = torch.randn(d_model)\n",
        "compare_sae_with_steering(sae, demo_steering_vector, activations)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Steering vectors often decompose into 1-3 main SAE features\")\n",
        "print(\"  - This reveals what 'concepts' make up the steering direction\")\n",
        "print(\"  - Can use SAE features for more targeted steering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Feature Splitting\n",
        "\n",
        "Explore how features split as SAE capacity increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SAEs with different capacities\n",
        "capacities = [d_model * 2, d_model * 4, d_model * 8]\n",
        "capacity_results = []\n",
        "\n",
        "print(\"Training SAEs with different capacities...\\n\")\n",
        "\n",
        "for capacity in capacities:\n",
        "    print(f\"Training {capacity}-feature SAE...\")\n",
        "    \n",
        "    # Create and train SAE\n",
        "    sae_temp = SparseAutoencoder(d_model, capacity, l1_coeff=1e-3).to(device)\n",
        "    optimizer_temp = optim.Adam(sae_temp.parameters(), lr=1e-3)\n",
        "    \n",
        "    # Quick training (fewer epochs for demo)\n",
        "    for epoch in range(20):\n",
        "        indices = torch.randperm(activations.shape[0])\n",
        "        for i in range(0, activations.shape[0], batch_size):\n",
        "            batch_indices = indices[i:i+batch_size]\n",
        "            batch = activations[batch_indices].to(device)\n",
        "            \n",
        "            loss, _, _ = sae_temp.loss(batch)\n",
        "            optimizer_temp.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_temp.step()\n",
        "    \n",
        "    # Evaluate\n",
        "    with torch.no_grad():\n",
        "        _, features = sae_temp(activations.to(device))\n",
        "        recon_loss = ((sae_temp(activations.to(device))[0] - activations.to(device))**2).mean().item()\n",
        "        l0 = (features > 0).float().sum(dim=1).mean().item()\n",
        "        \n",
        "        capacity_results.append({\n",
        "            'capacity': capacity,\n",
        "            'recon_loss': recon_loss,\n",
        "            'l0': l0,\n",
        "            'sparsity': l0 / capacity\n",
        "        })\n",
        "    \n",
        "    print(f\"  Recon loss: {recon_loss:.4f}\")\n",
        "    print(f\"  Active features: {l0:.1f}/{capacity} ({100*l0/capacity:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize capacity-performance tradeoff\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Reconstruction vs capacity\n",
        "axes[0].plot([r['capacity'] for r in capacity_results],\n",
        "            [r['recon_loss'] for r in capacity_results],\n",
        "            marker='o')\n",
        "axes[0].set_xlabel('SAE Capacity (# features)')\n",
        "axes[0].set_ylabel('Reconstruction Loss')\n",
        "axes[0].set_title('Reconstruction Quality vs Capacity')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Active features vs capacity\n",
        "axes[1].plot([r['capacity'] for r in capacity_results],\n",
        "            [r['l0'] for r in capacity_results],\n",
        "            marker='o', label='Active features')\n",
        "axes[1].plot([r['capacity'] for r in capacity_results],\n",
        "            [r['capacity'] for r in capacity_results],\n",
        "            'r--', label='Total capacity')\n",
        "axes[1].set_xlabel('SAE Capacity (# features)')\n",
        "axes[1].set_ylabel('Number of Features')\n",
        "axes[1].set_title('Active Features vs Capacity')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Higher capacity → better reconstruction (less information loss)\")\n",
        "print(\"  - Higher capacity → features split into subfeatrues\")\n",
        "print(\"  - Trade-off: interpretability (fewer features) vs completeness (more features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Your Project Template\n",
        "\n",
        "Apply SAEs to discover features for your concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Week 7 Project Template: SAE Feature Discovery for Your Concept\\n\")\n",
        "\n",
        "print(\"1. Train or load SAEs\")\n",
        "# Option A: Train your own SAEs on relevant layers\n",
        "# Option B: Use pre-trained SAEs from Neuronpedia\n",
        "\n",
        "print(\"\\n2. Extract features on your dataset\")\n",
        "# Run your concept-relevant texts through SAE\n",
        "# Identify which features activate\n",
        "\n",
        "print(\"\\n3. Interpret features\")\n",
        "# Use automated interpretation (GPT-4) on max-activating examples\n",
        "# Identify 10-20 features most relevant to your concept\n",
        "\n",
        "print(\"\\n4. Validate features\")\n",
        "# Monosemanticity: coherent concept?\n",
        "# Consistency: activates reliably?\n",
        "# Causality: steering tests (Week 2 methods)\n",
        "\n",
        "print(\"\\n5. Compare with other methods\")\n",
        "# How do SAE features relate to:\n",
        "#   - Steering vectors (Week 2)\n",
        "#   - Probe directions (Week 6)\n",
        "#   - Circuit components (Week 5)\n",
        "\n",
        "print(\"\\n6. Analyze completeness\")\n",
        "# Do SAE features capture all aspects of your concept?\n",
        "# What's missing?\n",
        "# Are multiple features needed?\n",
        "\n",
        "print(\"\\n7. Document findings\")\n",
        "# Create feature catalog with interpretations\n",
        "# Validation results\n",
        "# Comparison with other methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this exercise, you've learned:\n",
        "- How superposition allows networks to represent more features than dimensions\n",
        "- How SAEs use sparse decomposition to reverse superposition\n",
        "- How to train and evaluate SAEs\n",
        "- How to interpret features using automated methods\n",
        "- How to validate feature quality (monosemanticity, causality)\n",
        "- How SAE features relate to other discovery methods\n",
        "- How feature splitting occurs as capacity increases\n",
        "\n",
        "Key takeaways:\n",
        "- **Superposition explains polysemanticity** - features interfere in limited dimensions\n",
        "- **SAEs discover features unsupervised** - no labels needed\n",
        "- **Validation is critical** - feature presence ≠ causal use\n",
        "- **Comparison reveals structure** - how features relate to steering/probes\n",
        "- **Capacity creates trade-offs** - completeness vs interpretability\n",
        "\n",
        "For your project:\n",
        "1. Use SAEs to discover features related to your concept\n",
        "2. Validate quality with multiple methods\n",
        "3. Compare with previous findings (steering, probes, circuits)\n",
        "4. Identify gaps in feature coverage\n",
        "5. Use validated features for steering and analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
