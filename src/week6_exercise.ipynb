{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6 Exercise: Probes and Masks\n",
        "\n",
        "In this exercise, you'll gain hands-on experience with:\n",
        "- Training linear and MLP probes\n",
        "- Implementing control tasks\n",
        "- Testing for overfitting and underfitting\n",
        "- Training learned masks with sparse regularization\n",
        "- Comparing probe/mask findings with causal interventions\n",
        "- Interpreting agreements and disagreements between methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy matplotlib scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load GPT-2 small\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nModel: {model_name}\")\n",
        "print(f\"Number of layers: {model.config.n_layer}\")\n",
        "print(f\"Hidden size: {model.config.n_embd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Creating a Probe Dataset\n",
        "\n",
        "First, let's create a balanced dataset for probing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Probing for sentiment (positive vs negative)\n",
        "positive_examples = [\n",
        "    \"This movie was amazing and wonderful\",\n",
        "    \"I love this product, it's fantastic\",\n",
        "    \"The weather is beautiful today\",\n",
        "    \"What a great experience at the restaurant\",\n",
        "    \"The book was inspiring and uplifting\",\n",
        "    \"I'm so happy with my purchase\",\n",
        "    \"The service was excellent and friendly\",\n",
        "    \"This is the best thing ever\",\n",
        "    \"I enjoyed every moment of it\",\n",
        "    \"Outstanding quality and performance\",\n",
        "    \"Absolutely delighted with the results\",\n",
        "    \"A truly remarkable achievement\",\n",
        "    \"The team did an excellent job\",\n",
        "    \"I highly recommend this to everyone\",\n",
        "    \"Perfect in every way possible\"\n",
        "]\n",
        "\n",
        "negative_examples = [\n",
        "    \"This movie was terrible and boring\",\n",
        "    \"I hate this product, it's awful\",\n",
        "    \"The weather is dreadful today\",\n",
        "    \"What a horrible experience at the restaurant\",\n",
        "    \"The book was depressing and disappointing\",\n",
        "    \"I'm so unhappy with my purchase\",\n",
        "    \"The service was terrible and rude\",\n",
        "    \"This is the worst thing ever\",\n",
        "    \"I regret every moment of it\",\n",
        "    \"Poor quality and terrible performance\",\n",
        "    \"Absolutely disappointed with the results\",\n",
        "    \"A truly disastrous failure\",\n",
        "    \"The team did an awful job\",\n",
        "    \"I strongly advise against this\",\n",
        "    \"Flawed in every way possible\"\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "texts = positive_examples + negative_examples\n",
        "labels = [1] * len(positive_examples) + [0] * len(negative_examples)\n",
        "\n",
        "print(f\"Dataset size: {len(texts)} examples\")\n",
        "print(f\"Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}\")\n",
        "print(f\"\\nExample positive: {positive_examples[0]}\")\n",
        "print(f\"Example negative: {negative_examples[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Extracting Hidden States\n",
        "\n",
        "Extract representations from all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hidden_states(texts, layer_idx=-1, position=-1):\n",
        "    \"\"\"\n",
        "    Extract hidden states from specified layer and position.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings\n",
        "        layer_idx: Which layer (-1 for last layer)\n",
        "        position: Which token position (-1 for last token)\n",
        "    \n",
        "    Returns:\n",
        "        hidden_states: [num_examples, hidden_size] numpy array\n",
        "    \"\"\"\n",
        "    all_hidden_states = []\n",
        "    \n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            # Get hidden states from specified layer\n",
        "            hidden_state = outputs.hidden_states[layer_idx][0, position, :]\n",
        "            all_hidden_states.append(hidden_state.cpu().numpy())\n",
        "    \n",
        "    return np.array(all_hidden_states)\n",
        "\n",
        "\n",
        "# Extract from last layer as an example\n",
        "print(\"Extracting hidden states from last layer...\")\n",
        "hidden_states = extract_hidden_states(texts, layer_idx=-1, position=-1)\n",
        "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
        "print(f\"  {hidden_states.shape[0]} examples × {hidden_states.shape[1]} dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Training Linear Probes\n",
        "\n",
        "Train linear probes at every layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_linear_probe(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Train a linear probe (logistic regression).\n",
        "    \n",
        "    Args:\n",
        "        X: Features [num_examples, hidden_size]\n",
        "        y: Labels [num_examples]\n",
        "        test_size: Fraction for test set\n",
        "    \n",
        "    Returns:\n",
        "        probe: Trained model\n",
        "        accuracy: Test accuracy\n",
        "    \"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Train linear probe\n",
        "    probe = LogisticRegression(max_iter=1000, random_state=random_state)\n",
        "    probe.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = probe.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    return probe, accuracy, (X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "# Train probes at all layers\n",
        "print(\"Training linear probes at all layers...\\n\")\n",
        "layer_accuracies = []\n",
        "\n",
        "for layer_idx in range(model.config.n_layer + 1):  # +1 for embedding layer\n",
        "    # Extract hidden states from this layer\n",
        "    X = extract_hidden_states(texts, layer_idx=layer_idx, position=-1)\n",
        "    \n",
        "    # Train probe\n",
        "    probe, accuracy, splits = train_linear_probe(X, labels)\n",
        "    layer_accuracies.append(accuracy)\n",
        "    \n",
        "    print(f\"Layer {layer_idx}: {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nBest layer: {np.argmax(layer_accuracies)} (accuracy: {max(layer_accuracies):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize probe accuracy across layers\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(layer_accuracies)), layer_accuracies, marker='o')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', label='Random baseline')\n",
        "plt.xlabel('Layer')\n",
        "plt.ylabel('Probe Accuracy')\n",
        "plt.title('Linear Probe Accuracy Across Layers')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation: Higher accuracy indicates the concept is more linearly accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Training MLP Probes\n",
        "\n",
        "Compare linear probes with nonlinear (MLP) probes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPProbe(nn.Module):\n",
        "    \"\"\"Simple MLP probe with one hidden layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mlp_probe(X, y, hidden_size=128, epochs=50, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train an MLP probe.\n",
        "    \"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
        "    y_train_t = torch.LongTensor(y_train).to(device)\n",
        "    y_test_t = torch.LongTensor(y_test).to(device)\n",
        "    \n",
        "    # Create probe\n",
        "    probe = MLPProbe(X.shape[1], hidden_size).to(device)\n",
        "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Train\n",
        "    probe.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = probe(X_train_t)\n",
        "        loss = criterion(outputs, y_train_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Evaluate\n",
        "    probe.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = probe(X_test_t)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == y_test_t).float().mean().item()\n",
        "    \n",
        "    return probe, accuracy\n",
        "\n",
        "\n",
        "# Compare linear vs MLP probes at each layer\n",
        "print(\"Training MLP probes at all layers...\\n\")\n",
        "mlp_accuracies = []\n",
        "\n",
        "for layer_idx in range(model.config.n_layer + 1):\n",
        "    X = extract_hidden_states(texts, layer_idx=layer_idx, position=-1)\n",
        "    probe, accuracy = train_mlp_probe(X, labels, epochs=50)\n",
        "    mlp_accuracies.append(accuracy)\n",
        "    print(f\"Layer {layer_idx}: {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nBest MLP layer: {np.argmax(mlp_accuracies)} (accuracy: {max(mlp_accuracies):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare linear vs MLP probes\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(layer_accuracies)), layer_accuracies, marker='o', label='Linear Probe')\n",
        "plt.plot(range(len(mlp_accuracies)), mlp_accuracies, marker='s', label='MLP Probe')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', label='Random baseline')\n",
        "plt.xlabel('Layer')\n",
        "plt.ylabel('Probe Accuracy')\n",
        "plt.title('Linear vs MLP Probe Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Calculate gap\n",
        "gaps = np.array(mlp_accuracies) - np.array(layer_accuracies)\n",
        "print(f\"\\nAverage MLP advantage: {gaps.mean():.4f}\")\n",
        "print(f\"Max gap at layer: {np.argmax(gaps)} (gap: {gaps.max():.4f})\")\n",
        "print(\"\\nInterpretation: Large gap suggests information is present but not linearly accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Control Tasks\n",
        "\n",
        "Validate probes with control tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selectivity control: Test if sentiment probe responds to tense (should not)\n",
        "past_tense = [\n",
        "    \"The cat walked to the store yesterday\",\n",
        "    \"She finished her homework last night\",\n",
        "    \"They played soccer in the park\",\n",
        "    \"He cooked dinner for everyone\",\n",
        "    \"We watched a movie together\"\n",
        "]\n",
        "\n",
        "present_tense = [\n",
        "    \"The cat walks to the store today\",\n",
        "    \"She finishes her homework tonight\",\n",
        "    \"They play soccer in the park\",\n",
        "    \"He cooks dinner for everyone\",\n",
        "    \"We watch a movie together\"\n",
        "]\n",
        "\n",
        "control_texts = past_tense + present_tense\n",
        "control_labels = [0] * len(past_tense) + [1] * len(present_tense)  # 0=past, 1=present\n",
        "\n",
        "# Extract hidden states for control task\n",
        "best_layer = np.argmax(layer_accuracies)\n",
        "print(f\"Testing selectivity on best layer: {best_layer}\\n\")\n",
        "\n",
        "X_control = extract_hidden_states(control_texts, layer_idx=best_layer, position=-1)\n",
        "\n",
        "# Train probe on original task (sentiment)\n",
        "X_sentiment = extract_hidden_states(texts, layer_idx=best_layer, position=-1)\n",
        "sentiment_probe, sentiment_acc, _ = train_linear_probe(X_sentiment, labels)\n",
        "\n",
        "# Test on control task (tense)\n",
        "y_control_pred = sentiment_probe.predict(X_control)\n",
        "control_acc = accuracy_score(control_labels, y_control_pred)\n",
        "\n",
        "print(f\"Sentiment probe on sentiment task: {sentiment_acc:.4f}\")\n",
        "print(f\"Sentiment probe on tense task (control): {control_acc:.4f}\")\n",
        "print(f\"Random baseline: 0.5000\")\n",
        "\n",
        "if abs(control_acc - 0.5) < 0.1:\n",
        "    print(\"\\n✓ PASS: Probe is selective (random on control task)\")\n",
        "else:\n",
        "    print(\"\\n✗ FAIL: Probe may be picking up confounds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random label test: Check for overfitting\n",
        "print(\"Testing for overfitting with random labels...\\n\")\n",
        "\n",
        "# Randomize labels\n",
        "random_labels = np.random.permutation(labels)\n",
        "\n",
        "# Train probe on random labels\n",
        "random_probe, random_acc, _ = train_linear_probe(X_sentiment, random_labels)\n",
        "\n",
        "print(f\"Probe on real labels: {sentiment_acc:.4f}\")\n",
        "print(f\"Probe on random labels: {random_acc:.4f}\")\n",
        "print(f\"Random baseline: 0.5000\")\n",
        "\n",
        "if random_acc < 0.6:\n",
        "    print(\"\\n✓ PASS: Probe is not overfitting (can't learn random labels)\")\n",
        "else:\n",
        "    print(\"\\n✗ WARNING: Probe may be overfitting (learns random labels too well)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Testing for Underfitting\n",
        "\n",
        "Check if linear probe is too simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_fitting(linear_acc, mlp_acc, threshold=0.15):\n",
        "    \"\"\"\n",
        "    Diagnose overfitting, underfitting, or good fit.\n",
        "    \"\"\"\n",
        "    gap = mlp_acc - linear_acc\n",
        "    \n",
        "    print(f\"Linear probe accuracy: {linear_acc:.4f}\")\n",
        "    print(f\"MLP probe accuracy: {mlp_acc:.4f}\")\n",
        "    print(f\"Gap: {gap:.4f}\\n\")\n",
        "    \n",
        "    if linear_acc < 0.6 and mlp_acc < 0.6:\n",
        "        print(\"Diagnosis: INFORMATION ABSENT or WRONG LAYER\")\n",
        "        print(\"  - Concept may not be encoded here\")\n",
        "        print(\"  - Try different layers\")\n",
        "        print(\"  - Verify with interventions\")\n",
        "    \n",
        "    elif linear_acc > 0.8 and gap < threshold:\n",
        "        print(\"Diagnosis: GOOD FIT (linear)\")\n",
        "        print(\"  - Information is linearly accessible\")\n",
        "        print(\"  - Linear probe is sufficient\")\n",
        "        print(\"  - Representation is relatively simple\")\n",
        "    \n",
        "    elif linear_acc < 0.7 and mlp_acc > 0.8:\n",
        "        print(\"Diagnosis: UNDERFITTING (linear probe too simple)\")\n",
        "        print(\"  - Information is present but nonlinear\")\n",
        "        print(\"  - Linear probe cannot extract it\")\n",
        "        print(\"  - MLP probe succeeds\")\n",
        "        print(\"  - Representation is complex/distributed\")\n",
        "    \n",
        "    elif linear_acc > mlp_acc:\n",
        "        print(\"Diagnosis: POSSIBLE OVERFITTING (MLP)\")\n",
        "        print(\"  - MLP may be overfitting to noise\")\n",
        "        print(\"  - Trust linear probe more\")\n",
        "        print(\"  - Try regularization on MLP\")\n",
        "    \n",
        "    else:\n",
        "        print(\"Diagnosis: MODERATE NONLINEARITY\")\n",
        "        print(\"  - Some nonlinear structure present\")\n",
        "        print(\"  - Both probes partially successful\")\n",
        "\n",
        "\n",
        "# Diagnose each layer\n",
        "print(\"Fitting diagnosis for each layer:\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for layer_idx in [0, best_layer, model.config.n_layer]:\n",
        "    print(f\"\\nLayer {layer_idx}:\")\n",
        "    diagnose_fitting(layer_accuracies[layer_idx], mlp_accuracies[layer_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Learned Masks with Regularization\n",
        "\n",
        "Train masks to identify important components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedModel(nn.Module):\n",
        "    \"\"\"Model with learnable component masks.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_components, num_classes=2):\n",
        "        super().__init__()\n",
        "        # Mask parameters (one per component)\n",
        "        self.mask_logits = nn.Parameter(torch.zeros(num_components))\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(num_components, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply sigmoid to get masks in [0, 1]\n",
        "        masks = torch.sigmoid(self.mask_logits)\n",
        "        \n",
        "        # Mask the input\n",
        "        x_masked = x * masks\n",
        "        \n",
        "        # Classify\n",
        "        return self.classifier(x_masked), masks\n",
        "\n",
        "\n",
        "def train_masked_model(X, y, l1_lambda=0.01, epochs=100, lr=0.01):\n",
        "    \"\"\"\n",
        "    Train a model with learned masks and L1 regularization.\n",
        "    \"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
        "    y_train_t = torch.LongTensor(y_train).to(device)\n",
        "    y_test_t = torch.LongTensor(y_test).to(device)\n",
        "    \n",
        "    # Create model\n",
        "    masked_model = MaskedModel(X.shape[1]).to(device)\n",
        "    optimizer = optim.Adam(masked_model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    test_accs = []\n",
        "    sparsities = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        masked_model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs, masks = masked_model(X_train_t)\n",
        "        \n",
        "        # Task loss + L1 regularization on masks\n",
        "        task_loss = criterion(outputs, y_train_t)\n",
        "        l1_reg = masks.abs().sum()\n",
        "        loss = task_loss + l1_lambda * l1_reg\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "        \n",
        "        # Evaluate\n",
        "        if epoch % 10 == 0:\n",
        "            masked_model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs, masks = masked_model(X_test_t)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                accuracy = (predicted == y_test_t).float().mean().item()\n",
        "                sparsity = (masks > 0.5).float().mean().item()\n",
        "                \n",
        "                test_accs.append(accuracy)\n",
        "                sparsities.append(sparsity)\n",
        "    \n",
        "    # Final evaluation\n",
        "    masked_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs, final_masks = masked_model(X_test_t)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        final_accuracy = (predicted == y_test_t).float().mean().item()\n",
        "        final_sparsity = (final_masks > 0.5).float().sum().item()\n",
        "    \n",
        "    return masked_model, final_accuracy, final_sparsity, final_masks\n",
        "\n",
        "\n",
        "# Train with different regularization strengths\n",
        "print(\"Training masked models with different L1 regularization...\\n\")\n",
        "\n",
        "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.5]\n",
        "results = []\n",
        "\n",
        "for l1_lambda in lambdas:\n",
        "    masked_model, acc, sparsity, masks = train_masked_model(\n",
        "        X_sentiment, labels, l1_lambda=l1_lambda, epochs=100\n",
        "    )\n",
        "    results.append((l1_lambda, acc, sparsity))\n",
        "    print(f\"λ={l1_lambda:.3f}: Accuracy={acc:.4f}, Active components={int(sparsity)}/{X_sentiment.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sparsity-performance tradeoff\n",
        "lambdas_list, accs, sparsities = zip(*results)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy vs lambda\n",
        "ax1.plot(lambdas_list, accs, marker='o')\n",
        "ax1.set_xlabel('L1 Regularization (λ)')\n",
        "ax1.set_ylabel('Test Accuracy')\n",
        "ax1.set_title('Accuracy vs Regularization Strength')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Sparsity vs accuracy\n",
        "ax2.plot(sparsities, accs, marker='o')\n",
        "ax2.set_xlabel('Number of Active Components')\n",
        "ax2.set_ylabel('Test Accuracy')\n",
        "ax2.set_title('Sparsity-Performance Tradeoff')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best tradeoff (highest accuracy with fewest components)\n",
        "efficiency = np.array(accs) / (np.array(sparsities) + 1)\n",
        "best_idx = np.argmax(efficiency)\n",
        "print(f\"\\nBest tradeoff: λ={lambdas_list[best_idx]:.3f}\")\n",
        "print(f\"  Accuracy: {accs[best_idx]:.4f}\")\n",
        "print(f\"  Active components: {int(sparsities[best_idx])}/{X_sentiment.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize learned masks\n",
        "plt.figure(figsize=(12, 4))\n",
        "mask_values = masks.cpu().numpy()\n",
        "plt.bar(range(len(mask_values)), mask_values)\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', label='Threshold (0.5)')\n",
        "plt.xlabel('Component (Dimension)')\n",
        "plt.ylabel('Mask Weight')\n",
        "plt.title('Learned Mask Weights (Best λ)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Top components\n",
        "top_k = 10\n",
        "top_indices = np.argsort(mask_values)[-top_k:][::-1]\n",
        "print(f\"\\nTop {top_k} components by mask weight:\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"  {i+1}. Dimension {idx}: {mask_values[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Comparing Probes with Interventions\n",
        "\n",
        "Validate probe findings with causal interventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ablation_test(model, text, layer_idx, top_components, position=-1):\n",
        "    \"\"\"\n",
        "    Test if ablating top components affects output.\n",
        "    \n",
        "    This is a simplified version - full implementation would require hooks.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Baseline\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(**inputs)\n",
        "        logits_baseline = outputs_baseline.logits[0, -1, :]\n",
        "        probs_baseline = torch.softmax(logits_baseline, dim=-1)\n",
        "    \n",
        "    # For simplicity, we'll use a proxy: check if the components\n",
        "    # identified by masks correlate with model behavior\n",
        "    # In a full implementation, you would actually patch these components\n",
        "    \n",
        "    return probs_baseline\n",
        "\n",
        "\n",
        "# Compare probe and mask findings\n",
        "print(\"Comparing probe and mask findings:\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nProbe analysis (Layer {best_layer}):\")\n",
        "print(f\"  Linear probe accuracy: {layer_accuracies[best_layer]:.4f}\")\n",
        "print(f\"  MLP probe accuracy: {mlp_accuracies[best_layer]:.4f}\")\n",
        "print(f\"  → Sentiment information is {'linearly' if layer_accuracies[best_layer] > 0.8 else 'nonlinearly'} accessible\")\n",
        "\n",
        "print(f\"\\nMask analysis:\")\n",
        "print(f\"  Best λ: {lambdas_list[best_idx]:.3f}\")\n",
        "print(f\"  Active components: {int(sparsities[best_idx])}/{X_sentiment.shape[1]}\")\n",
        "print(f\"  → Only {100 * sparsities[best_idx] / X_sentiment.shape[1]:.1f}% of components needed\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "if layer_accuracies[best_layer] > 0.8 and sparsities[best_idx] < X_sentiment.shape[1] * 0.2:\n",
        "    print(\"  ✓ Information is linearly accessible\")\n",
        "    print(\"  ✓ Representation is sparse (few components matter)\")\n",
        "    print(\"  → Strong candidate for causal role\")\n",
        "    print(\"  → Validate with intervention experiments\")\n",
        "elif layer_accuracies[best_layer] > 0.8:\n",
        "    print(\"  ✓ Information is linearly accessible\")\n",
        "    print(\"  ⚠ Representation is distributed (many components)\")\n",
        "    print(\"  → May be present but not causally used\")\n",
        "    print(\"  → Intervention critical to validate\")\n",
        "else:\n",
        "    print(\"  ⚠ Information not linearly accessible\")\n",
        "    print(\"  → May be encoded nonlinearly or not present\")\n",
        "    print(\"  → Check other layers or use MLP probes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Analyzing Agreement and Disagreement\n",
        "\n",
        "When do probes and interventions give different answers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "print(\"Summary: Probes, Masks, and Interventions\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary_data = {\n",
        "    'Method': ['Linear Probe', 'MLP Probe', 'Learned Masks', 'Intervention (Week 4)'],\n",
        "    'Question Answered': [\n",
        "        'Is info linearly accessible?',\n",
        "        'Is info computationally accessible?',\n",
        "        'Which components are sufficient?',\n",
        "        'Is info causally used?'\n",
        "    ],\n",
        "    'Speed': ['Fast', 'Fast', 'Medium', 'Slow'],\n",
        "    'Causal Claim': ['No', 'No', 'Weak', 'Yes'],\n",
        "    'Best For': [\n",
        "        'Quick exploration',\n",
        "        'Nonlinear patterns',\n",
        "        'Component selection',\n",
        "        'Validation'\n",
        "    ]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(summary_data)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\\nBest Practice Workflow:\")\n",
        "print(\"1. Start with linear probes (fast exploration)\")\n",
        "print(\"2. Try MLP probes if linear fails (check nonlinearity)\")\n",
        "print(\"3. Use learned masks (identify important components)\")\n",
        "print(\"4. Validate with interventions (test causal role)\")\n",
        "print(\"5. Investigate disagreements (reveal structure)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Your Project Template\n",
        "\n",
        "Apply these methods to your concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Week 6 Project Template: Probing and Masking Your Concept\\n\")\n",
        "\n",
        "print(\"1. Create your dataset\")\n",
        "MY_CONCEPT = \"[Your concept here]\"\n",
        "my_positive_examples = [\n",
        "    # Examples with your concept\n",
        "]\n",
        "my_negative_examples = [\n",
        "    # Examples without your concept\n",
        "]\n",
        "\n",
        "print(\"\\n2. Train linear probes at all layers\")\n",
        "# Use extract_hidden_states and train_linear_probe functions\n",
        "\n",
        "print(\"\\n3. Train MLP probes for comparison\")\n",
        "# Use train_mlp_probe function\n",
        "\n",
        "print(\"\\n4. Implement control tasks\")\n",
        "# Create selectivity and random label tests\n",
        "\n",
        "print(\"\\n5. Train learned masks with regularization\")\n",
        "# Try different λ values, find best tradeoff\n",
        "\n",
        "print(\"\\n6. Compare with Week 4 intervention results\")\n",
        "# Do probe/mask findings match intervention findings?\n",
        "\n",
        "print(\"\\n7. Analyze disagreements\")\n",
        "# What do they reveal about your concept's representation?\n",
        "\n",
        "print(\"\\n8. Document findings\")\n",
        "# Create visualizations and write report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this exercise, you've learned:\n",
        "- How to train and interpret linear probes\n",
        "- When to use MLP probes (nonlinear patterns)\n",
        "- How to validate probes with control tasks\n",
        "- How to diagnose overfitting and underfitting\n",
        "- How to train learned masks with sparse regularization\n",
        "- How to compare auxiliary models with causal interventions\n",
        "- What agreements and disagreements reveal\n",
        "\n",
        "Key takeaways:\n",
        "- **Probes show what could be extracted, not what is used**\n",
        "- **Always validate with interventions**\n",
        "- **Disagreements are informative**\n",
        "- **Use multiple methods for triangulation**\n",
        "\n",
        "For your project:\n",
        "1. Systematically probe all layers\n",
        "2. Use control tasks to validate findings\n",
        "3. Train masks to identify important components\n",
        "4. Compare with Week 4/5 intervention results\n",
        "5. Interpret what the comparison reveals about representation structure"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
