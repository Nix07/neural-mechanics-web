{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Pun Evaluation Testing with Multi-Model Verification\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week3/pun_evaluation_testing.ipynb)\n",
    "\n",
    "This notebook builds on `pun_dataset_builder.ipynb` to:\n",
    "1. **Validate** generated pun examples using cross-model verification\n",
    "2. **Distill** into multiple unambiguous test formats\n",
    "3. **Evaluate** across multiple models of varying sizes\n",
    "4. **Analyze** results by model size and family\n",
    "\n",
    "**Key Concepts Demonstrated:**\n",
    "- **Cross-model validation:** Use different models to validate LLM-generated data\n",
    "- **Multi-format evaluation:** Same concept tested in different task formats\n",
    "- **Scaling analysis:** How does concept understanding vary with model size?\n",
    "\n",
    "**Supports:** Anthropic Claude, OpenAI GPT, Google Gemini, Together.ai, and Groq APIs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, run `pun_dataset_builder.ipynb` to generate `pun_dataset.json`.\n",
    "\n",
    "## References\n",
    "- [Model-Written Evaluations](https://arxiv.org/abs/2212.09251) - Perez et al.\n",
    "- [LLM-as-Judge](https://arxiv.org/abs/2306.05685) - Zheng et al.\n",
    "- [LAMA: Language Models as Knowledge Bases](https://arxiv.org/abs/1909.01066)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-title",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - uncomment the providers you plan to use\n",
    "!pip install -q pandas matplotlib seaborn\n",
    "\n",
    "# Provider SDKs (install whichever you have API keys for)\n",
    "!pip install -q anthropic        # Anthropic Claude\n",
    "!pip install -q openai           # OpenAI GPT\n",
    "!pip install -q google-generativeai  # Google Gemini (free tier!)\n",
    "!pip install -q together         # Together.ai (multi-model)\n",
    "!pip install -q groq             # Groq (free tier, fast!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict, field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Set your API key(s) - use whichever provider(s) you have access to\n",
    "# You can set these as environment variables or paste directly (less secure)\n",
    "\n",
    "# Option 1: Anthropic Claude\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 2: OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 3: Google Gemini (FREE tier available!)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 4: Together.ai (good for multi-model testing)\n",
    "# os.environ[\"TOGETHER_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 5: Groq (FREE tier, very fast!)\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-llm-interface-title",
   "metadata": {},
   "source": [
    "## Unified LLM Interface\n",
    "\n",
    "We create a wrapper that works with any of the supported providers. This lets you use whichever API you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-llm-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"Unified interface for multiple LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"gemini\", model: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize with a provider: 'anthropic', 'openai', 'gemini', 'together', or 'groq'\n",
    "        \n",
    "        Args:\n",
    "            provider: The API provider to use\n",
    "            model: Optional model override (uses sensible defaults if not specified)\n",
    "        \"\"\"\n",
    "        self.provider = provider.lower()\n",
    "        \n",
    "        if self.provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            self.client = Anthropic()\n",
    "            self.model = model or \"claude-sonnet-4-20250514\"\n",
    "            \n",
    "        elif self.provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.model = model or \"gpt-4o\"\n",
    "            \n",
    "        elif self.provider == \"gemini\":\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "            self.model = model or \"gemini-1.5-flash\"\n",
    "            self.client = genai.GenerativeModel(self.model)\n",
    "            \n",
    "        elif self.provider == \"together\":\n",
    "            import together\n",
    "            self.client = together.Together()\n",
    "            self.model = model or \"meta-llama/Llama-3.1-70B-Instruct-Turbo\"\n",
    "            \n",
    "        elif self.provider == \"groq\":\n",
    "            from groq import Groq\n",
    "            self.client = Groq()\n",
    "            self.model = model or \"llama-3.1-70b-versatile\"\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {provider}. Choose from: anthropic, openai, gemini, together, groq\")\n",
    "    \n",
    "    def generate(self, prompt: str, system: str = \"\", max_tokens: int = 1024, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Generate a response from the LLM.\"\"\"\n",
    "        \n",
    "        if self.provider == \"anthropic\":\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                system=system if system else \"You are a helpful assistant.\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif self.provider == \"openai\":\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"gemini\":\n",
    "            full_prompt = f\"{system}\\n\\n{prompt}\" if system else prompt\n",
    "            response = self.client.generate_content(\n",
    "                full_prompt,\n",
    "                generation_config={\"max_output_tokens\": max_tokens, \"temperature\": temperature}\n",
    "            )\n",
    "            return response.text\n",
    "            \n",
    "        elif self.provider == \"together\":\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"groq\":\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-choose-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHOOSE YOUR PROVIDER HERE!\n",
    "# ============================================================\n",
    "# Options: \"anthropic\", \"openai\", \"gemini\", \"together\", \"groq\"\n",
    "#\n",
    "# Recommendations:\n",
    "#   - \"gemini\" : Free tier, good for validation (Part 2)\n",
    "#   - \"groq\"   : Free tier, very fast, good for validation\n",
    "#   - \"together\": Best for multi-model evaluation (Part 4)\n",
    "# ============================================================\n",
    "\n",
    "VALIDATION_PROVIDER = \"gemini\"  # Provider for cross-model validation\n",
    "EVAL_PROVIDER = \"together\"      # Provider for multi-model evaluation\n",
    "\n",
    "# Initialize the validation client\n",
    "llm = LLMClient(VALIDATION_PROVIDER)\n",
    "print(f\"Validation provider: {VALIDATION_PROVIDER}\")\n",
    "print(f\"Validation model: {llm.model}\")\n",
    "\n",
    "# Test the connection\n",
    "test_response = llm.generate(\"What is 2+2? Answer with just the number.\", max_tokens=10)\n",
    "print(f\"Test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset generated by pun_dataset_builder.ipynb\n",
    "try:\n",
    "    with open('pun_dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    print(f\"Loaded pun_dataset.json successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: pun_dataset.json not found.\")\n",
    "    print(\"Please run pun_dataset_builder.ipynb first to generate the dataset.\")\n",
    "    dataset = {'puns': [], 'pairs': [], 'cloze': []}\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"\\n=== Dataset Statistics ===\")\n",
    "print(f\"Puns: {len(dataset.get('puns', []))} examples\")\n",
    "print(f\"Literal/Pun Pairs: {len(dataset.get('pairs', []))} pairs\")\n",
    "print(f\"Cloze Examples: {len(dataset.get('cloze', []))} examples\")\n",
    "\n",
    "# Show sample pun\n",
    "if dataset.get('puns'):\n",
    "    sample = dataset['puns'][0]\n",
    "    print(f\"\\nSample pun:\")\n",
    "    print(f\"  {sample['setup']} {sample['punchline']}\")\n",
    "    print(f\"  Pun word: '{sample['pun_word']}' ({sample['meaning1']} / {sample['meaning2']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-validation-title",
   "metadata": {},
   "source": [
    "## Part 2: Cross-Model Validation\n",
    "\n",
    "A key insight from evaluation research: **use a different model or radically different prompt to validate generated examples**. This helps avoid self-confirmation bias where the same model that generated examples also validates them.\n",
    "\n",
    "We use the unified LLM interface so you can validate with any provider you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-validation-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of validating a pun example.\"\"\"\n",
    "    is_valid_pun: bool\n",
    "    double_meaning_works: bool\n",
    "    validator_explanation: str\n",
    "    pun_word_correct: bool\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "    \n",
    "def validate_pun(pun: Dict, llm_client: LLMClient) -> ValidationResult:\n",
    "    \"\"\"\n",
    "    Use a different LLM to validate a pun example.\n",
    "    \n",
    "    Key checks:\n",
    "    1. Is this actually a pun?\n",
    "    2. Does the claimed double meaning work?\n",
    "    3. Is the identified pun word correct?\n",
    "    \"\"\"\n",
    "    \n",
    "    full_joke = f\"{pun['setup']} {pun['punchline']}\"\n",
    "    \n",
    "    validation_prompt = f\"\"\"Analyze this joke and answer the following questions:\n",
    "\n",
    "Joke: \"{full_joke}\"\n",
    "Claimed pun word: \"{pun['pun_word']}\"\n",
    "Claimed meaning 1: \"{pun['meaning1']}\"\n",
    "Claimed meaning 2: \"{pun['meaning2']}\"\n",
    "\n",
    "Questions:\n",
    "1. Is this actually a pun (a joke that plays on words with multiple meanings)? Answer YES or NO.\n",
    "2. Does the word \"{pun['pun_word']}\" actually have both meanings claimed? Answer YES or NO.\n",
    "3. Is \"{pun['pun_word']}\" the correct word that creates the pun, or is it a different word? Answer CORRECT or WRONG.\n",
    "4. How confident are you in your assessment? Answer HIGH, MEDIUM, or LOW.\n",
    "5. Briefly explain the pun in your own words (1-2 sentences).\n",
    "\n",
    "Format your response exactly as:\n",
    "IS_PUN: [YES/NO]\n",
    "MEANINGS_WORK: [YES/NO]\n",
    "WORD_CORRECT: [CORRECT/WRONG]\n",
    "CONFIDENCE: [HIGH/MEDIUM/LOW]\n",
    "EXPLANATION: [your explanation]\"\"\"\n",
    "    \n",
    "    response = llm_client.generate(validation_prompt, max_tokens=300)\n",
    "    \n",
    "    # Parse the structured response\n",
    "    lines = response.strip().split('\\n')\n",
    "    result = {\n",
    "        'is_pun': False,\n",
    "        'meanings_work': False,\n",
    "        'word_correct': False,\n",
    "        'confidence': 'low',\n",
    "        'explanation': ''\n",
    "    }\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('IS_PUN:'):\n",
    "            result['is_pun'] = 'YES' in line.upper()\n",
    "        elif line.startswith('MEANINGS_WORK:'):\n",
    "            result['meanings_work'] = 'YES' in line.upper()\n",
    "        elif line.startswith('WORD_CORRECT:'):\n",
    "            result['word_correct'] = 'CORRECT' in line.upper()\n",
    "        elif line.startswith('CONFIDENCE:'):\n",
    "            conf = line.split(':')[1].strip().lower()\n",
    "            result['confidence'] = conf if conf in ['high', 'medium', 'low'] else 'medium'\n",
    "        elif line.startswith('EXPLANATION:'):\n",
    "            result['explanation'] = line.split(':', 1)[1].strip()\n",
    "    \n",
    "    return ValidationResult(\n",
    "        is_valid_pun=result['is_pun'],\n",
    "        double_meaning_works=result['meanings_work'],\n",
    "        validator_explanation=result['explanation'],\n",
    "        pun_word_correct=result['word_correct'],\n",
    "        confidence=result['confidence']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all puns in the dataset\n",
    "# Note: This makes API calls, so we add rate limiting\n",
    "\n",
    "validated_puns = []\n",
    "validation_stats = {'valid': 0, 'invalid': 0, 'flagged': 0}\n",
    "\n",
    "# Rate limits vary by provider\n",
    "RATE_LIMITS = {\n",
    "    'gemini': 0.5,    # ~15 RPM on free tier\n",
    "    'groq': 0.3,      # Fast, but has limits\n",
    "    'together': 0.5,\n",
    "    'anthropic': 0.2,\n",
    "    'openai': 0.2,\n",
    "}\n",
    "sleep_time = RATE_LIMITS.get(VALIDATION_PROVIDER, 0.5)\n",
    "\n",
    "print(f\"Validating puns using {VALIDATION_PROVIDER}...\\n\")\n",
    "\n",
    "for i, pun in enumerate(dataset.get('puns', [])):\n",
    "    # Rate limiting\n",
    "    if i > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    try:\n",
    "        result = validate_pun(pun, llm)\n",
    "        \n",
    "        # Determine if pun passes validation\n",
    "        passes = result.is_valid_pun and result.double_meaning_works and result.pun_word_correct\n",
    "        \n",
    "        validated_pun = {\n",
    "            **pun,\n",
    "            'validation': asdict(result),\n",
    "            'passes_validation': passes\n",
    "        }\n",
    "        validated_puns.append(validated_pun)\n",
    "        \n",
    "        status = \"VALID\" if passes else \"FLAGGED\"\n",
    "        print(f\"{i+1}. {status}: {pun['setup'][:40]}...\")\n",
    "        print(f\"   Validator says: {result.validator_explanation[:80]}...\")\n",
    "        \n",
    "        if passes:\n",
    "            validation_stats['valid'] += 1\n",
    "        else:\n",
    "            validation_stats['flagged'] += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{i+1}. ERROR: {str(e)[:50]}\")\n",
    "        validation_stats['invalid'] += 1\n",
    "\n",
    "print(f\"\\n=== Validation Summary ===\")\n",
    "print(f\"Valid: {validation_stats['valid']}\")\n",
    "print(f\"Flagged: {validation_stats['flagged']}\")\n",
    "print(f\"Errors: {validation_stats['invalid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-filter-validated",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only validated puns for downstream tasks\n",
    "filtered_puns = [p for p in validated_puns if p['passes_validation']]\n",
    "\n",
    "print(f\"Keeping {len(filtered_puns)}/{len(validated_puns)} puns that passed validation\")\n",
    "\n",
    "# Show examples of filtered vs kept\n",
    "if len(validated_puns) > len(filtered_puns):\n",
    "    print(\"\\nExamples of filtered-out puns:\")\n",
    "    for p in validated_puns:\n",
    "        if not p['passes_validation']:\n",
    "            print(f\"  - {p['setup']} {p['punchline']}\")\n",
    "            print(f\"    Reason: {p['validation']}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-formats-title",
   "metadata": {},
   "source": [
    "## Part 3: Distill to Multiple Test Formats\n",
    "\n",
    "We convert validated puns into multiple evaluation formats. Each format tests a different aspect of pun understanding:\n",
    "\n",
    "- **Format A (Binary Classification):** Can the model recognize a pun?\n",
    "- **Format B (Forced Choice):** Can the model distinguish pun from literal usage?\n",
    "- **Format C (Cloze Completion):** Can the model predict the pun word from context?\n",
    "- **Format D (Word Identification):** Can the model identify which word creates the pun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-format-a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BinaryClassificationExample:\n",
    "    \"\"\"Format A: Binary pun/not-pun classification.\"\"\"\n",
    "    text: str\n",
    "    is_pun: bool\n",
    "    source_type: str  # \"pun\" or \"literal_control\"\n",
    "    pun_word: Optional[str] = None\n",
    "\n",
    "def create_binary_classification_dataset(puns: List[Dict], pairs: List[Dict]) -> List[BinaryClassificationExample]:\n",
    "    \"\"\"\n",
    "    Create binary classification examples.\n",
    "    \n",
    "    Includes:\n",
    "    - Pun examples (positive class)\n",
    "    - Matched literal sentences (negative class / controls)\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Add puns as positive examples\n",
    "    for pun in puns:\n",
    "        examples.append(BinaryClassificationExample(\n",
    "            text=f\"{pun['setup']} {pun['punchline']}\",\n",
    "            is_pun=True,\n",
    "            source_type=\"pun\",\n",
    "            pun_word=pun['pun_word']\n",
    "        ))\n",
    "    \n",
    "    # Add literal contexts as negative examples (from pairs)\n",
    "    for pair in pairs:\n",
    "        examples.append(BinaryClassificationExample(\n",
    "            text=pair['literal_context1'],\n",
    "            is_pun=False,\n",
    "            source_type=\"literal_control\",\n",
    "            pun_word=pair['target_word']\n",
    "        ))\n",
    "        examples.append(BinaryClassificationExample(\n",
    "            text=pair['literal_context2'],\n",
    "            is_pun=False,\n",
    "            source_type=\"literal_control\",\n",
    "            pun_word=pair['target_word']\n",
    "        ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "binary_dataset = create_binary_classification_dataset(\n",
    "    filtered_puns if filtered_puns else dataset.get('puns', []),\n",
    "    dataset.get('pairs', [])\n",
    ")\n",
    "\n",
    "print(f\"Binary Classification Dataset: {len(binary_dataset)} examples\")\n",
    "print(f\"  Puns: {sum(1 for ex in binary_dataset if ex.is_pun)}\")\n",
    "print(f\"  Non-puns: {sum(1 for ex in binary_dataset if not ex.is_pun)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nSample positive (pun):\")\n",
    "pos = next((ex for ex in binary_dataset if ex.is_pun), None)\n",
    "if pos:\n",
    "    print(f\"  Text: {pos.text}\")\n",
    "    print(f\"  Label: is_pun = True\")\n",
    "\n",
    "print(\"\\nSample negative (literal):\")\n",
    "neg = next((ex for ex in binary_dataset if not ex.is_pun), None)\n",
    "if neg:\n",
    "    print(f\"  Text: {neg.text}\")\n",
    "    print(f\"  Label: is_pun = False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-format-b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ForcedChoiceExample:\n",
    "    \"\"\"Format B: Forced choice between pun and literal sentence.\"\"\"\n",
    "    option_a: str\n",
    "    option_b: str\n",
    "    correct_answer: str  # \"A\" or \"B\"\n",
    "    target_word: str\n",
    "    pun_position: str  # \"A\" or \"B\" (for position bias analysis)\n",
    "\n",
    "def create_forced_choice_dataset(pairs: List[Dict]) -> List[ForcedChoiceExample]:\n",
    "    \"\"\"\n",
    "    Create forced choice (pairwise) examples.\n",
    "    \n",
    "    Uses matched pairs from the dataset (same word, pun vs literal usage).\n",
    "    Randomizes A/B position to control for position bias.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        # Randomly assign pun to A or B\n",
    "        pun_first = random.random() < 0.5\n",
    "        \n",
    "        if pun_first:\n",
    "            examples.append(ForcedChoiceExample(\n",
    "                option_a=pair['pun_context'],\n",
    "                option_b=pair['literal_context1'],\n",
    "                correct_answer=\"A\",\n",
    "                target_word=pair['target_word'],\n",
    "                pun_position=\"A\"\n",
    "            ))\n",
    "        else:\n",
    "            examples.append(ForcedChoiceExample(\n",
    "                option_a=pair['literal_context1'],\n",
    "                option_b=pair['pun_context'],\n",
    "                correct_answer=\"B\",\n",
    "                target_word=pair['target_word'],\n",
    "                pun_position=\"B\"\n",
    "            ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "forced_choice_dataset = create_forced_choice_dataset(dataset.get('pairs', []))\n",
    "\n",
    "print(f\"Forced Choice Dataset: {len(forced_choice_dataset)} examples\")\n",
    "\n",
    "# Check position balance\n",
    "pos_a = sum(1 for ex in forced_choice_dataset if ex.pun_position == \"A\")\n",
    "print(f\"  Pun in position A: {pos_a}\")\n",
    "print(f\"  Pun in position B: {len(forced_choice_dataset) - pos_a}\")\n",
    "\n",
    "# Show example\n",
    "if forced_choice_dataset:\n",
    "    ex = forced_choice_dataset[0]\n",
    "    print(f\"\\nSample forced choice:\")\n",
    "    print(f\"  Which sentence contains a pun?\")\n",
    "    print(f\"  A: {ex.option_a}\")\n",
    "    print(f\"  B: {ex.option_b}\")\n",
    "    print(f\"  Correct: {ex.correct_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-format-c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClozeCompletionExample:\n",
    "    \"\"\"Format C: Cloze (fill-in-the-blank) completion.\"\"\"\n",
    "    prompt_template: str  # \"This joke works because '___' can mean both {meaning1} and {meaning2}.\"\n",
    "    full_prompt: str  # The complete prompt with context\n",
    "    target_word: str\n",
    "    foils: List[str]  # Distractor words\n",
    "    context: str  # The original joke\n",
    "\n",
    "def create_cloze_dataset(puns: List[Dict]) -> List[ClozeCompletionExample]:\n",
    "    \"\"\"\n",
    "    Create cloze completion examples for LAMA-style probing.\n",
    "    \n",
    "    Format: \"This joke works because '___' can mean both [meaning1] and [meaning2].\"\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for pun in puns:\n",
    "        full_joke = f\"{pun['setup']} {pun['punchline']}\"\n",
    "        \n",
    "        # Create the cloze prompt\n",
    "        template = f\"This joke works because '___' can mean both {pun['meaning1']} and {pun['meaning2']}.\"\n",
    "        full_prompt = f\"Joke: \\\"{full_joke}\\\"\\n\\n{template}\"\n",
    "        \n",
    "        # Generate simple foils (we could use LLM for better foils)\n",
    "        # For now, use other pun words as foils\n",
    "        other_words = [p['pun_word'] for p in puns if p['pun_word'] != pun['pun_word']]\n",
    "        foils = random.sample(other_words, min(3, len(other_words))) if other_words else []\n",
    "        \n",
    "        examples.append(ClozeCompletionExample(\n",
    "            prompt_template=template,\n",
    "            full_prompt=full_prompt,\n",
    "            target_word=pun['pun_word'],\n",
    "            foils=foils,\n",
    "            context=full_joke\n",
    "        ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "cloze_dataset = create_cloze_dataset(\n",
    "    filtered_puns if filtered_puns else dataset.get('puns', [])\n",
    ")\n",
    "\n",
    "print(f\"Cloze Completion Dataset: {len(cloze_dataset)} examples\")\n",
    "\n",
    "# Show example\n",
    "if cloze_dataset:\n",
    "    ex = cloze_dataset[0]\n",
    "    print(f\"\\nSample cloze:\")\n",
    "    print(f\"  {ex.full_prompt}\")\n",
    "    print(f\"  Target: {ex.target_word}\")\n",
    "    print(f\"  Foils: {ex.foils}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-format-d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WordIdentificationExample:\n",
    "    \"\"\"Format D: Identify which word creates the pun.\"\"\"\n",
    "    joke: str\n",
    "    options: List[str]  # Multiple choice options\n",
    "    correct_index: int  # Index of correct answer (0-based)\n",
    "    correct_word: str\n",
    "\n",
    "def create_word_identification_dataset(puns: List[Dict]) -> List[WordIdentificationExample]:\n",
    "    \"\"\"\n",
    "    Create word identification examples.\n",
    "    \n",
    "    Task: Given a pun, identify which word creates the double meaning.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for pun in puns:\n",
    "        full_joke = f\"{pun['setup']} {pun['punchline']}\"\n",
    "        pun_word = pun['pun_word'].lower()\n",
    "        \n",
    "        # Extract words from the joke for options\n",
    "        words = [w.strip('.,!?\"') for w in full_joke.split()]\n",
    "        words = [w for w in words if len(w) > 2]  # Filter short words\n",
    "        \n",
    "        # Make sure pun_word is in options\n",
    "        if pun_word not in [w.lower() for w in words]:\n",
    "            continue  # Skip if pun word not found\n",
    "        \n",
    "        # Select distractors (other words from the joke)\n",
    "        distractors = [w for w in words if w.lower() != pun_word][:3]\n",
    "        \n",
    "        if len(distractors) < 3:\n",
    "            continue  # Need at least 3 distractors\n",
    "        \n",
    "        # Build options list with pun word at random position\n",
    "        options = distractors[:3]\n",
    "        correct_index = random.randint(0, 3)\n",
    "        options.insert(correct_index, pun['pun_word'])\n",
    "        \n",
    "        examples.append(WordIdentificationExample(\n",
    "            joke=full_joke,\n",
    "            options=options,\n",
    "            correct_index=correct_index,\n",
    "            correct_word=pun['pun_word']\n",
    "        ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "word_id_dataset = create_word_identification_dataset(\n",
    "    filtered_puns if filtered_puns else dataset.get('puns', [])\n",
    ")\n",
    "\n",
    "print(f\"Word Identification Dataset: {len(word_id_dataset)} examples\")\n",
    "\n",
    "# Show example\n",
    "if word_id_dataset:\n",
    "    ex = word_id_dataset[0]\n",
    "    print(f\"\\nSample word identification:\")\n",
    "    print(f\"  Joke: {ex.joke}\")\n",
    "    print(f\"  Which word creates the pun?\")\n",
    "    for i, opt in enumerate(ex.options):\n",
    "        marker = \" <--\" if i == ex.correct_index else \"\"\n",
    "        print(f\"    {chr(65+i)}: {opt}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all test formats\n",
    "print(\"=== Test Format Summary ===\")\n",
    "print(f\"\\nFormat A (Binary Classification): {len(binary_dataset)} examples\")\n",
    "print(f\"  Task: Is this a pun? Yes/No\")\n",
    "print(f\"\\nFormat B (Forced Choice): {len(forced_choice_dataset)} examples\")\n",
    "print(f\"  Task: Which sentence contains a pun? A/B\")\n",
    "print(f\"\\nFormat C (Cloze Completion): {len(cloze_dataset)} examples\")\n",
    "print(f\"  Task: Fill in the blank with the pun word\")\n",
    "print(f\"\\nFormat D (Word Identification): {len(word_id_dataset)} examples\")\n",
    "print(f\"  Task: Which word creates the pun? A/B/C/D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-evaluation-title",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Model Testing\n",
    "\n",
    "Now we evaluate multiple models on each test format. We test models across different:\n",
    "- **Sizes:** 7B, 8B, 9B --> 27B, 70B, 72B\n",
    "- **Families:** Llama, Mistral, Qwen, Gemma\n",
    "\n",
    "**Provider options for multi-model testing:**\n",
    "- **Together.ai**: Best selection of models in one API\n",
    "- **Groq**: Free, fast, but fewer models (Llama, Mistral, Gemma)\n",
    "- **Mix providers**: Use Gemini + Groq + others for variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model registry for different providers\n",
    "# Each entry: (provider, model_id, display_name, family, size_b)\n",
    "\n",
    "TOGETHER_MODELS = {\n",
    "    \"llama-8b\": (\"together\", \"meta-llama/Llama-3.1-8B-Instruct-Turbo\", \"llama\", 8),\n",
    "    \"llama-70b\": (\"together\", \"meta-llama/Llama-3.1-70B-Instruct-Turbo\", \"llama\", 70),\n",
    "    \"mistral-7b\": (\"together\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"mistral\", 7),\n",
    "    \"mixtral-8x7b\": (\"together\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"mistral\", 56),\n",
    "    \"qwen-7b\": (\"together\", \"Qwen/Qwen2.5-7B-Instruct-Turbo\", \"qwen\", 7),\n",
    "    \"qwen-72b\": (\"together\", \"Qwen/Qwen2.5-72B-Instruct-Turbo\", \"qwen\", 72),\n",
    "    \"gemma-9b\": (\"together\", \"google/gemma-2-9b-it\", \"gemma\", 9),\n",
    "    \"gemma-27b\": (\"together\", \"google/gemma-2-27b-it\", \"gemma\", 27),\n",
    "}\n",
    "\n",
    "GROQ_MODELS = {\n",
    "    \"llama-8b\": (\"groq\", \"llama-3.1-8b-instant\", \"llama\", 8),\n",
    "    \"llama-70b\": (\"groq\", \"llama-3.1-70b-versatile\", \"llama\", 70),\n",
    "    \"mixtral-8x7b\": (\"groq\", \"mixtral-8x7b-32768\", \"mistral\", 56),\n",
    "    \"gemma-9b\": (\"groq\", \"gemma2-9b-it\", \"gemma\", 9),\n",
    "}\n",
    "\n",
    "GEMINI_MODELS = {\n",
    "    \"gemini-flash\": (\"gemini\", \"gemini-1.5-flash\", \"gemini\", 0),  # Size unknown\n",
    "    \"gemini-pro\": (\"gemini\", \"gemini-1.5-pro\", \"gemini\", 0),\n",
    "}\n",
    "\n",
    "# Select which model set to use based on your provider\n",
    "if EVAL_PROVIDER == \"together\":\n",
    "    EVAL_MODELS = TOGETHER_MODELS\n",
    "elif EVAL_PROVIDER == \"groq\":\n",
    "    EVAL_MODELS = GROQ_MODELS\n",
    "elif EVAL_PROVIDER == \"gemini\":\n",
    "    EVAL_MODELS = GEMINI_MODELS\n",
    "else:\n",
    "    # Default to groq (free)\n",
    "    EVAL_MODELS = GROQ_MODELS\n",
    "\n",
    "print(f\"Evaluation provider: {EVAL_PROVIDER}\")\n",
    "print(f\"Available models: {len(EVAL_MODELS)}\")\n",
    "for name, (provider, model_id, family, size) in EVAL_MODELS.items():\n",
    "    size_str = f\"{size}B\" if size > 0 else \"unknown\"\n",
    "    print(f\"  {name}: {family} family, {size_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-multi-model-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelEvaluator:\n",
    "    \"\"\"Evaluate multiple models using the unified LLM interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, models: Dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models: Dict mapping name -> (provider, model_id, family, size_b)\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.clients = {}  # Lazy initialization\n",
    "        \n",
    "    def get_client(self, model_name: str) -> LLMClient:\n",
    "        \"\"\"Get or create a client for the given model.\"\"\"\n",
    "        if model_name not in self.clients:\n",
    "            provider, model_id, _, _ = self.models[model_name]\n",
    "            self.clients[model_name] = LLMClient(provider, model=model_id)\n",
    "        return self.clients[model_name]\n",
    "    \n",
    "    def evaluate_binary(self, model_name: str, examples: List[BinaryClassificationExample],\n",
    "                       max_examples: int = 20) -> Dict:\n",
    "        \"\"\"Evaluate binary pun classification.\"\"\"\n",
    "        client = self.get_client(model_name)\n",
    "        results = []\n",
    "        correct = 0\n",
    "        \n",
    "        eval_examples = random.sample(examples, min(max_examples, len(examples)))\n",
    "        \n",
    "        for ex in eval_examples:\n",
    "            prompt = f\"\"\"Does this text contain a pun (a joke based on a word with two meanings)?\n",
    "\n",
    "Text: \"{ex.text}\"\n",
    "\n",
    "Answer with just Yes or No.\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.generate(prompt, max_tokens=10)\n",
    "                response_lower = response.strip().lower()\n",
    "                predicted_pun = 'yes' in response_lower and 'no' not in response_lower[:3]\n",
    "                \n",
    "                is_correct = predicted_pun == ex.is_pun\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'text': ex.text[:50],\n",
    "                    'true_label': ex.is_pun,\n",
    "                    'predicted': predicted_pun,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({'error': str(e), 'correct': False})\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'format': 'binary',\n",
    "            'accuracy': correct / len(eval_examples) if eval_examples else 0,\n",
    "            'correct': correct,\n",
    "            'total': len(eval_examples),\n",
    "            'results': results\n",
    "        }\n",
    "    \n",
    "    def evaluate_forced_choice(self, model_name: str, examples: List[ForcedChoiceExample],\n",
    "                               max_examples: int = 20) -> Dict:\n",
    "        \"\"\"Evaluate forced choice pun identification.\"\"\"\n",
    "        client = self.get_client(model_name)\n",
    "        results = []\n",
    "        correct = 0\n",
    "        \n",
    "        eval_examples = random.sample(examples, min(max_examples, len(examples)))\n",
    "        \n",
    "        for ex in eval_examples:\n",
    "            prompt = f\"\"\"Which sentence contains a pun?\n",
    "\n",
    "A: {ex.option_a}\n",
    "B: {ex.option_b}\n",
    "\n",
    "Answer with just A or B.\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.generate(prompt, max_tokens=10)\n",
    "                response_upper = response.strip().upper()\n",
    "                \n",
    "                if 'A' in response_upper and 'B' not in response_upper:\n",
    "                    predicted = 'A'\n",
    "                elif 'B' in response_upper and 'A' not in response_upper:\n",
    "                    predicted = 'B'\n",
    "                elif response_upper.startswith('A'):\n",
    "                    predicted = 'A'\n",
    "                elif response_upper.startswith('B'):\n",
    "                    predicted = 'B'\n",
    "                else:\n",
    "                    predicted = response_upper[0] if response_upper else None\n",
    "                \n",
    "                is_correct = predicted == ex.correct_answer\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'correct_answer': ex.correct_answer,\n",
    "                    'predicted': predicted,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({'error': str(e), 'correct': False})\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'format': 'forced_choice',\n",
    "            'accuracy': correct / len(eval_examples) if eval_examples else 0,\n",
    "            'correct': correct,\n",
    "            'total': len(eval_examples),\n",
    "            'results': results\n",
    "        }\n",
    "    \n",
    "    def evaluate_word_identification(self, model_name: str, examples: List[WordIdentificationExample],\n",
    "                                     max_examples: int = 20) -> Dict:\n",
    "        \"\"\"Evaluate pun word identification.\"\"\"\n",
    "        client = self.get_client(model_name)\n",
    "        results = []\n",
    "        correct = 0\n",
    "        \n",
    "        eval_examples = random.sample(examples, min(max_examples, len(examples)))\n",
    "        \n",
    "        for ex in eval_examples:\n",
    "            options_str = \"\\n\".join([f\"{chr(65+i)}: {opt}\" for i, opt in enumerate(ex.options)])\n",
    "            \n",
    "            prompt = f\"\"\"Which word creates the pun in this joke?\n",
    "\n",
    "Joke: \"{ex.joke}\"\n",
    "\n",
    "{options_str}\n",
    "\n",
    "Answer with just the letter (A, B, C, or D).\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = client.generate(prompt, max_tokens=10)\n",
    "                response_upper = response.strip().upper()\n",
    "                \n",
    "                predicted_letter = None\n",
    "                for letter in ['A', 'B', 'C', 'D']:\n",
    "                    if letter in response_upper:\n",
    "                        predicted_letter = letter\n",
    "                        break\n",
    "                \n",
    "                if predicted_letter:\n",
    "                    predicted_index = ord(predicted_letter) - ord('A')\n",
    "                    is_correct = predicted_index == ex.correct_index\n",
    "                else:\n",
    "                    is_correct = False\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'correct_word': ex.correct_word,\n",
    "                    'predicted_letter': predicted_letter,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({'error': str(e), 'correct': False})\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'format': 'word_identification',\n",
    "            'accuracy': correct / len(eval_examples) if eval_examples else 0,\n",
    "            'correct': correct,\n",
    "            'total': len(eval_examples),\n",
    "            'results': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation across models\n",
    "# Note: Adjust which models to test based on your API access and budget\n",
    "\n",
    "evaluator = MultiModelEvaluator(EVAL_MODELS)\n",
    "all_results = []\n",
    "\n",
    "# Select a subset of models for faster evaluation\n",
    "# Modify this list based on what you want to test\n",
    "MODELS_TO_TEST = list(EVAL_MODELS.keys())[:2]  # Start with first 2 models\n",
    "\n",
    "print(\"Running multi-model evaluation...\")\n",
    "print(f\"Models: {MODELS_TO_TEST}\")\n",
    "print(f\"Formats: binary, forced_choice, word_identification\")\n",
    "print()\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Binary classification\n",
    "    if binary_dataset:\n",
    "        print(f\"  Format A (Binary)...\", end=\" \")\n",
    "        result = evaluator.evaluate_binary(model_name, binary_dataset, max_examples=10)\n",
    "        all_results.append(result)\n",
    "        print(f\"Accuracy: {result['accuracy']:.1%}\")\n",
    "    \n",
    "    # Forced choice\n",
    "    if forced_choice_dataset:\n",
    "        print(f\"  Format B (Forced Choice)...\", end=\" \")\n",
    "        result = evaluator.evaluate_forced_choice(model_name, forced_choice_dataset, max_examples=10)\n",
    "        all_results.append(result)\n",
    "        print(f\"Accuracy: {result['accuracy']:.1%}\")\n",
    "    \n",
    "    # Word identification\n",
    "    if word_id_dataset:\n",
    "        print(f\"  Format D (Word ID)...\", end=\" \")\n",
    "        result = evaluator.evaluate_word_identification(model_name, word_id_dataset, max_examples=10)\n",
    "        all_results.append(result)\n",
    "        print(f\"Accuracy: {result['accuracy']:.1%}\")\n",
    "\n",
    "print(\"\\n=== Evaluation Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-analysis-title",
   "metadata": {},
   "source": [
    "## Part 5: Results Analysis\n",
    "\n",
    "Now we analyze the results to understand:\n",
    "1. How accuracy varies with model size\n",
    "2. Differences between model families\n",
    "3. Which puns are hardest\n",
    "4. Whether different formats correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "def get_model_info(model_name: str) -> Tuple[str, int]:\n",
    "    \"\"\"Get family and size for a model.\"\"\"\n",
    "    if model_name in EVAL_MODELS:\n",
    "        _, _, family, size = EVAL_MODELS[model_name]\n",
    "        return family, size\n",
    "    return \"unknown\", 0\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'model': r['model'],\n",
    "        'format': r['format'],\n",
    "        'accuracy': r['accuracy'],\n",
    "        'correct': r['correct'],\n",
    "        'total': r['total'],\n",
    "        'family': get_model_info(r['model'])[0],\n",
    "        'size_b': get_model_info(r['model'])[1]\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Accuracy vs Model Size\n",
    "if len(results_df) > 0 and results_df['size_b'].max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Different colors for different formats\n",
    "    formats = results_df['format'].unique()\n",
    "    colors = plt.cm.Set2(range(len(formats)))\n",
    "    \n",
    "    for fmt, color in zip(formats, colors):\n",
    "        fmt_data = results_df[results_df['format'] == fmt]\n",
    "        ax.scatter(fmt_data['size_b'], fmt_data['accuracy'], \n",
    "                   c=[color], s=100, label=fmt, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Model Size (B parameters)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Pun Understanding vs Model Size', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance (binary)')\n",
    "    ax.axhline(y=0.25, color='gray', linestyle=':', alpha=0.5, label='Chance (4-way)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pun_accuracy_vs_size.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results with size info to plot. Run evaluation first or use a provider with size data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Accuracy by Model Family\n",
    "if len(results_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Pivot for grouped bar chart\n",
    "    pivot_df = results_df.pivot_table(\n",
    "        index='family', \n",
    "        columns='format', \n",
    "        values='accuracy',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    if len(pivot_df) > 0:\n",
    "        pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Model Family', fontsize=12)\n",
    "        ax.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax.set_title('Pun Understanding by Model Family', fontsize=14)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(title='Test Format')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pun_accuracy_by_family.png', dpi=150)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough data for family comparison.\")\n",
    "else:\n",
    "    print(\"No results to plot yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Which puns are hardest?\n",
    "def analyze_errors(results: List[Dict]) -> Dict:\n",
    "    \"\"\"Identify patterns in model errors.\"\"\"\n",
    "    \n",
    "    # Aggregate errors by example\n",
    "    example_errors = defaultdict(int)\n",
    "    example_totals = defaultdict(int)\n",
    "    \n",
    "    for result in results:\n",
    "        if result['format'] == 'binary' and 'results' in result:\n",
    "            for r in result['results']:\n",
    "                if 'text' in r:\n",
    "                    example_totals[r['text']] += 1\n",
    "                    if not r.get('correct', False):\n",
    "                        example_errors[r['text']] += 1\n",
    "    \n",
    "    # Calculate error rates\n",
    "    error_rates = {\n",
    "        text: example_errors[text] / example_totals[text]\n",
    "        for text in example_totals\n",
    "    }\n",
    "    \n",
    "    # Sort by error rate\n",
    "    hardest = sorted(error_rates.items(), key=lambda x: -x[1])[:5]\n",
    "    \n",
    "    return {\n",
    "        'hardest_examples': hardest,\n",
    "        'avg_error_rate': sum(error_rates.values()) / len(error_rates) if error_rates else 0\n",
    "    }\n",
    "\n",
    "if all_results:\n",
    "    error_analysis = analyze_errors(all_results)\n",
    "    \n",
    "    print(\"=== Error Analysis ===\")\n",
    "    print(f\"\\nAverage error rate: {error_analysis['avg_error_rate']:.1%}\")\n",
    "    print(f\"\\nHardest examples (highest error rate):\")\n",
    "    for text, rate in error_analysis['hardest_examples']:\n",
    "        print(f\"  {rate:.0%} error: {text}...\")\n",
    "else:\n",
    "    print(\"Run evaluation first to analyze errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-format-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format correlation: Do models that do well on one format do well on others?\n",
    "if len(results_df) > 0 and len(results_df['format'].unique()) > 1:\n",
    "    format_pivot = results_df.pivot_table(\n",
    "        index='model',\n",
    "        columns='format',\n",
    "        values='accuracy'\n",
    "    )\n",
    "    \n",
    "    if len(format_pivot.columns) > 1:\n",
    "        print(\"=== Format Correlation ===\")\n",
    "        print(\"\\nAccuracy by model and format:\")\n",
    "        display(format_pivot)\n",
    "        \n",
    "        print(\"\\nCorrelation between formats:\")\n",
    "        display(format_pivot.corr())\n",
    "else:\n",
    "    print(\"Need results from multiple formats to compute correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-export-title",
   "metadata": {},
   "source": [
    "## Part 6: Export Clean Dataset\n",
    "\n",
    "Export the validated and filtered dataset along with all test formats for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-export-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    'metadata': {\n",
    "        'source': 'pun_evaluation_testing.ipynb',\n",
    "        'validation_provider': VALIDATION_PROVIDER,\n",
    "        'eval_provider': EVAL_PROVIDER,\n",
    "        'validated_count': len(filtered_puns) if filtered_puns else len(dataset.get('puns', [])),\n",
    "        'formats': ['binary', 'forced_choice', 'cloze', 'word_identification']\n",
    "    },\n",
    "    'validated_puns': filtered_puns if filtered_puns else dataset.get('puns', []),\n",
    "    'test_formats': {\n",
    "        'binary_classification': [asdict(ex) for ex in binary_dataset],\n",
    "        'forced_choice': [asdict(ex) for ex in forced_choice_dataset],\n",
    "        'cloze_completion': [asdict(ex) for ex in cloze_dataset],\n",
    "        'word_identification': [asdict(ex) for ex in word_id_dataset]\n",
    "    },\n",
    "    'evaluation_results': all_results\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open('pun_evaluation_dataset.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "print(\"Saved pun_evaluation_dataset.json\")\n",
    "\n",
    "# Save results as CSV for easy analysis\n",
    "if len(results_df) > 0:\n",
    "    results_df.to_csv('pun_evaluation_results.csv', index=False)\n",
    "    print(\"Saved pun_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-export-formats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also export individual format datasets as CSVs\n",
    "if binary_dataset:\n",
    "    binary_df = pd.DataFrame([asdict(ex) for ex in binary_dataset])\n",
    "    binary_df.to_csv('pun_test_binary.csv', index=False)\n",
    "    print(f\"Saved pun_test_binary.csv ({len(binary_df)} examples)\")\n",
    "\n",
    "if forced_choice_dataset:\n",
    "    fc_df = pd.DataFrame([asdict(ex) for ex in forced_choice_dataset])\n",
    "    fc_df.to_csv('pun_test_forced_choice.csv', index=False)\n",
    "    print(f\"Saved pun_test_forced_choice.csv ({len(fc_df)} examples)\")\n",
    "\n",
    "if cloze_dataset:\n",
    "    cloze_df = pd.DataFrame([asdict(ex) for ex in cloze_dataset])\n",
    "    cloze_df.to_csv('pun_test_cloze.csv', index=False)\n",
    "    print(f\"Saved pun_test_cloze.csv ({len(cloze_df)} examples)\")\n",
    "\n",
    "if word_id_dataset:\n",
    "    word_df = pd.DataFrame([{\n",
    "        'joke': ex.joke,\n",
    "        'option_a': ex.options[0] if len(ex.options) > 0 else '',\n",
    "        'option_b': ex.options[1] if len(ex.options) > 1 else '',\n",
    "        'option_c': ex.options[2] if len(ex.options) > 2 else '',\n",
    "        'option_d': ex.options[3] if len(ex.options) > 3 else '',\n",
    "        'correct_index': ex.correct_index,\n",
    "        'correct_word': ex.correct_word\n",
    "    } for ex in word_id_dataset])\n",
    "    word_df.to_csv('pun_test_word_id.csv', index=False)\n",
    "    print(f\"Saved pun_test_word_id.csv ({len(word_df)} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-colab-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Zip all outputs\n",
    "    !zip -r pun_evaluation_outputs.zip pun_evaluation_*.json pun_evaluation_*.csv pun_test_*.csv pun_accuracy_*.png 2>/dev/null || true\n",
    "    files.download('pun_evaluation_outputs.zip')\n",
    "except ImportError:\n",
    "    print(\"Not in Colab - files saved to current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded** pun datasets from `pun_dataset_builder.ipynb`\n",
    "2. **Validated** examples using cross-model verification (with provider of your choice)\n",
    "3. **Created multiple test formats:**\n",
    "   - Binary classification (is this a pun?)\n",
    "   - Forced choice (which is the pun?)\n",
    "   - Cloze completion (fill in the pun word)\n",
    "   - Word identification (which word creates the pun?)\n",
    "4. **Evaluated** multiple models across different sizes and families\n",
    "5. **Analyzed** results by model size, family, and identified hardest examples\n",
    "\n",
    "### Provider Recommendations\n",
    "\n",
    "| Use Case | Recommended Provider | Why |\n",
    "|----------|---------------------|-----|\n",
    "| Validation (Part 2) | Gemini or Groq | Free tiers, capable models |\n",
    "| Multi-model eval (Part 4) | Together.ai | Best model variety |\n",
    "| Budget-friendly | Groq | Free, fast, good Llama models |\n",
    "| Best quality | Anthropic/OpenAI | Frontier models |\n",
    "\n",
    "### Connection to Course\n",
    "\n",
    "This notebook demonstrates Week 3 concepts:\n",
    "- **Model-Written Evals:** LLMs help create and validate evaluation data\n",
    "- **LLM-as-Judge:** Cross-validation prevents self-confirmation bias\n",
    "- **LAMA-style probing:** Cloze format tests knowledge extraction\n",
    "- **Scaling analysis:** How concept understanding varies with model size\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The exported datasets can be used in later weeks:\n",
    "- **Week 4:** Visualize pun word representations in embedding space\n",
    "- **Week 5:** Causal tracing to find where pun understanding happens\n",
    "- **Week 6:** Train probes to detect pun processing in hidden states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_minor_version": "5.3.0",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
