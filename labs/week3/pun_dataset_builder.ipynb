{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pun Evaluation Datasets with LLMs\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week3/pun_dataset_builder.ipynb)\n",
    "\n",
    "This notebook demonstrates **model-written evaluations**—using LLMs to help create evaluation datasets. We'll build datasets for studying how language models process puns, useful for both behavioral evaluation and interpretability experiments.\n",
    "\n",
    "**What we'll create:**\n",
    "1. Pun examples with controlled structure (setup → punchline)\n",
    "2. Matched literal/pun pairs for the same ambiguous words\n",
    "3. Cloze-style evaluation sets for probing\n",
    "4. Quality ratings using LLM-as-judge\n",
    "\n",
    "**Supports:** Anthropic Claude, OpenAI GPT, and Google Gemini APIs\n",
    "\n",
    "## References\n",
    "- [Model-Written Evaluations](https://arxiv.org/abs/2212.09251) - Perez et al.\n",
    "- [LLM-as-Judge](https://arxiv.org/abs/2306.05685) - Zheng et al.\n",
    "- [LAMA: Language Models as Knowledge Bases](https://arxiv.org/abs/1909.01066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the API clients you plan to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install whichever API client(s) you need\n",
    "!pip install -q anthropic openai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import pandas as pd\n",
    "\n",
    "# Set your API key(s) - use whichever provider you have access to\n",
    "# You can set these as environment variables or paste directly (less secure)\n",
    "\n",
    "# Option 1: Anthropic Claude\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 2: OpenAI\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Option 3: Google Gemini\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified LLM Interface\n",
    "\n",
    "We create a simple wrapper that works with any of the three providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"Unified interface for Anthropic, OpenAI, and Gemini APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"anthropic\"):\n",
    "        \"\"\"\n",
    "        Initialize with a provider: 'anthropic', 'openai', or 'gemini'\n",
    "        \"\"\"\n",
    "        self.provider = provider.lower()\n",
    "        \n",
    "        if self.provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            self.client = Anthropic()\n",
    "            self.model = \"claude-sonnet-4-20250514\"\n",
    "            \n",
    "        elif self.provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.model = \"gpt-4o\"\n",
    "            \n",
    "        elif self.provider == \"gemini\":\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "            self.client = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "            self.model = \"gemini-1.5-pro\"\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {provider}\")\n",
    "    \n",
    "    def generate(self, prompt: str, system: str = \"\", max_tokens: int = 1024) -> str:\n",
    "        \"\"\"Generate a response from the LLM.\"\"\"\n",
    "        \n",
    "        if self.provider == \"anthropic\":\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                system=system if system else \"You are a helpful assistant.\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif self.provider == \"openai\":\n",
    "            messages = []\n",
    "            if system:\n",
    "                messages.append({\"role\": \"system\", \"content\": system})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"gemini\":\n",
    "            full_prompt = f\"{system}\\n\\n{prompt}\" if system else prompt\n",
    "            response = self.client.generate_content(full_prompt)\n",
    "            return response.text\n",
    "\n",
    "# Choose your provider here!\n",
    "PROVIDER = \"anthropic\"  # or \"openai\" or \"gemini\"\n",
    "llm = LLMClient(PROVIDER)\n",
    "print(f\"Using {PROVIDER} with model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "test_response = llm.generate(\"What do you call a fish without eyes? (Give just the punchline)\")\n",
    "print(f\"Test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generating Pun Examples\n",
    "\n",
    "Let's generate puns with controlled structure. We'll ask the LLM to create puns that follow a specific format, making them easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PunExample:\n",
    "    \"\"\"A structured pun example.\"\"\"\n",
    "    setup: str\n",
    "    punchline: str\n",
    "    pun_word: str\n",
    "    meaning1: str  # First meaning (usually literal/expected)\n",
    "    meaning2: str  # Second meaning (the joke)\n",
    "    category: str  # Type of pun: homophone, homograph, compound, etc.\n",
    "    \n",
    "    def full_joke(self) -> str:\n",
    "        return f\"{self.setup} {self.punchline}\"\n",
    "\n",
    "def generate_puns(topic: str, n: int = 5, llm: LLMClient = llm) -> List[PunExample]:\n",
    "    \"\"\"Generate structured pun examples on a given topic.\"\"\"\n",
    "    \n",
    "    system = \"\"\"You are an expert at creating and analyzing puns. \n",
    "When asked to create puns, you provide them in a structured JSON format.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Generate {n} puns related to \"{topic}\". \n",
    "\n",
    "For each pun, provide a JSON object with these fields:\n",
    "- setup: The setup/question part of the joke\n",
    "- punchline: The punchline/answer\n",
    "- pun_word: The word that has double meaning\n",
    "- meaning1: The literal/expected meaning\n",
    "- meaning2: The humorous/unexpected meaning  \n",
    "- category: Type of pun (homophone, homograph, compound, or other)\n",
    "\n",
    "Return a JSON array of {n} pun objects. Only return the JSON, no other text.\n",
    "\n",
    "Example format:\n",
    "[\n",
    "  {{\n",
    "    \"setup\": \"Why do electricians make good swimmers?\",\n",
    "    \"punchline\": \"Because they know the current.\",\n",
    "    \"pun_word\": \"current\",\n",
    "    \"meaning1\": \"electrical current\",\n",
    "    \"meaning2\": \"water current\",\n",
    "    \"category\": \"homograph\"\n",
    "  }}\n",
    "]\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, system=system, max_tokens=2000)\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    try:\n",
    "        # Find JSON array in response\n",
    "        json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            puns_data = json.loads(json_match.group())\n",
    "        else:\n",
    "            puns_data = json.loads(response)\n",
    "        \n",
    "        return [PunExample(**p) for p in puns_data]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate puns about different topics\n",
    "topics = [\"science\", \"music\", \"food\", \"sports\"]\n",
    "\n",
    "all_puns = []\n",
    "for topic in topics:\n",
    "    print(f\"\\nGenerating puns about {topic}...\")\n",
    "    puns = generate_puns(topic, n=3)\n",
    "    all_puns.extend(puns)\n",
    "    \n",
    "    for pun in puns:\n",
    "        print(f\"  - {pun.setup} {pun.punchline}\")\n",
    "        print(f\"    Pun word: '{pun.pun_word}' ({pun.meaning1} / {pun.meaning2})\")\n",
    "\n",
    "print(f\"\\nTotal puns generated: {len(all_puns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Matched Literal/Pun Pairs\n",
    "\n",
    "For interpretability experiments, we need pairs of sentences where the same word is used literally vs. as a pun. This lets us compare how the model processes the same word in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class PunLiteralPair:\n",
    "    \"\"\"A matched pair of pun and literal usage of the same word.\"\"\"\n",
    "    target_word: str\n",
    "    pun_context: str\n",
    "    literal_context1: str  # Using meaning 1\n",
    "    literal_context2: str  # Using meaning 2\n",
    "    meaning1: str\n",
    "    meaning2: str\n",
    "\n",
    "def generate_matched_pairs(pun_words: List[str], llm: LLMClient = llm) -> List[PunLiteralPair]:\n",
    "    \"\"\"Generate matched pun/literal pairs for given words.\"\"\"\n",
    "    \n",
    "    system = \"\"\"You create matched sentence pairs for linguistic analysis.\n",
    "Given a word with multiple meanings, you create sentences using it in different contexts.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"For each of these words that can be used in puns, create:\n",
    "1. A pun context (joke that plays on the double meaning)\n",
    "2. A literal context using meaning 1\n",
    "3. A literal context using meaning 2\n",
    "\n",
    "Words: {json.dumps(pun_words)}\n",
    "\n",
    "Return a JSON array with objects containing:\n",
    "- target_word: the word\n",
    "- pun_context: a joke/pun using the word\n",
    "- literal_context1: sentence using first meaning\n",
    "- literal_context2: sentence using second meaning\n",
    "- meaning1: description of first meaning\n",
    "- meaning2: description of second meaning\n",
    "\n",
    "Only return the JSON array, no other text.\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, system=system, max_tokens=2000)\n",
    "    \n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            pairs_data = json.loads(json_match.group())\n",
    "        else:\n",
    "            pairs_data = json.loads(response)\n",
    "        return [PunLiteralPair(**p) for p in pairs_data]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that commonly appear in puns\n",
    "pun_words = [\"current\", \"interest\", \"bark\", \"bass\", \"light\", \"wave\", \"scale\", \"battery\"]\n",
    "\n",
    "pairs = generate_matched_pairs(pun_words[:4])  # Start with 4\n",
    "\n",
    "for pair in pairs:\n",
    "    print(f\"\\n=== {pair.target_word.upper()} ===\")\n",
    "    print(f\"  Meaning 1: {pair.meaning1}\")\n",
    "    print(f\"  Meaning 2: {pair.meaning2}\")\n",
    "    print(f\"  \\n  PUN: {pair.pun_context}\")\n",
    "    print(f\"  LITERAL 1: {pair.literal_context1}\")\n",
    "    print(f\"  LITERAL 2: {pair.literal_context2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cloze-Style Evaluation Sets\n",
    "\n",
    "For probing experiments, we need sentences where the pun word is blanked out. This lets us test whether a model can predict the pun word from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClozeExample:\n",
    "    \"\"\"A cloze (fill-in-the-blank) example for probing.\"\"\"\n",
    "    prompt: str  # Text with blank\n",
    "    target: str  # The correct word\n",
    "    context_type: str  # \"pun\" or \"literal\"\n",
    "    foils: List[str]  # Alternative answers that don't fit as well\n",
    "\n",
    "def create_cloze_examples(pairs: List[PunLiteralPair]) -> List[ClozeExample]:\n",
    "    \"\"\"Convert matched pairs into cloze examples.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        word = pair.target_word\n",
    "        \n",
    "        # Create cloze for pun context\n",
    "        if word.lower() in pair.pun_context.lower():\n",
    "            # Replace the target word with ___\n",
    "            pun_cloze = re.sub(\n",
    "                rf'\\b{re.escape(word)}\\b', \n",
    "                '___', \n",
    "                pair.pun_context, \n",
    "                flags=re.IGNORECASE,\n",
    "                count=1\n",
    "            )\n",
    "            examples.append(ClozeExample(\n",
    "                prompt=pun_cloze,\n",
    "                target=word,\n",
    "                context_type=\"pun\",\n",
    "                foils=[]  # Will fill in later\n",
    "            ))\n",
    "        \n",
    "        # Create cloze for literal contexts\n",
    "        for ctx, ctx_type in [(pair.literal_context1, \"literal1\"), \n",
    "                               (pair.literal_context2, \"literal2\")]:\n",
    "            if word.lower() in ctx.lower():\n",
    "                literal_cloze = re.sub(\n",
    "                    rf'\\b{re.escape(word)}\\b',\n",
    "                    '___',\n",
    "                    ctx,\n",
    "                    flags=re.IGNORECASE,\n",
    "                    count=1\n",
    "                )\n",
    "                examples.append(ClozeExample(\n",
    "                    prompt=literal_cloze,\n",
    "                    target=word,\n",
    "                    context_type=ctx_type,\n",
    "                    foils=[]\n",
    "                ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "cloze_examples = create_cloze_examples(pairs)\n",
    "\n",
    "print(f\"Created {len(cloze_examples)} cloze examples:\\n\")\n",
    "for ex in cloze_examples:\n",
    "    print(f\"[{ex.context_type}] {ex.prompt}\")\n",
    "    print(f\"  Answer: {ex.target}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LLM-as-Judge for Quality Rating\n",
    "\n",
    "Not all generated puns are equally good. Let's use the LLM to rate pun quality on multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PunRating:\n",
    "    \"\"\"Quality ratings for a pun.\"\"\"\n",
    "    humor_score: int  # 1-5: How funny is it?\n",
    "    clarity_score: int  # 1-5: How clear is the double meaning?\n",
    "    originality_score: int  # 1-5: How original/novel?\n",
    "    groan_factor: int  # 1-5: How much of a \"groaner\" is it?\n",
    "    explanation: str\n",
    "\n",
    "def rate_pun(pun: PunExample, llm: LLMClient = llm) -> PunRating:\n",
    "    \"\"\"Use LLM-as-judge to rate a pun's quality.\"\"\"\n",
    "    \n",
    "    system = \"\"\"You are an expert judge of pun quality. \n",
    "Rate puns on multiple dimensions and explain your ratings.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Rate this pun on a scale of 1-5 for each dimension:\n",
    "\n",
    "Pun: \"{pun.full_joke()}\"\n",
    "Pun word: \"{pun.pun_word}\" (plays on: {pun.meaning1} vs {pun.meaning2})\n",
    "\n",
    "Dimensions:\n",
    "1. humor_score: How funny is it? (1=not funny, 5=hilarious)\n",
    "2. clarity_score: How clear is the double meaning? (1=confusing, 5=crystal clear)\n",
    "3. originality_score: How original? (1=very common, 5=never heard before)\n",
    "4. groan_factor: How much of a \"dad joke\" groaner? (1=not at all, 5=maximum groan)\n",
    "\n",
    "Return a JSON object with:\n",
    "- humor_score: int 1-5\n",
    "- clarity_score: int 1-5  \n",
    "- originality_score: int 1-5\n",
    "- groan_factor: int 1-5\n",
    "- explanation: brief explanation of your ratings\n",
    "\n",
    "Only return the JSON object, no other text.\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, system=system, max_tokens=500)\n",
    "    \n",
    "    try:\n",
    "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            rating_data = json.loads(json_match.group())\n",
    "        else:\n",
    "            rating_data = json.loads(response)\n",
    "        return PunRating(**rating_data)\n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"Failed to parse rating: {e}\")\n",
    "        return PunRating(3, 3, 3, 3, \"Failed to parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate our generated puns\n",
    "rated_puns = []\n",
    "\n",
    "for pun in all_puns[:5]:  # Rate first 5\n",
    "    print(f\"\\nRating: {pun.full_joke()}\")\n",
    "    rating = rate_pun(pun)\n",
    "    rated_puns.append((pun, rating))\n",
    "    \n",
    "    print(f\"  Humor: {rating.humor_score}/5\")\n",
    "    print(f\"  Clarity: {rating.clarity_score}/5\")\n",
    "    print(f\"  Originality: {rating.originality_score}/5\")\n",
    "    print(f\"  Groan factor: {rating.groan_factor}/5\")\n",
    "    print(f\"  Note: {rating.explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Export Datasets\n",
    "\n",
    "Let's export our generated data in formats useful for interpretability experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_dataframe(puns: List[PunExample], \n",
    "                        pairs: List[PunLiteralPair],\n",
    "                        cloze: List[ClozeExample]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Export all datasets to DataFrames.\"\"\"\n",
    "    \n",
    "    # Puns DataFrame\n",
    "    puns_df = pd.DataFrame([asdict(p) for p in puns])\n",
    "    puns_df['full_joke'] = puns_df['setup'] + ' ' + puns_df['punchline']\n",
    "    \n",
    "    # Pairs DataFrame  \n",
    "    pairs_df = pd.DataFrame([asdict(p) for p in pairs])\n",
    "    \n",
    "    # Cloze DataFrame\n",
    "    cloze_df = pd.DataFrame([asdict(c) for c in cloze])\n",
    "    \n",
    "    return {\n",
    "        'puns': puns_df,\n",
    "        'pairs': pairs_df,\n",
    "        'cloze': cloze_df\n",
    "    }\n",
    "\n",
    "datasets = export_to_dataframe(all_puns, pairs, cloze_examples)\n",
    "\n",
    "print(\"Puns dataset:\")\n",
    "display(datasets['puns'].head())\n",
    "\n",
    "print(\"\\nPairs dataset:\")\n",
    "display(datasets['pairs'].head())\n",
    "\n",
    "print(\"\\nCloze dataset:\")\n",
    "display(datasets['cloze'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to files\n",
    "for name, df in datasets.items():\n",
    "    filename = f\"pun_{name}_dataset.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename} ({len(df)} rows)\")\n",
    "\n",
    "# Also save as JSON for easy loading\n",
    "all_data = {\n",
    "    'puns': [asdict(p) for p in all_puns],\n",
    "    'pairs': [asdict(p) for p in pairs],\n",
    "    'cloze': [asdict(c) for c in cloze_examples]\n",
    "}\n",
    "\n",
    "with open('pun_dataset.json', 'w') as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "print(\"Saved pun_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part 6: Getting Files to Your Local Machine\n\nColab runs in the cloud, so you need to download your generated files. Use Colab's built-in download to save files directly to your computer:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from google.colab import files\n\n# Download individual files\nfiles.download('pun_dataset.json')\nfiles.download('pun_puns_dataset.csv')\nfiles.download('pun_pairs_dataset.csv')\nfiles.download('pun_cloze_dataset.csv')\n\n# Or zip everything and download once\n!zip -r pun_datasets.zip pun_*.csv pun_*.json\nfiles.download('pun_datasets.zip')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive\n\n# Mount Google Drive (will prompt for authorization)\ndrive.mount('/content/drive')\n\n# Create a folder for your project\nproject_folder = '/content/drive/MyDrive/neural-mechanics-project/data'\n!mkdir -p {project_folder}\n\n# Copy files to Drive\n!cp pun_dataset.json {project_folder}/\n!cp pun_*.csv {project_folder}/\n\nprint(f\"Files saved to Google Drive: {project_folder}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Generate Domain-Specific Puns\n",
    "\n",
    "Generate puns specific to your research domain. If you're studying legal concepts, medical terms, or mathematical notation, create puns that use terminology from that field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create Minimal Pairs\n",
    "\n",
    "Create pairs of sentences that differ only in whether they set up a pun. This is useful for causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minimal_pairs(pun_word: str, llm: LLMClient = llm) -> Dict:\n",
    "    \"\"\"Generate minimal pairs: same ending, different setup (pun vs literal).\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Create a minimal pair of sentences that both end with the word \"{pun_word}\".\n",
    "\n",
    "1. A pun setup: A question/setup that makes \"{pun_word}\" funny as a punchline\n",
    "2. A literal setup: A sentence where \"{pun_word}\" is just the normal/expected word\n",
    "\n",
    "Both sentences should end with exactly \"{pun_word}\" as the final word.\n",
    "\n",
    "Return JSON with:\n",
    "- pun_setup: the joke setup ending in \"{pun_word}\"\n",
    "- literal_setup: the normal sentence ending in \"{pun_word}\"\n",
    "- pun_word: \"{pun_word}\"\n",
    "\n",
    "Only return the JSON object.\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, max_tokens=300)\n",
    "    \n",
    "    try:\n",
    "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": response}\n",
    "\n",
    "# Example\n",
    "minimal_pair = generate_minimal_pairs(\"current\")\n",
    "print(json.dumps(minimal_pair, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Generate Foils for Cloze Tasks\n",
    "\n",
    "Add distractor words (foils) to the cloze examples. Good foils should be plausible but not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_foils_to_cloze(example: ClozeExample, llm: LLMClient = llm) -> ClozeExample:\n",
    "    \"\"\"Add distractor words to a cloze example.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Given this fill-in-the-blank sentence, suggest 3 plausible but incorrect words.\n",
    "\n",
    "Sentence: {example.prompt}\n",
    "Correct answer: {example.target}\n",
    "\n",
    "Provide 3 words that:\n",
    "1. Could grammatically fit in the blank\n",
    "2. Are semantically related but don't create the intended meaning\n",
    "3. Would be reasonable guesses\n",
    "\n",
    "Return a JSON array of 3 strings. Only return the JSON array.\"\"\"\n",
    "    \n",
    "    response = llm.generate(prompt, max_tokens=100)\n",
    "    \n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            foils = json.loads(json_match.group())\n",
    "        else:\n",
    "            foils = json.loads(response)\n",
    "        example.foils = foils\n",
    "    except json.JSONDecodeError:\n",
    "        example.foils = []\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Add foils to our cloze examples\n",
    "for ex in cloze_examples[:3]:\n",
    "    ex = add_foils_to_cloze(ex)\n",
    "    print(f\"Prompt: {ex.prompt}\")\n",
    "    print(f\"Answer: {ex.target}\")\n",
    "    print(f\"Foils: {ex.foils}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned how to use LLMs to create evaluation datasets:\n",
    "\n",
    "1. **Generate structured examples** with controlled format (setup, punchline, meanings)\n",
    "2. **Create matched pairs** for comparing pun vs literal usage\n",
    "3. **Build cloze tasks** for probing experiments\n",
    "4. **Use LLM-as-judge** to rate quality on multiple dimensions\n",
    "5. **Export datasets** in formats useful for interpretability research\n",
    "\n",
    "### For Your Project\n",
    "\n",
    "Adapt this approach to your concept:\n",
    "- Generate examples that use your concept in controlled ways\n",
    "- Create matched pairs (concept present vs absent)\n",
    "- Build cloze tasks for probing\n",
    "- Rate quality and filter to high-quality examples\n",
    "\n",
    "### Tips for Dataset Quality\n",
    "\n",
    "- Generate more examples than you need, then filter\n",
    "- Use LLM-as-judge to identify the best examples\n",
    "- Manually review a sample to catch systematic errors\n",
    "- Consider having multiple LLMs generate/rate for diversity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}