{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Causal Tracing and Activation Patching with Puns\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week5/causal_tracing_puns.ipynb)\n",
    "\n",
    "This notebook demonstrates **causal mediation analysis (CMA)** and **activation patching** using [nnsight](https://nnsight.net/) and the [NDIF](https://ndif.us/) remote inference API.\n",
    "\n",
    "**Key Idea:** Visualization shows us *what* is represented; causal intervention tells us *where* it's computed. By patching activations between two runs, we can identify which components are causally responsible for a behavior.\n",
    "\n",
    "We'll use **Llama 3 70B** via NDIF to explore where pun understanding is localized in the model.\n",
    "\n",
    "## References\n",
    "- [ROME: Locating and Editing Factual Associations](https://arxiv.org/abs/2202.05262) - Meng et al.\n",
    "- [Function Vectors in Large Language Models](https://arxiv.org/abs/2310.15213) - Todd et al.\n",
    "- [nnsight documentation](https://nnsight.net/)\n",
    "- [NDIF - National Deep Inference Fabric](https://ndif.us/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install nnsight if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nnsight import LanguageModel, CONFIG\n\n# Configure NDIF API key from Colab secrets\ntry:\n    from google.colab import userdata\n    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\nexcept:\n    pass  # Not in Colab or secret not set\n\n# Use remote=True to run on NDIF's shared GPU resources\nREMOTE = True"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Llama 3 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B\", device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Activation Patching\n",
    "\n",
    "**Activation patching** replaces activations from one forward pass with those from another. This lets us test causal hypotheses:\n",
    "\n",
    "- **Clean run:** Model produces correct/expected output\n",
    "- **Corrupted run:** Model produces different output  \n",
    "- **Patched run:** Replace some activations from corrupted with clean\n",
    "- **Measure:** Does patching restore the clean behavior?\n",
    "\n",
    "If patching a specific component restores clean behavior, that component is **causally important**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Pun vs Literal Pairs\n",
    "\n",
    "For pun experiments, we need pairs where the same word appears in pun and literal contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pun and literal pairs for \"current\"\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "literal_prompt = \"The wires carry high voltage electrical\"\n",
    "\n",
    "# Both should predict \"current\" but for different reasons\n",
    "target_token = \" current\"\n",
    "\n",
    "print(f\"Pun context: {pun_prompt}\")\n",
    "print(f\"Literal context: {literal_prompt}\")\n",
    "print(f\"Target: '{target_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Basic Activation Collection\n",
    "\n",
    "First, let's collect activations from both contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompt, model, layers=None, remote=True):\n",
    "    \"\"\"\n",
    "    Collect hidden state activations at specified layers.\n",
    "    Returns dict: layer_idx -> activation tensor\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    if layers is None:\n",
    "        layers = list(range(n_layers))\n",
    "    \n",
    "    activations = {}\n",
    "    \n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        for layer_idx in layers:\n",
    "            # Get hidden states after this layer\n",
    "            hidden = model.model.layers[layer_idx].output[0].save()\n",
    "            activations[layer_idx] = hidden\n",
    "    \n",
    "    # Convert to actual tensors\n",
    "    return {k: v.value for k, v in activations.items()}\n",
    "\n",
    "# Collect activations for both prompts\n",
    "pun_activations = get_activations(pun_prompt, model, remote=REMOTE)\n",
    "literal_activations = get_activations(literal_prompt, model, remote=REMOTE)\n",
    "\n",
    "print(f\"Collected activations from {len(pun_activations)} layers\")\n",
    "print(f\"Pun activation shape: {pun_activations[0].shape}\")\n",
    "print(f\"Literal activation shape: {literal_activations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 2: Simple Activation Patching\n",
    "\n",
    "Let's patch activations from the pun context into the literal context and see how it affects predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_and_measure(base_prompt, patch_activations, patch_layer, model, \n",
    "                      target_token, remote=True):\n",
    "    \"\"\"\n",
    "    Run base_prompt but patch in activations at a specific layer.\n",
    "    Returns probability of target token.\n",
    "    \"\"\"\n",
    "    target_id = model.tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    with model.trace(base_prompt, remote=remote) as tracer:\n",
    "        # Patch the layer's hidden state\n",
    "        # Note: We need to handle different sequence lengths\n",
    "        base_hidden = model.model.layers[patch_layer].output[0]\n",
    "        patch_hidden = patch_activations[patch_layer]\n",
    "        \n",
    "        # Patch at the last position (most relevant for next-token prediction)\n",
    "        # We replace the last token's activation with the patched version's last token\n",
    "        base_hidden[:, -1, :] = patch_hidden[:, -1, :]\n",
    "        \n",
    "        # Get final logits\n",
    "        logits = model.lm_head.output.save()\n",
    "    \n",
    "    # Compute probability of target\n",
    "    probs = torch.softmax(logits.value[0, -1], dim=-1)\n",
    "    target_prob = probs[target_id].item()\n",
    "    \n",
    "    return target_prob\n",
    "\n",
    "# Get baseline probabilities\n",
    "def get_target_prob(prompt, target_token, model, remote=True):\n",
    "    \"\"\"Get probability of target token without patching.\"\"\"\n",
    "    target_id = model.tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        logits = model.lm_head.output.save()\n",
    "    \n",
    "    probs = torch.softmax(logits.value[0, -1], dim=-1)\n",
    "    return probs[target_id].item()\n",
    "\n",
    "pun_baseline = get_target_prob(pun_prompt, target_token, model, remote=REMOTE)\n",
    "literal_baseline = get_target_prob(literal_prompt, target_token, model, remote=REMOTE)\n",
    "\n",
    "print(f\"P('{target_token}') in pun context: {pun_baseline:.4f}\")\n",
    "print(f\"P('{target_token}') in literal context: {literal_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Patching Across Layers\n",
    "\n",
    "Let's see how patching at different layers affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch pun activations into literal context at each layer\n",
    "layers_to_test = list(range(0, model.config.num_hidden_layers, 5))  # Every 5th layer\n",
    "\n",
    "patched_probs = []\n",
    "for layer in layers_to_test:\n",
    "    prob = patch_and_measure(literal_prompt, pun_activations, layer, \n",
    "                              model, target_token, remote=REMOTE)\n",
    "    patched_probs.append(prob)\n",
    "    print(f\"Layer {layer:2d}: P('{target_token}') = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(layers_to_test, patched_probs, alpha=0.7)\n",
    "plt.axhline(y=literal_baseline, color='r', linestyle='--', label='Literal baseline')\n",
    "plt.axhline(y=pun_baseline, color='g', linestyle='--', label='Pun baseline')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f'P(\"{target_token}\")')\n",
    "plt.title('Effect of Patching Pun Activations into Literal Context')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 3: ROME-Style Causal Tracing\n",
    "\n",
    "The ROME paper uses a more sophisticated approach:\n",
    "1. **Corrupt** the input by adding noise to embeddings\n",
    "2. **Restore** clean activations at specific (layer, position) pairs\n",
    "3. **Measure** how much each restoration helps recover the correct output\n",
    "\n",
    "This creates a \"causal map\" showing where information flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_trace(prompt, target_token, model, noise_std=0.1, \n",
    "                 layers_to_test=None, remote=True):\n",
    "    \"\"\"\n",
    "    ROME-style causal tracing.\n",
    "    \n",
    "    1. Run clean pass, save all activations\n",
    "    2. Run corrupted pass (noise on embeddings)\n",
    "    3. For each (layer, position), run corrupted + restore that activation\n",
    "    4. Measure recovery of target probability\n",
    "    \n",
    "    Returns: 2D array of recovery scores [layers x positions]\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    if layers_to_test is None:\n",
    "        layers_to_test = list(range(0, n_layers, 4))  # Sample every 4th layer\n",
    "    \n",
    "    target_id = model.tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    tokens = model.tokenizer.encode(prompt)\n",
    "    n_positions = len(tokens)\n",
    "    \n",
    "    # Step 1: Clean run - collect all activations\n",
    "    clean_activations = {}\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        for layer_idx in layers_to_test:\n",
    "            hidden = model.model.layers[layer_idx].output[0].save()\n",
    "            clean_activations[layer_idx] = hidden\n",
    "        clean_logits = model.lm_head.output.save()\n",
    "    \n",
    "    clean_prob = torch.softmax(clean_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    clean_activations = {k: v.value for k, v in clean_activations.items()}\n",
    "    \n",
    "    # Step 2: Corrupted run (add noise to embeddings)\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        # Add noise to the embedding output\n",
    "        embed_out = model.model.embed_tokens.output\n",
    "        noise = torch.randn_like(embed_out) * noise_std\n",
    "        embed_out[:] = embed_out + noise\n",
    "        \n",
    "        corrupted_logits = model.lm_head.output.save()\n",
    "    \n",
    "    corrupted_prob = torch.softmax(corrupted_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    \n",
    "    print(f\"Clean prob: {clean_prob:.4f}\")\n",
    "    print(f\"Corrupted prob: {corrupted_prob:.4f}\")\n",
    "    \n",
    "    # Step 3: For each (layer, position), restore and measure\n",
    "    recovery_map = np.zeros((len(layers_to_test), n_positions))\n",
    "    \n",
    "    for layer_i, layer_idx in enumerate(layers_to_test):\n",
    "        for pos in range(n_positions):\n",
    "            # Corrupted run with restoration at (layer, pos)\n",
    "            with model.trace(prompt, remote=remote) as tracer:\n",
    "                # Add noise to embeddings\n",
    "                embed_out = model.model.embed_tokens.output\n",
    "                noise = torch.randn_like(embed_out) * noise_std\n",
    "                embed_out[:] = embed_out + noise\n",
    "                \n",
    "                # Restore clean activation at specific position\n",
    "                hidden = model.model.layers[layer_idx].output[0]\n",
    "                hidden[:, pos, :] = clean_activations[layer_idx][:, pos, :]\n",
    "                \n",
    "                patched_logits = model.lm_head.output.save()\n",
    "            \n",
    "            patched_prob = torch.softmax(patched_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "            \n",
    "            # Recovery = how much of the lost probability we restored\n",
    "            if clean_prob > corrupted_prob:\n",
    "                recovery = (patched_prob - corrupted_prob) / (clean_prob - corrupted_prob + 1e-8)\n",
    "            else:\n",
    "                recovery = 0\n",
    "            \n",
    "            recovery_map[layer_i, pos] = recovery\n",
    "        \n",
    "        print(f\"Layer {layer_idx:2d} done\")\n",
    "    \n",
    "    return recovery_map, layers_to_test, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run causal tracing on our pun\n",
    "recovery_map, layers, tokens = causal_trace(\n",
    "    pun_prompt, target_token, model, \n",
    "    noise_std=0.1, remote=REMOTE\n",
    ")\n",
    "\n",
    "print(f\"\\nRecovery map shape: {recovery_map.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the causal trace\n",
    "token_strs = [model.tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.imshow(recovery_map, aspect='auto', cmap='Reds', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Recovery (0=no effect, 1=full recovery)')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Layer')\n",
    "plt.title(f'Causal Trace: Where is \"{target_token}\" computed?\\nPrompt: \"{pun_prompt}\"')\n",
    "plt.xticks(range(len(token_strs)), token_strs, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(range(len(layers)), layers)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 4: Average Indirect Effect (AIE)\n",
    "\n",
    "To systematically identify important components, we compute the **Average Indirect Effect (AIE)** across multiple examples:\n",
    "\n",
    "$$\\text{AIE}(\\text{component}) = \\mathbb{E}[P(\\text{correct} | \\text{patch component}) - P(\\text{correct} | \\text{no patch})]$$\n",
    "\n",
    "Components with high AIE are causally important for the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More pun examples for computing AIE\n",
    "pun_examples = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up with his girlfriend? He lost\", \" interest\"),\n",
    "    (\"What did the ocean say to the beach? Nothing, it just\", \" waved\"),\n",
    "    (\"Why don't scientists trust atoms? Because they make up\", \" everything\"),\n",
    "]\n",
    "\n",
    "def compute_layer_aie(examples, model, noise_std=0.1, remote=True):\n",
    "    \"\"\"\n",
    "    Compute AIE for each layer across multiple examples.\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    layers_to_test = list(range(0, n_layers, 8))  # Every 8th layer for speed\n",
    "    \n",
    "    aie_per_layer = {layer: [] for layer in layers_to_test}\n",
    "    \n",
    "    for prompt, target in examples:\n",
    "        target_id = model.tokenizer.encode(target, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Clean run\n",
    "        clean_activations = {}\n",
    "        with model.trace(prompt, remote=remote) as tracer:\n",
    "            for layer_idx in layers_to_test:\n",
    "                hidden = model.model.layers[layer_idx].output[0].save()\n",
    "                clean_activations[layer_idx] = hidden\n",
    "            clean_logits = model.lm_head.output.save()\n",
    "        \n",
    "        clean_prob = torch.softmax(clean_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "        clean_activations = {k: v.value for k, v in clean_activations.items()}\n",
    "        \n",
    "        # Corrupted run\n",
    "        with model.trace(prompt, remote=remote) as tracer:\n",
    "            embed_out = model.model.embed_tokens.output\n",
    "            noise = torch.randn_like(embed_out) * noise_std\n",
    "            embed_out[:] = embed_out + noise\n",
    "            corrupted_logits = model.lm_head.output.save()\n",
    "        \n",
    "        corrupted_prob = torch.softmax(corrupted_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "        \n",
    "        # Patched runs for each layer\n",
    "        for layer_idx in layers_to_test:\n",
    "            with model.trace(prompt, remote=remote) as tracer:\n",
    "                embed_out = model.model.embed_tokens.output\n",
    "                noise = torch.randn_like(embed_out) * noise_std\n",
    "                embed_out[:] = embed_out + noise\n",
    "                \n",
    "                # Restore entire layer (all positions)\n",
    "                hidden = model.model.layers[layer_idx].output[0]\n",
    "                hidden[:] = clean_activations[layer_idx]\n",
    "                \n",
    "                patched_logits = model.lm_head.output.save()\n",
    "            \n",
    "            patched_prob = torch.softmax(patched_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "            \n",
    "            # Indirect effect = patched - corrupted\n",
    "            ie = patched_prob - corrupted_prob\n",
    "            aie_per_layer[layer_idx].append(ie)\n",
    "        \n",
    "        print(f\"Processed: {prompt[:40]}... -> {target}\")\n",
    "    \n",
    "    # Average across examples\n",
    "    return {layer: np.mean(effects) for layer, effects in aie_per_layer.items()}\n",
    "\n",
    "aie_scores = compute_layer_aie(pun_examples, model, remote=REMOTE)\n",
    "print(\"\\nAIE scores per layer:\")\n",
    "for layer, score in sorted(aie_scores.items()):\n",
    "    print(f\"  Layer {layer:2d}: AIE = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AIE across layers\n",
    "layers = sorted(aie_scores.keys())\n",
    "scores = [aie_scores[l] for l in layers]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(layers, scores, alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Indirect Effect (AIE)')\n",
    "plt.title('Which Layers Are Causally Important for Pun Prediction?')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top layers\n",
    "sorted_layers = sorted(aie_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 3 most important layers:\")\n",
    "for layer, score in sorted_layers[:3]:\n",
    "    print(f\"  Layer {layer}: AIE = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 5: MLP vs Attention Comparison\n",
    "\n",
    "ROME found that factual knowledge is stored in MLPs, while entity tracking uses attention. Where is pun processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mlp_vs_attention(prompt, target_token, model, layer_idx, \n",
    "                             noise_std=0.1, remote=True):\n",
    "    \"\"\"\n",
    "    Compare the causal importance of MLP vs attention outputs at a layer.\n",
    "    \"\"\"\n",
    "    target_id = model.tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Clean run - save MLP and attention outputs separately\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        # In Llama, self_attn gives attention output, mlp gives MLP output\n",
    "        attn_out = model.model.layers[layer_idx].self_attn.output[0].save()\n",
    "        mlp_out = model.model.layers[layer_idx].mlp.output.save()\n",
    "        clean_logits = model.lm_head.output.save()\n",
    "    \n",
    "    clean_prob = torch.softmax(clean_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    clean_attn = attn_out.value\n",
    "    clean_mlp = mlp_out.value\n",
    "    \n",
    "    # Corrupted baseline\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        embed_out = model.model.embed_tokens.output\n",
    "        noise = torch.randn_like(embed_out) * noise_std\n",
    "        embed_out[:] = embed_out + noise\n",
    "        corrupted_logits = model.lm_head.output.save()\n",
    "    \n",
    "    corrupted_prob = torch.softmax(corrupted_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    \n",
    "    # Patch only attention\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        embed_out = model.model.embed_tokens.output\n",
    "        noise = torch.randn_like(embed_out) * noise_std\n",
    "        embed_out[:] = embed_out + noise\n",
    "        \n",
    "        attn_out = model.model.layers[layer_idx].self_attn.output[0]\n",
    "        attn_out[:] = clean_attn\n",
    "        \n",
    "        attn_patched_logits = model.lm_head.output.save()\n",
    "    \n",
    "    attn_patched_prob = torch.softmax(attn_patched_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    \n",
    "    # Patch only MLP\n",
    "    with model.trace(prompt, remote=remote) as tracer:\n",
    "        embed_out = model.model.embed_tokens.output\n",
    "        noise = torch.randn_like(embed_out) * noise_std\n",
    "        embed_out[:] = embed_out + noise\n",
    "        \n",
    "        mlp_o = model.model.layers[layer_idx].mlp.output\n",
    "        mlp_o[:] = clean_mlp\n",
    "        \n",
    "        mlp_patched_logits = model.lm_head.output.save()\n",
    "    \n",
    "    mlp_patched_prob = torch.softmax(mlp_patched_logits.value[0, -1], dim=-1)[target_id].item()\n",
    "    \n",
    "    attn_effect = attn_patched_prob - corrupted_prob\n",
    "    mlp_effect = mlp_patched_prob - corrupted_prob\n",
    "    \n",
    "    return {\n",
    "        'clean': clean_prob,\n",
    "        'corrupted': corrupted_prob,\n",
    "        'attn_patched': attn_patched_prob,\n",
    "        'mlp_patched': mlp_patched_prob,\n",
    "        'attn_effect': attn_effect,\n",
    "        'mlp_effect': mlp_effect\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MLP vs attention at several layers\n",
    "layers_to_compare = [16, 32, 48, 64]\n",
    "\n",
    "results = []\n",
    "for layer in layers_to_compare:\n",
    "    result = compare_mlp_vs_attention(pun_prompt, target_token, model, layer, remote=REMOTE)\n",
    "    result['layer'] = layer\n",
    "    results.append(result)\n",
    "    print(f\"Layer {layer}: MLP effect={result['mlp_effect']:.4f}, Attn effect={result['attn_effect']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MLP vs Attention\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(layers_to_compare))\n",
    "width = 0.35\n",
    "\n",
    "mlp_effects = [r['mlp_effect'] for r in results]\n",
    "attn_effects = [r['attn_effect'] for r in results]\n",
    "\n",
    "ax.bar(x - width/2, mlp_effects, width, label='MLP', color='steelblue')\n",
    "ax.bar(x + width/2, attn_effects, width, label='Attention', color='coral')\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Causal Effect')\n",
    "ax.set_title('MLP vs Attention: Where is Pun Processing?')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([r['layer'] for r in results])\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Exercise 1: Cross-Context Patching\n",
    "\n",
    "Patch activations from a pun context into a literal context (and vice versa). Can we transfer pun \"understanding\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement cross-context patching\n",
    "# 1. Get activations from pun: \"Why do electricians make good swimmers? Because they know the\"\n",
    "# 2. Get activations from literal: \"The river has a strong water\"\n",
    "# 3. Patch pun -> literal at different layers\n",
    "# 4. Measure: Does \"current\" become more likely in the literal context?\n",
    "\n",
    "pun = \"Why do electricians make good swimmers? Because they know the\"\n",
    "literal = \"The river has a strong water\"\n",
    "\n",
    "# Your code here:\n",
    "# pun_acts = get_activations(pun, model, remote=REMOTE)\n",
    "# ...\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Exercise 2: Find the \"Pun Switch\"\n",
    "\n",
    "Is there a specific layer where patching flips the model's interpretation from literal to pun (or vice versa)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Systematic search for the \"pun switch\"\n",
    "# For each layer:\n",
    "#   1. Run literal prompt\n",
    "#   2. Patch in pun activations at that layer\n",
    "#   3. Check if top prediction changes from literal meaning to pun meaning\n",
    "\n",
    "# Example: For \"current\", does the model go from predicting electrical-related\n",
    "# words to swimming-related words?\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Exercise 3: Multiple Pun Types\n",
    "\n",
    "Compare causal signatures across different types of puns:\n",
    "- **Homograph puns:** Same spelling, different meanings (\"current\")\n",
    "- **Homophone puns:** Same sound, different spellings (\"knight/night\")\n",
    "- **Compound puns:** Play on phrases (\"time flies like an arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare causal traces for different pun types\n",
    "homograph_puns = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up? He lost\", \" interest\"),\n",
    "]\n",
    "\n",
    "homophone_puns = [\n",
    "    (\"What do you call a fake noodle? An\", \" imp\"),  # impasta\n",
    "]\n",
    "\n",
    "compound_puns = [\n",
    "    (\"Time flies like an arrow. Fruit flies like a\", \" banana\"),\n",
    "]\n",
    "\n",
    "# Your code here: Run causal_trace on each type and compare patterns\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Activation patching** replaces activations to test causal hypotheses\n",
    "\n",
    "2. **ROME-style causal tracing** maps where information is processed by systematically restoring corrupted activations\n",
    "\n",
    "3. **Average Indirect Effect (AIE)** identifies causally important components across examples\n",
    "\n",
    "4. **MLP vs Attention comparison** helps distinguish where different types of information are processed\n",
    "\n",
    "### Key Questions for Your Research\n",
    "\n",
    "- Is your concept localized to specific layers, or distributed?\n",
    "- Is it processed more in MLPs (like factual knowledge) or attention (like entity tracking)?\n",
    "- Can you transfer concept understanding between contexts via patching?\n",
    "- How do causal findings relate to your visualization results from Week 1?\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Apply these techniques to your research concept\n",
    "2. Create minimal pairs for your domain\n",
    "3. Run systematic AIE analysis to find important components\n",
    "4. Compare causal importance with representation geometry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}