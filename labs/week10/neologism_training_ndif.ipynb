{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neologisms via NDIF\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week10/neologism_training_ndif.ipynb)\n",
    "\n",
    "This notebook demonstrates **neologism learning** - teaching a model a new word by training only its embedding - using NDIF for remote training on large models.\n",
    "\n",
    "**Key Idea:** We add a \"new word\" to the model's vocabulary and train its embedding to capture a specific concept. Then we can ask the model \"What does [neologism] mean?\" and it will explain the concept in natural language.\n",
    "\n",
    "## Why Neologisms?\n",
    "- **Concept extraction**: What does the model think a concept means?\n",
    "- **Steering**: Use the neologism to control model behavior\n",
    "- **Alignment**: Teach precise human concepts to models\n",
    "- **Interpretability**: Self-verbalization of learned representations\n",
    "\n",
    "## Remote Training Pattern\n",
    "Like probe training (Week 6), we run the forward/backward pass on NDIF and download gradients to update locally.\n",
    "\n",
    "## References\n",
    "- [We Can't Understand AI Using our Existing Vocabulary](https://arxiv.org/abs/2502.07586) (Hewitt, Geirhos & Kim, 2025)\n",
    "- [nnsight documentation](https://nnsight.net/)\n",
    "- [NDIF](https://ndif.us/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Use NDIF for remote execution on large models\n",
    "REMOTE = True\n",
    "\n",
    "# For local testing, set REMOTE = False and use a smaller model\n",
    "if REMOTE:\n",
    "    MODEL_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
    "else:\n",
    "    MODEL_ID = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = LanguageModel(MODEL_ID, device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Embedding dim: {model.config.hidden_size}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Neologism Training Approach\n",
    "\n",
    "Instead of modifying the vocabulary, we:\n",
    "1. Choose a **placeholder token** (e.g., `[NEO]` or an unused token)\n",
    "2. Learn a **custom embedding** for this token\n",
    "3. **Intervene** during forward pass to replace the placeholder's embedding with our learned one\n",
    "4. Train by **downloading gradients** from NDIF\n",
    "\n",
    "This is similar to soft prompts / prefix tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteNeologismTrainer:\n",
    "    \"\"\"\n",
    "    Train a neologism embedding using NDIF remote execution.\n",
    "    \n",
    "    The neologism is represented by a placeholder token whose embedding\n",
    "    we replace during forward passes with our learned embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, placeholder_token=\"[NEO]\", lr=0.1):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Use a placeholder token - we'll replace its embedding\n",
    "        # Option 1: Add a new token\n",
    "        # Option 2: Use an existing rare token\n",
    "        self.placeholder = placeholder_token\n",
    "        \n",
    "        # Check if placeholder exists, if not use a rare token\n",
    "        placeholder_ids = model.tokenizer.encode(placeholder_token, add_special_tokens=False)\n",
    "        if len(placeholder_ids) == 1:\n",
    "            self.placeholder_id = placeholder_ids[0]\n",
    "        else:\n",
    "            # Use an uncommon token as placeholder\n",
    "            # We'll use a high vocab ID that's rarely used\n",
    "            self.placeholder_id = model.config.vocab_size - 100\n",
    "            self.placeholder = model.tokenizer.decode([self.placeholder_id])\n",
    "        \n",
    "        print(f\"Placeholder token: '{self.placeholder}' (id: {self.placeholder_id})\")\n",
    "        \n",
    "        # Initialize neologism embedding\n",
    "        # Start from the placeholder's original embedding or random\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        self.neologism_embedding = torch.randn(self.embedding_dim) * 0.02\n",
    "        \n",
    "    def get_prompt_with_neologism(self, template, position=\"prefix\"):\n",
    "        \"\"\"\n",
    "        Create a prompt with the neologism placeholder.\n",
    "        \n",
    "        Args:\n",
    "            template: Prompt template with {neo} placeholder\n",
    "            position: Where to put neologism (\"prefix\", \"inline\")\n",
    "        \"\"\"\n",
    "        return template.replace(\"{neo}\", self.placeholder)\n",
    "    \n",
    "    def train_step(self, prompts, targets, remote=True):\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompts containing {neo} placeholder\n",
    "            targets: List of target completions\n",
    "            remote: Whether to run on NDIF\n",
    "            \n",
    "        Returns:\n",
    "            loss: Training loss\n",
    "        \"\"\"\n",
    "        # Replace {neo} with placeholder token\n",
    "        prompts_with_neo = [self.get_prompt_with_neologism(p) for p in prompts]\n",
    "        \n",
    "        # Tokenize\n",
    "        full_texts = [p + t for p, t in zip(prompts_with_neo, targets)]\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        accumulated_grad = torch.zeros_like(self.neologism_embedding)\n",
    "        \n",
    "        for text, prompt in zip(full_texts, prompts_with_neo):\n",
    "            # Find position of placeholder token\n",
    "            tokens = self.model.tokenizer.encode(text)\n",
    "            prompt_tokens = self.model.tokenizer.encode(prompt)\n",
    "            \n",
    "            # Find where placeholder appears\n",
    "            neo_positions = [i for i, t in enumerate(tokens) if t == self.placeholder_id]\n",
    "            \n",
    "            if not neo_positions:\n",
    "                print(f\"Warning: Placeholder not found in '{text[:50]}...'\")\n",
    "                continue\n",
    "            \n",
    "            with self.model.trace(text, remote=remote) as tracer:\n",
    "                # Get the embedding layer output\n",
    "                embed_output = self.model.model.embed_tokens.output\n",
    "                \n",
    "                # Create our neologism embedding on the correct device\n",
    "                neo_emb = self.neologism_embedding.to(embed_output.device)\n",
    "                neo_emb.requires_grad_(True)\n",
    "                \n",
    "                # Replace embedding at placeholder positions\n",
    "                for pos in neo_positions:\n",
    "                    embed_output[0, pos, :] = neo_emb\n",
    "                \n",
    "                # Get logits\n",
    "                logits = self.model.output.logits\n",
    "                \n",
    "                # Compute loss on target tokens (after prompt)\n",
    "                prompt_len = len(prompt_tokens)\n",
    "                target_logits = logits[0, prompt_len-1:-1, :]  # Predict next tokens\n",
    "                target_ids = torch.tensor(tokens[prompt_len:]).to(logits.device)\n",
    "                \n",
    "                loss = F.cross_entropy(target_logits, target_ids)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Save the gradient\n",
    "                grad = neo_emb.grad.save()\n",
    "                loss_val = loss.save()\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            accumulated_grad += grad.value.cpu()\n",
    "            total_loss += loss_val.value.item()\n",
    "        \n",
    "        # Update embedding\n",
    "        self.neologism_embedding -= self.lr * accumulated_grad / len(prompts)\n",
    "        \n",
    "        return total_loss / len(prompts)\n",
    "    \n",
    "    def generate_with_neologism(self, prompt, max_new_tokens=50, remote=True):\n",
    "        \"\"\"\n",
    "        Generate text using the learned neologism.\n",
    "        \"\"\"\n",
    "        prompt_with_neo = self.get_prompt_with_neologism(prompt)\n",
    "        tokens = self.model.tokenizer.encode(prompt_with_neo)\n",
    "        neo_positions = [i for i, t in enumerate(tokens) if t == self.placeholder_id]\n",
    "        \n",
    "        # We need to generate token by token with intervention\n",
    "        generated = list(tokens)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            input_text = self.model.tokenizer.decode(generated)\n",
    "            \n",
    "            with self.model.trace(input_text, remote=remote) as tracer:\n",
    "                embed_output = self.model.model.embed_tokens.output\n",
    "                \n",
    "                # Replace at original neologism positions\n",
    "                neo_emb = self.neologism_embedding.to(embed_output.device)\n",
    "                for pos in neo_positions:\n",
    "                    if pos < embed_output.shape[1]:\n",
    "                        embed_output[0, pos, :] = neo_emb\n",
    "                \n",
    "                logits = self.model.output.logits.save()\n",
    "            \n",
    "            # Get next token\n",
    "            next_token_logits = logits.value[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits).item()\n",
    "            \n",
    "            # Stop at EOS\n",
    "            if next_token == self.model.tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            generated.append(next_token)\n",
    "        \n",
    "        return self.model.tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train a \"Pun\" Neologism\n",
    "\n",
    "Let's train a neologism that captures the concept of \"pun\" - then ask the model what it means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: examples that demonstrate what a pun is\n",
    "pun_training_data = [\n",
    "    # Format: (prompt with {neo}, target completion)\n",
    "    (\n",
    "        \"Here is an example of {neo}: Why do electricians make good swimmers?\",\n",
    "        \" Because they know the current.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I used to be a banker, but I lost\",\n",
    "        \" interest.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A classic {neo}: Time flies like an arrow; fruit flies like a\",\n",
    "        \" banana.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Here's {neo}: Why can't a bicycle stand on its own?\",\n",
    "        \" Because it's two tired.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I'm reading a book about anti-gravity.\",\n",
    "        \" It's impossible to put down.\"\n",
    "    ),\n",
    "    (\n",
    "        \"{neo} example: What do you call a fish without eyes?\",\n",
    "        \" A fsh.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Another {neo}: The math teacher called in sick because she had too many\",\n",
    "        \" problems.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Classic {neo}: I used to work at a calendar factory but got fired for taking\",\n",
    "        \" a day off.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompts = [p for p, t in pun_training_data]\n",
    "targets = [t for p, t in pun_training_data]\n",
    "\n",
    "print(f\"Training examples: {len(pun_training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = RemoteNeologismTrainer(model, placeholder_token=\"[PUN]\", lr=0.5)\n",
    "\n",
    "print(f\"Initial embedding norm: {trainer.neologism_embedding.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neologism\n",
    "n_epochs = 20\n",
    "losses = []\n",
    "\n",
    "print(\"Training neologism...\")\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss = trainer.train_step(prompts, targets, remote=REMOTE)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal embedding norm: {trainer.neologism_embedding.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neologism Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Self-Verbalization - What Does the Neologism Mean?\n",
    "\n",
    "Now we ask the model to explain what the neologism means in its own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model what the neologism means\n",
    "definition_prompts = [\n",
    "    \"The word {neo} means:\",\n",
    "    \"A {neo} is defined as:\",\n",
    "    \"{neo} refers to:\",\n",
    "    \"When someone says {neo}, they mean:\",\n",
    "]\n",
    "\n",
    "print(\"Model's understanding of the neologism:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in definition_prompts:\n",
    "    output = trainer.generate_with_neologism(prompt, max_new_tokens=50, remote=REMOTE)\n",
    "    print(f\"\\nPrompt: {prompt.replace('{neo}', '[PUN]')}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model can generate new puns using the neologism\n",
    "generation_prompts = [\n",
    "    \"Here is a new {neo}:\",\n",
    "    \"Tell me {neo}:\",\n",
    "    \"Give me an example of {neo}:\",\n",
    "]\n",
    "\n",
    "print(\"Can the model generate NEW puns using the neologism?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in generation_prompts:\n",
    "    output = trainer.generate_with_neologism(prompt, max_new_tokens=80, remote=REMOTE)\n",
    "    print(f\"\\nPrompt: {prompt.replace('{neo}', '[PUN]')}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train a \"Non-Pun\" Neologism for Comparison\n",
    "\n",
    "Let's train a contrasting neologism on non-pun sentences to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-pun training data\n",
    "nonpun_training_data = [\n",
    "    (\n",
    "        \"Here is an example of {neo}: The electrician fixed the wiring in the\",\n",
    "        \" kitchen yesterday.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I went to the bank to deposit my\",\n",
    "        \" paycheck.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A {neo}: Time passes quickly when you're having\",\n",
    "        \" fun.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Here's {neo}: She rode her bicycle to the grocery\",\n",
    "        \" store.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I'm reading a book about ancient\",\n",
    "        \" history.\"\n",
    "    ),\n",
    "    (\n",
    "        \"{neo} example: The fish swam in the clear blue\",\n",
    "        \" ocean.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Another {neo}: The math teacher explained the difficult\",\n",
    "        \" concept.\"\n",
    "    ),\n",
    "    (\n",
    "        \"{neo}: I marked the important dates on my\",\n",
    "        \" calendar.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train non-pun neologism\n",
    "nonpun_trainer = RemoteNeologismTrainer(model, placeholder_token=\"[LIT]\", lr=0.5)\n",
    "\n",
    "nonpun_prompts = [p for p, t in nonpun_training_data]\n",
    "nonpun_targets = [t for p, t in nonpun_training_data]\n",
    "\n",
    "print(\"Training non-pun neologism...\")\n",
    "nonpun_losses = []\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss = nonpun_trainer.train_step(nonpun_prompts, nonpun_targets, remote=REMOTE)\n",
    "    nonpun_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two neologisms\n",
    "print(\"Comparing PUN vs LITERAL neologism definitions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_prompt = \"The word {neo} means:\"\n",
    "\n",
    "pun_def = trainer.generate_with_neologism(comparison_prompt, max_new_tokens=50, remote=REMOTE)\n",
    "lit_def = nonpun_trainer.generate_with_neologism(comparison_prompt, max_new_tokens=50, remote=REMOTE)\n",
    "\n",
    "print(f\"\\nPUN neologism: {pun_def}\")\n",
    "print(f\"\\nLITERAL neologism: {lit_def}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Embedding Analysis\n",
    "\n",
    "How does the learned neologism embedding relate to existing word embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_tokens(model, embedding, k=10):\n",
    "    \"\"\"\n",
    "    Find the k closest tokens to a given embedding.\n",
    "    \"\"\"\n",
    "    # Get the embedding matrix\n",
    "    with model.trace(\"\", remote=False) as tracer:\n",
    "        embed_matrix = model.model.embed_tokens.weight.save()\n",
    "    \n",
    "    embed_matrix = embed_matrix.value.cpu().float()\n",
    "    embedding = embedding.float()\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    embedding_norm = embedding / embedding.norm()\n",
    "    matrix_norm = embed_matrix / embed_matrix.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    similarities = matrix_norm @ embedding_norm\n",
    "    top_k = similarities.topk(k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, sim in zip(top_k.indices, top_k.values):\n",
    "        token = model.tokenizer.decode([idx.item()])\n",
    "        results.append((token, sim.item()))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Find closest tokens to pun neologism\n",
    "print(\"Tokens closest to PUN neologism embedding:\")\n",
    "pun_neighbors = get_closest_tokens(model, trainer.neologism_embedding)\n",
    "for token, sim in pun_neighbors:\n",
    "    print(f\"  {repr(token):15} similarity: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nTokens closest to LITERAL neologism embedding:\")\n",
    "lit_neighbors = get_closest_tokens(model, nonpun_trainer.neologism_embedding)\n",
    "for token, sim in lit_neighbors:\n",
    "    print(f\"  {repr(token):15} similarity: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings for known pun-related words\n",
    "pun_related_words = [\"pun\", \"joke\", \"humor\", \"funny\", \"wordplay\", \"wit\"]\n",
    "\n",
    "def get_token_embedding(model, word):\n",
    "    \"\"\"Get embedding for a token.\"\"\"\n",
    "    token_id = model.tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "    \n",
    "    with model.trace(\"\", remote=False) as tracer:\n",
    "        embed_matrix = model.model.embed_tokens.weight.save()\n",
    "    \n",
    "    return embed_matrix.value[token_id].cpu()\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b) / (a.norm() * b.norm())\n",
    "\n",
    "print(\"Similarity between PUN neologism and pun-related words:\")\n",
    "for word in pun_related_words:\n",
    "    try:\n",
    "        word_emb = get_token_embedding(model, word)\n",
    "        sim = cosine_similarity(trainer.neologism_embedding.float(), word_emb.float())\n",
    "        print(f\"  {word:15} similarity: {sim.item():.4f}\")\n",
    "    except:\n",
    "        print(f\"  {word:15} (tokenization issue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Steering with Neologisms\n",
    "\n",
    "Can we use the neologism to steer generation toward puns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering: Can adding the neologism make outputs more pun-like?\n",
    "test_prompts = [\n",
    "    \"Why do scientists like\",\n",
    "    \"The chef said that cooking is\",\n",
    "    \"Musicians always\",\n",
    "]\n",
    "\n",
    "print(\"Steering comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Without neologism\n",
    "    with model.trace(prompt, remote=REMOTE) as tracer:\n",
    "        logits = model.output.logits.save()\n",
    "    \n",
    "    # Simple greedy generation for comparison\n",
    "    next_tokens = torch.argmax(logits.value[0, -1, :], dim=-1)\n",
    "    without_neo = model.tokenizer.decode([next_tokens.item()])\n",
    "    \n",
    "    # With neologism prefix\n",
    "    neo_prompt = f\"{{neo}} {prompt}\"\n",
    "    output_with_neo = trainer.generate_with_neologism(neo_prompt, max_new_tokens=20, remote=REMOTE)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"  Without neologism: ...{without_neo}...\")\n",
    "    print(f\"  With [PUN] prefix: {output_with_neo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Train a Concept Neologism\n",
    "\n",
    "Train a neologism for a different concept (e.g., sarcasm, metaphor, rhyme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create training data for your concept\n",
    "# Train a neologism\n",
    "# Test self-verbalization\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Neologism Arithmetic\n",
    "\n",
    "Can we combine neologism embeddings like word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try embedding arithmetic\n",
    "# e.g., pun_embedding - literal_embedding + something_else\n",
    "# What concept does the result represent?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Layer-Specific Neologisms\n",
    "\n",
    "Instead of intervening at the embedding layer, try intervening at intermediate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the trainer to intervene at a specific layer\n",
    "# Does the neologism capture different aspects at different layers?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Neologism learning** trains a new word embedding to capture a concept\n",
    "\n",
    "2. **Remote training via NDIF** works by downloading gradients and updating locally\n",
    "\n",
    "3. **Self-verbalization** lets us ask the model what the neologism means\n",
    "\n",
    "4. **Embedding analysis** reveals how the neologism relates to existing vocabulary\n",
    "\n",
    "5. **Steering** with neologisms can influence generation toward specific styles\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Neologisms provide a way to **extract** what the model thinks a concept means\n",
    "- The learned embedding captures statistical patterns from training examples\n",
    "- Self-verbalization can reveal \"machine-only synonyms\" that make sense to the model\n",
    "- This bridges interpretability and alignment: we can teach precise concepts to models\n",
    "\n",
    "### Connections to Course Themes\n",
    "\n",
    "| Week | Method | Connection |\n",
    "|------|--------|------------|\n",
    "| 1 | Logit Lens | Both reveal internal representations |\n",
    "| 4 | Geometry | Neologism embeddings live in same space |\n",
    "| 6 | Probes | Both use gradient descent on NDIF |\n",
    "| 8 | Circuits | Neologisms activate specific circuits |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
