{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training Neologisms via NDIF\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week10/neologism_training_ndif.ipynb)\n\nThis notebook demonstrates **neologism learning** - teaching a model a new word by training only its embedding - using NDIF for remote training on large models.\n\n**Key Idea:** We add a \"new word\" to the model's vocabulary and train its embedding to capture a specific concept. Then we can ask the model \"What does [neologism] mean?\" and it will explain the concept in natural language.\n\n## Why Neologisms?\n- **Concept extraction**: What does the model think a concept means?\n- **Steering**: Use the neologism to control model behavior\n- **Alignment**: Teach precise human concepts to models\n- **Interpretability**: Self-verbalization of learned representations\n\n## Remote Training with Sessions\nUsing nnsight's `session` and `iter` APIs, we run the **entire training loop** on NDIF:\n- The neologism embedding is created and updated remotely\n- The optimizer runs remotely\n- Only the final trained embedding is downloaded\n\nThis is much more efficient than downloading gradients each step!\n\n## References\n- [We Can't Understand AI Using our Existing Vocabulary](https://arxiv.org/abs/2502.07586) (Hewitt, Geirhos & Kim, 2025)\n- [nnsight documentation](https://nnsight.net/)\n- [NDIF](https://ndif.us/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Use NDIF for remote execution on large models\n",
    "REMOTE = True\n",
    "\n",
    "# For local testing, set REMOTE = False and use a smaller model\n",
    "if REMOTE:\n",
    "    MODEL_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
    "else:\n",
    "    MODEL_ID = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = LanguageModel(MODEL_ID, device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Embedding dim: {model.config.hidden_size}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: The Neologism Training Approach\n\nInstead of modifying the vocabulary, we:\n1. Choose a **placeholder token** (e.g., `[NEO]` or an unused token)\n2. Learn a **custom embedding** for this token\n3. **Intervene** during forward pass to replace the placeholder's embedding with our learned one\n4. Train using `session.iter()` - the entire loop runs on NDIF\n\nThis is similar to soft prompts / prefix tuning!\n\n### Data Loading Options\n\n**Option 1: In-memory dataset** (used in this notebook)\n- Data is serialized and sent to NDIF as part of the computation graph\n- Good for small datasets (< 1000 examples)\n\n**Option 2: Load from GitHub raw URL**\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files={\n    \"train\": \"https://raw.githubusercontent.com/your-org/repo/main/puns.csv\"\n})\n```\n\n**Option 3: HuggingFace Hub dataset**\n```python\nfrom datasets import Dataset\ndataset = Dataset.from_dict({\"prompt\": prompts, \"target\": targets})\ndataset.push_to_hub(\"your-username/pun-neologism-data\", private=True)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import nnsight\nfrom torch.utils.data import DataLoader\n\nclass RemoteNeologismTrainer:\n    \"\"\"\n    Train a neologism embedding using NDIF remote execution.\n\n    The neologism is represented by a placeholder token whose embedding\n    we replace during forward passes with our learned embedding.\n\n    Key feature: The ENTIRE training loop (all epochs) runs on NDIF using\n    session + iter. Only the final trained embedding is downloaded.\n    \"\"\"\n\n    def __init__(self, model, placeholder_token=\"[NEO]\", lr=0.1):\n        self.model = model\n        self.lr = lr\n\n        # Use a placeholder token - we'll replace its embedding\n        self.placeholder = placeholder_token\n\n        # Check if placeholder exists, if not use a rare token\n        placeholder_ids = model.tokenizer.encode(placeholder_token, add_special_tokens=False)\n        if len(placeholder_ids) == 1:\n            self.placeholder_id = placeholder_ids[0]\n        else:\n            # Use an uncommon token as placeholder\n            self.placeholder_id = model.config.vocab_size - 100\n            self.placeholder = model.tokenizer.decode([self.placeholder_id])\n\n        print(f\"Placeholder token: '{self.placeholder}' (id: {self.placeholder_id})\")\n\n        # Embedding dimension\n        self.embedding_dim = model.config.hidden_size\n        \n        # Neologism embedding will be created remotely during training\n        self.neologism_embedding = None\n\n    def get_prompt_with_neologism(self, template):\n        \"\"\"Create a prompt with the neologism placeholder.\"\"\"\n        return template.replace(\"{neo}\", self.placeholder)\n\n    def _prepare_data(self, prompts, targets):\n        \"\"\"Prepare tokenized training data.\"\"\"\n        prompts_with_neo = [self.get_prompt_with_neologism(p) for p in prompts]\n        full_texts = [p + t for p, t in zip(prompts_with_neo, targets)]\n\n        data = []\n        for text, prompt in zip(full_texts, prompts_with_neo):\n            tokens = self.model.tokenizer.encode(text)\n            prompt_tokens = self.model.tokenizer.encode(prompt)\n            neo_positions = [i for i, t in enumerate(tokens) if t == self.placeholder_id]\n            if neo_positions:  # Only include examples with neologism\n                data.append({\n                    'tokens': tokens,\n                    'prompt_len': len(prompt_tokens),\n                    'neo_positions': neo_positions\n                })\n        return data\n\n    def train(self, prompts, targets, n_epochs=20, batch_size=4, remote=True):\n        \"\"\"\n        Train the neologism embedding with ENTIRE loop on NDIF.\n\n        Uses nnsight session + iter to run all epochs remotely.\n        Only downloads the final trained embedding.\n\n        Args:\n            prompts: List of prompt templates with {neo} placeholder\n            targets: List of target completions\n            n_epochs: Number of training epochs\n            batch_size: Examples per batch\n            remote: Whether to run on NDIF\n\n        Returns:\n            losses: List of per-epoch losses\n        \"\"\"\n        # Prepare data\n        data = self._prepare_data(prompts, targets)\n        \n        # Create epoch list for iteration\n        epoch_data = list(range(n_epochs))\n\n        print(f\"Training {n_epochs} epochs with {len(data)} examples...\")\n        print(f\"Running entire loop on {'NDIF' if remote else 'local'}...\")\n\n        # Run ENTIRE training loop on NDIF using session + iter\n        with self.model.session(remote=remote) as session:\n            \n            # Initialize neologism embedding remotely\n            neo_emb = torch.nn.Parameter(torch.randn(self.embedding_dim) * 0.02)\n            \n            # Create optimizer - runs on remote device\n            optimizer = torch.optim.AdamW([neo_emb], lr=self.lr)\n            \n            # Create dataloader for batching\n            dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n            \n            # Storage for losses (will be populated remotely)\n            all_losses = []\n            \n            # Iterate over epochs using session.iter\n            with session.iter(epoch_data) as epoch:\n                \n                epoch_loss = torch.tensor(0.0)\n                n_batches = 0\n                \n                # Iterate over batches\n                with session.iter(dataloader) as batch:\n                    \n                    batch_loss = torch.tensor(0.0)\n                    \n                    # Process each example in batch\n                    for example in batch:\n                        tokens = example['tokens']\n                        prompt_len = example['prompt_len']\n                        neo_positions = example['neo_positions']\n                        \n                        input_ids = torch.tensor([tokens])\n                        \n                        # Forward pass with intervention\n                        with self.model.trace(input_ids) as tracer:\n                            # Get embedding output\n                            embed_output = self.model.model.embed_tokens.output\n                            \n                            # Replace embedding at placeholder positions\n                            neo_emb_dev = neo_emb.to(embed_output.device)\n                            for pos in neo_positions:\n                                embed_output[0, pos, :] = neo_emb_dev\n                            \n                            # Get logits and compute loss\n                            logits = self.model.output.logits\n                            target_logits = logits[0, prompt_len-1:-1, :]\n                            target_ids = torch.tensor(tokens[prompt_len:]).to(logits.device)\n                            \n                            loss = F.cross_entropy(target_logits, target_ids)\n                            loss.backward()\n                            \n                            batch_loss = batch_loss.to(loss.device) + loss\n                    \n                    # Accumulate epoch loss\n                    epoch_loss = epoch_loss.to(batch_loss.device) + batch_loss\n                    n_batches += 1\n                    \n                    # Update parameters\n                    optimizer.step()\n                    optimizer.zero_grad()\n                \n                # Log epoch loss\n                avg_epoch_loss = epoch_loss / max(n_batches, 1)\n                nnsight.log(f\"Epoch loss: \", avg_epoch_loss)\n                all_losses.append(avg_epoch_loss)\n            \n            # Save final embedding - THIS is what we download\n            final_embedding = neo_emb.detach().clone().save()\n            final_losses = [l.save() for l in all_losses]\n\n        # Store the trained embedding locally\n        self.neologism_embedding = final_embedding.value.cpu()\n        losses = [l.value.item() for l in final_losses]\n        \n        print(f\"Training complete! Final embedding norm: {self.neologism_embedding.norm().item():.4f}\")\n        \n        return losses\n\n    def train_step(self, prompts, targets, remote=True):\n        \"\"\"\n        Single training step (for compatibility with original API).\n        Processes all examples in one remote call.\n        \"\"\"\n        if self.neologism_embedding is None:\n            self.neologism_embedding = torch.randn(self.embedding_dim) * 0.02\n            \n        data = self._prepare_data(prompts, targets)\n        \n        with self.model.trace(remote=remote) as tracer:\n            neo_emb = self.neologism_embedding.clone()\n            neo_emb.requires_grad_(True)\n            total_loss = torch.tensor(0.0)\n            n_examples = 0\n\n            for example in data:\n                tokens = example['tokens']\n                prompt_len = example['prompt_len']\n                neo_positions = example['neo_positions']\n                input_ids = torch.tensor([tokens])\n\n                with tracer.invoke(input_ids):\n                    embed_output = self.model.model.embed_tokens.output\n                    neo_emb_dev = neo_emb.to(embed_output.device)\n                    for pos in neo_positions:\n                        embed_output[0, pos, :] = neo_emb_dev\n\n                    logits = self.model.output.logits\n                    target_logits = logits[0, prompt_len-1:-1, :]\n                    target_ids = torch.tensor(tokens[prompt_len:]).to(logits.device)\n                    example_loss = F.cross_entropy(target_logits, target_ids)\n                    total_loss = total_loss.to(example_loss.device) + example_loss\n                    n_examples += 1\n\n            avg_loss = total_loss / n_examples\n            avg_loss.backward()\n            final_grad = neo_emb.grad.save()\n            final_loss = avg_loss.save()\n\n        self.neologism_embedding -= self.lr * final_grad.value.cpu()\n        return final_loss.value.item()\n\n    def generate_with_neologism(self, prompt, max_new_tokens=50, remote=True):\n        \"\"\"Generate text using the learned neologism.\"\"\"\n        if self.neologism_embedding is None:\n            raise ValueError(\"No trained embedding. Call train() first.\")\n            \n        prompt_with_neo = self.get_prompt_with_neologism(prompt)\n        tokens = self.model.tokenizer.encode(prompt_with_neo)\n        neo_positions = [i for i, t in enumerate(tokens) if t == self.placeholder_id]\n\n        generated = list(tokens)\n\n        for _ in range(max_new_tokens):\n            input_ids = torch.tensor([generated])\n\n            with self.model.trace(remote=remote) as tracer:\n                with tracer.invoke(input_ids):\n                    embed_output = self.model.model.embed_tokens.output\n                    neo_emb = self.neologism_embedding.to(embed_output.device)\n                    for pos in neo_positions:\n                        if pos < embed_output.shape[1]:\n                            embed_output[0, pos, :] = neo_emb\n                    logits = self.model.output.logits.save()\n\n            next_token_logits = logits.value[0, -1, :]\n            next_token = torch.argmax(next_token_logits).item()\n\n            if next_token == self.model.tokenizer.eos_token_id:\n                break\n            generated.append(next_token)\n\n        return self.model.tokenizer.decode(generated)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train a \"Pun\" Neologism\n",
    "\n",
    "Let's train a neologism that captures the concept of \"pun\" - then ask the model what it means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: examples that demonstrate what a pun is\n",
    "pun_training_data = [\n",
    "    # Format: (prompt with {neo}, target completion)\n",
    "    (\n",
    "        \"Here is an example of {neo}: Why do electricians make good swimmers?\",\n",
    "        \" Because they know the current.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I used to be a banker, but I lost\",\n",
    "        \" interest.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A classic {neo}: Time flies like an arrow; fruit flies like a\",\n",
    "        \" banana.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Here's {neo}: Why can't a bicycle stand on its own?\",\n",
    "        \" Because it's two tired.\"\n",
    "    ),\n",
    "    (\n",
    "        \"This is {neo}: I'm reading a book about anti-gravity.\",\n",
    "        \" It's impossible to put down.\"\n",
    "    ),\n",
    "    (\n",
    "        \"{neo} example: What do you call a fish without eyes?\",\n",
    "        \" A fsh.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Another {neo}: The math teacher called in sick because she had too many\",\n",
    "        \" problems.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Classic {neo}: I used to work at a calendar factory but got fired for taking\",\n",
    "        \" a day off.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompts = [p for p, t in pun_training_data]\n",
    "targets = [t for p, t in pun_training_data]\n",
    "\n",
    "print(f\"Training examples: {len(pun_training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize trainer\ntrainer = RemoteNeologismTrainer(model, placeholder_token=\"[PUN]\", lr=0.5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the neologism - ENTIRE loop runs on NDIF!\n# Only the final embedding is downloaded\nlosses = trainer.train(\n    prompts, \n    targets, \n    n_epochs=20, \n    batch_size=4, \n    remote=REMOTE\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Neologism Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Self-Verbalization - What Does the Neologism Mean?\n",
    "\n",
    "Now we ask the model to explain what the neologism means in its own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model what the neologism means\n",
    "definition_prompts = [\n",
    "    \"The word {neo} means:\",\n",
    "    \"A {neo} is defined as:\",\n",
    "    \"{neo} refers to:\",\n",
    "    \"When someone says {neo}, they mean:\",\n",
    "]\n",
    "\n",
    "print(\"Model's understanding of the neologism:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in definition_prompts:\n",
    "    output = trainer.generate_with_neologism(prompt, max_new_tokens=50, remote=REMOTE)\n",
    "    print(f\"\\nPrompt: {prompt.replace('{neo}', '[PUN]')}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model can generate new puns using the neologism\n",
    "generation_prompts = [\n",
    "    \"Here is a new {neo}:\",\n",
    "    \"Tell me {neo}:\",\n",
    "    \"Give me an example of {neo}:\",\n",
    "]\n",
    "\n",
    "print(\"Can the model generate NEW puns using the neologism?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in generation_prompts:\n",
    "    output = trainer.generate_with_neologism(prompt, max_new_tokens=80, remote=REMOTE)\n",
    "    print(f\"\\nPrompt: {prompt.replace('{neo}', '[PUN]')}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train a \"Non-Pun\" Neologism for Comparison\n",
    "\n",
    "Let's train a contrasting neologism on non-pun sentences to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Non-pun training data\nnonpun_training_data = [\n    (\n        \"Here is an example of {neo}: The electrician fixed the wiring in the\",\n        \" kitchen yesterday.\"\n    ),\n    (\n        \"This is {neo}: I went to the bank to deposit my\",\n        \" paycheck.\"\n    ),\n    (\n        \"A {neo}: Time passes quickly when you're having\",\n        \" fun.\"\n    ),\n    (\n        \"Here's {neo}: She rode her bicycle to the grocery\",\n        \" store.\"\n    ),\n    (\n        \"This is {neo}: I'm reading a book about ancient\",\n        \" history.\"\n    ),\n    (\n        \"{neo} example: The fish swam in the clear blue\",\n        \" ocean.\"\n    ),\n    (\n        \"Another {neo}: The math teacher explained the difficult\",\n        \" concept.\"\n    ),\n    (\n        \"{neo}: I marked the important dates on my\",\n        \" calendar.\"\n    ),\n]\n\n# Train non-pun neologism\nnonpun_trainer = RemoteNeologismTrainer(model, placeholder_token=\"[LIT]\", lr=0.5)\n\nnonpun_prompts = [p for p, t in nonpun_training_data]\nnonpun_targets = [t for p, t in nonpun_training_data]\n\nprint(\"Training non-pun neologism (entire loop on NDIF)...\")\nnonpun_losses = nonpun_trainer.train(\n    nonpun_prompts, \n    nonpun_targets, \n    n_epochs=20, \n    batch_size=4, \n    remote=REMOTE\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two neologisms\n",
    "print(\"Comparing PUN vs LITERAL neologism definitions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_prompt = \"The word {neo} means:\"\n",
    "\n",
    "pun_def = trainer.generate_with_neologism(comparison_prompt, max_new_tokens=50, remote=REMOTE)\n",
    "lit_def = nonpun_trainer.generate_with_neologism(comparison_prompt, max_new_tokens=50, remote=REMOTE)\n",
    "\n",
    "print(f\"\\nPUN neologism: {pun_def}\")\n",
    "print(f\"\\nLITERAL neologism: {lit_def}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Embedding Analysis\n",
    "\n",
    "How does the learned neologism embedding relate to existing word embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_tokens(model, embedding, k=10):\n",
    "    \"\"\"\n",
    "    Find the k closest tokens to a given embedding.\n",
    "    \"\"\"\n",
    "    # Get the embedding matrix\n",
    "    with model.trace(\"\", remote=False) as tracer:\n",
    "        embed_matrix = model.model.embed_tokens.weight.save()\n",
    "    \n",
    "    embed_matrix = embed_matrix.value.cpu().float()\n",
    "    embedding = embedding.float()\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    embedding_norm = embedding / embedding.norm()\n",
    "    matrix_norm = embed_matrix / embed_matrix.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    similarities = matrix_norm @ embedding_norm\n",
    "    top_k = similarities.topk(k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, sim in zip(top_k.indices, top_k.values):\n",
    "        token = model.tokenizer.decode([idx.item()])\n",
    "        results.append((token, sim.item()))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Find closest tokens to pun neologism\n",
    "print(\"Tokens closest to PUN neologism embedding:\")\n",
    "pun_neighbors = get_closest_tokens(model, trainer.neologism_embedding)\n",
    "for token, sim in pun_neighbors:\n",
    "    print(f\"  {repr(token):15} similarity: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nTokens closest to LITERAL neologism embedding:\")\n",
    "lit_neighbors = get_closest_tokens(model, nonpun_trainer.neologism_embedding)\n",
    "for token, sim in lit_neighbors:\n",
    "    print(f\"  {repr(token):15} similarity: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings for known pun-related words\n",
    "pun_related_words = [\"pun\", \"joke\", \"humor\", \"funny\", \"wordplay\", \"wit\"]\n",
    "\n",
    "def get_token_embedding(model, word):\n",
    "    \"\"\"Get embedding for a token.\"\"\"\n",
    "    token_id = model.tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "    \n",
    "    with model.trace(\"\", remote=False) as tracer:\n",
    "        embed_matrix = model.model.embed_tokens.weight.save()\n",
    "    \n",
    "    return embed_matrix.value[token_id].cpu()\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b) / (a.norm() * b.norm())\n",
    "\n",
    "print(\"Similarity between PUN neologism and pun-related words:\")\n",
    "for word in pun_related_words:\n",
    "    try:\n",
    "        word_emb = get_token_embedding(model, word)\n",
    "        sim = cosine_similarity(trainer.neologism_embedding.float(), word_emb.float())\n",
    "        print(f\"  {word:15} similarity: {sim.item():.4f}\")\n",
    "    except:\n",
    "        print(f\"  {word:15} (tokenization issue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Steering with Neologisms\n",
    "\n",
    "Can we use the neologism to steer generation toward puns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering: Can adding the neologism make outputs more pun-like?\n",
    "test_prompts = [\n",
    "    \"Why do scientists like\",\n",
    "    \"The chef said that cooking is\",\n",
    "    \"Musicians always\",\n",
    "]\n",
    "\n",
    "print(\"Steering comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Without neologism\n",
    "    with model.trace(prompt, remote=REMOTE) as tracer:\n",
    "        logits = model.output.logits.save()\n",
    "    \n",
    "    # Simple greedy generation for comparison\n",
    "    next_tokens = torch.argmax(logits.value[0, -1, :], dim=-1)\n",
    "    without_neo = model.tokenizer.decode([next_tokens.item()])\n",
    "    \n",
    "    # With neologism prefix\n",
    "    neo_prompt = f\"{{neo}} {prompt}\"\n",
    "    output_with_neo = trainer.generate_with_neologism(neo_prompt, max_new_tokens=20, remote=REMOTE)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"  Without neologism: ...{without_neo}...\")\n",
    "    print(f\"  With [PUN] prefix: {output_with_neo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Train a Concept Neologism\n",
    "\n",
    "Train a neologism for a different concept (e.g., sarcasm, metaphor, rhyme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create training data for your concept\n",
    "# Train a neologism\n",
    "# Test self-verbalization\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Neologism Arithmetic\n",
    "\n",
    "Can we combine neologism embeddings like word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try embedding arithmetic\n",
    "# e.g., pun_embedding - literal_embedding + something_else\n",
    "# What concept does the result represent?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Layer-Specific Neologisms\n",
    "\n",
    "Instead of intervening at the embedding layer, try intervening at intermediate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the trainer to intervene at a specific layer\n",
    "# Does the neologism capture different aspects at different layers?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we learned:\n\n1. **Neologism learning** trains a new word embedding to capture a concept\n\n2. **Session-based training on NDIF** runs the entire training loop remotely:\n   - `model.session()` creates a persistent remote context\n   - `session.iter()` enables remote iteration over epochs and batches\n   - Optimizer and parameters live on the remote device\n   - Only the final trained embedding is downloaded\n\n3. **Self-verbalization** lets us ask the model what the neologism means\n\n4. **Embedding analysis** reveals how the neologism relates to existing vocabulary\n\n5. **Steering** with neologisms can influence generation toward specific styles\n\n### Session vs Trace Patterns\n\n| Pattern | Use Case | Data Transfer |\n|---------|----------|---------------|\n| `model.trace()` | Single forward/backward pass | Download gradients each call |\n| `model.session()` + `session.iter()` | Full training loop | Download only final result |\n\n### Key Insights\n\n- Neologisms provide a way to **extract** what the model thinks a concept means\n- The learned embedding captures statistical patterns from training examples\n- Self-verbalization can reveal \"machine-only synonyms\" that make sense to the model\n- Session-based training is much more efficient for multi-epoch optimization\n\n### Connections to Course Themes\n\n| Week | Method | Connection |\n|------|--------|------------|\n| 1 | Logit Lens | Both reveal internal representations |\n| 4 | Geometry | Neologism embeddings live in same space |\n| 6 | Probes | Both use gradient descent on NDIF |\n| 8 | Circuits | Neologisms activate specific circuits |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}