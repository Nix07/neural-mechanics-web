{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pun Circuit Discovery with EAP-IG\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week8/pun_circuits_eap.ipynb)\n",
    "\n",
    "This notebook demonstrates **automated circuit discovery** using [EAP-IG](https://github.com/hannamw/EAP-IG) (Edge Attribution Patching with Integrated Gradients) by Michael Hanna. We'll discover which attention heads and MLPs form the \"pun circuit\" - the minimal subgraph responsible for pun recognition.\n",
    "\n",
    "**Key Idea:** A circuit is a minimal, faithful subgraph that implements a specific behavior. EAP-IG efficiently estimates the causal importance of each edge in the computational graph, allowing us to prune unimportant components and extract the circuit.\n",
    "\n",
    "## Methods Covered\n",
    "- Edge Attribution Patching (EAP)\n",
    "- Integrated Gradients for improved attribution (EAP-IG)\n",
    "- Circuit extraction via top-n pruning\n",
    "- Faithfulness evaluation\n",
    "\n",
    "## References\n",
    "- [EAP-IG GitHub](https://github.com/hannamw/EAP-IG)\n",
    "- [Have Faith in Faithfulness](https://arxiv.org/abs/2403.17806) (Hanna, Pezzelle & Belinkov, 2024)\n",
    "- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install EAP-IG and TransformerLens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformer_lens torch\n",
    "!pip install -q git+https://github.com/hannamw/EAP-IG.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from eap.graph import Graph\n",
    "from eap.attribute import attribute\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Model with TransformerLens\n",
    "\n",
    "EAP-IG uses TransformerLens, which wraps HuggingFace models with hooks for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Small with TransformerLens\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"Layers: {model.cfg.n_layers}\")\n",
    "print(f\"Heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"Hidden size: {model.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prepare Pun Dataset for Circuit Discovery\n",
    "\n",
    "For circuit discovery, we need pairs of:\n",
    "- **Clean inputs**: Pun setups where the model should predict the pun word\n",
    "- **Corrupted inputs**: Modified versions where pun prediction should fail\n",
    "\n",
    "The circuit is the part of the model that, when preserved, maintains pun prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pun examples with their punchline words\n",
    "# Format: (setup, punchline, corrupted_setup)\n",
    "# Corrupted versions break the pun by removing the dual-meaning trigger\n",
    "\n",
    "pun_data = [\n",
    "    # Electrician pun - \"current\" has dual meaning (electrical + water)\n",
    "    {\n",
    "        \"clean\": \"Why do electricians make good swimmers? Because they know the\",\n",
    "        \"corrupted\": \"Why do teachers make good swimmers? Because they know the\",\n",
    "        \"target\": \" current\",\n",
    "        \"wrong\": \" water\"\n",
    "    },\n",
    "    # Banker pun - \"interest\" has dual meaning (financial + romantic)\n",
    "    {\n",
    "        \"clean\": \"Why did the banker break up with his girlfriend? He lost\",\n",
    "        \"corrupted\": \"Why did the teacher break up with his girlfriend? He lost\",\n",
    "        \"target\": \" interest\",\n",
    "        \"wrong\": \" hope\"\n",
    "    },\n",
    "    # Calendar pun - \"dates\" has dual meaning (calendar + romantic)\n",
    "    {\n",
    "        \"clean\": \"Why did the calendar break up? It had too many\",\n",
    "        \"corrupted\": \"Why did the couple break up? It had too many\",\n",
    "        \"target\": \" dates\",\n",
    "        \"wrong\": \" problems\"\n",
    "    },\n",
    "    # Bicycle pun - \"tired\" has dual meaning (exhausted + tires)\n",
    "    {\n",
    "        \"clean\": \"Why can't a bicycle stand on its own? Because it's two\",\n",
    "        \"corrupted\": \"Why can't the old man stand on his own? Because he's too\",\n",
    "        \"target\": \" tired\",\n",
    "        \"wrong\": \" weak\"\n",
    "    },\n",
    "    # Time pun - \"second\" has dual meaning (time unit + another)\n",
    "    {\n",
    "        \"clean\": \"I used to work at a clock factory but I got fired for taking a\",\n",
    "        \"corrupted\": \"I used to work at a food factory but I got fired for taking a\",\n",
    "        \"target\": \" second\",\n",
    "        \"wrong\": \" break\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(pun_data)} pun examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Clean: {pun_data[0]['clean']}\")\n",
    "print(f\"  Target: {pun_data[0]['target']}\")\n",
    "print(f\"  Corrupted: {pun_data[0]['corrupted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunDataset(Dataset):\n",
    "    \"\"\"Dataset for pun circuit discovery.\"\"\"\n",
    "    \n",
    "    def __init__(self, pun_data: List[dict], tokenizer):\n",
    "        self.data = pun_data\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize clean and corrupted prompts\n",
    "        clean_tokens = self.tokenizer.encode(item[\"clean\"])\n",
    "        corrupted_tokens = self.tokenizer.encode(item[\"corrupted\"])\n",
    "        \n",
    "        # Get target and wrong token IDs\n",
    "        target_id = self.tokenizer.encode(item[\"target\"])[0]\n",
    "        wrong_id = self.tokenizer.encode(item[\"wrong\"])[0]\n",
    "        \n",
    "        return {\n",
    "            \"clean\": torch.tensor(clean_tokens),\n",
    "            \"corrupted\": torch.tensor(corrupted_tokens),\n",
    "            \"target_id\": target_id,\n",
    "            \"wrong_id\": wrong_id\n",
    "        }\n",
    "\n",
    "def collate_pun(batch):\n",
    "    \"\"\"Collate function that pads sequences.\"\"\"\n",
    "    # Find max length\n",
    "    max_len = max(\n",
    "        max(item[\"clean\"].shape[0] for item in batch),\n",
    "        max(item[\"corrupted\"].shape[0] for item in batch)\n",
    "    )\n",
    "    \n",
    "    # Pad sequences\n",
    "    clean_padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    corrupted_padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        clean_padded[i, :item[\"clean\"].shape[0]] = item[\"clean\"]\n",
    "        corrupted_padded[i, :item[\"corrupted\"].shape[0]] = item[\"corrupted\"]\n",
    "    \n",
    "    target_ids = torch.tensor([item[\"target_id\"] for item in batch])\n",
    "    wrong_ids = torch.tensor([item[\"wrong_id\"] for item in batch])\n",
    "    \n",
    "    return clean_padded, corrupted_padded, target_ids, wrong_ids\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = PunDataset(pun_data, model.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=len(pun_data), collate_fn=collate_pun)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define the Metric Function\n",
    "\n",
    "EAP-IG needs a metric that measures \"pun-ness\". We'll use the logit difference between the pun word and a wrong alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun_logit_diff(logits, target_ids, wrong_ids, loss=False, mean=True):\n",
    "    \"\"\"\n",
    "    Compute logit difference between pun word and wrong word.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model output logits [batch, seq, vocab]\n",
    "        target_ids: Token IDs for correct pun words [batch]\n",
    "        wrong_ids: Token IDs for wrong words [batch]\n",
    "        loss: If True, return negative (for minimization)\n",
    "        mean: If True, return mean over batch\n",
    "    \"\"\"\n",
    "    # Get logits at last position\n",
    "    last_logits = logits[:, -1, :]  # [batch, vocab]\n",
    "    \n",
    "    # Get logits for target and wrong tokens\n",
    "    batch_size = logits.shape[0]\n",
    "    target_logits = last_logits[torch.arange(batch_size), target_ids]\n",
    "    wrong_logits = last_logits[torch.arange(batch_size), wrong_ids]\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = target_logits - wrong_logits\n",
    "    \n",
    "    if mean:\n",
    "        diff = diff.mean()\n",
    "    \n",
    "    if loss:\n",
    "        return -diff  # Negative for loss minimization\n",
    "    \n",
    "    return diff\n",
    "\n",
    "# Test the metric\n",
    "clean_batch, corrupted_batch, target_ids, wrong_ids = next(iter(dataloader))\n",
    "clean_batch = clean_batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(clean_batch)\n",
    "    diff = pun_logit_diff(logits, target_ids.to(device), wrong_ids.to(device))\n",
    "    print(f\"Clean logit difference (pun - wrong): {diff.item():.3f}\")\n",
    "    print(f\"Positive = model prefers pun word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build Computational Graph\n",
    "\n",
    "EAP-IG represents the model as a graph where nodes are components (attention heads, MLPs) and edges are connections via the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the computational graph from the model\n",
    "graph = Graph.from_model(model)\n",
    "\n",
    "print(f\"Graph created!\")\n",
    "print(f\"Number of nodes: {len(graph.nodes)}\")\n",
    "print(f\"Number of edges: {len(graph.edges)}\")\n",
    "\n",
    "# Show some example nodes\n",
    "print(f\"\\nExample nodes:\")\n",
    "for i, node in enumerate(list(graph.nodes.values())[:10]):\n",
    "    print(f\"  {node.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Run EAP-IG Attribution\n",
    "\n",
    "Now we compute the importance of each edge for pun recognition. EAP-IG estimates the causal effect of each edge by:\n",
    "1. Computing gradients w.r.t. the metric\n",
    "2. Multiplying by activation differences (clean - corrupted)\n",
    "3. Integrating over steps for more accurate attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pun_dataloader(model, pun_data):\n",
    "    \"\"\"\n",
    "    Create a dataloader in the format EAP-IG expects.\n",
    "    Returns: (clean_tokens, corrupted_tokens, target_ids, wrong_ids)\n",
    "    \"\"\"\n",
    "    clean_list = []\n",
    "    corrupted_list = []\n",
    "    target_list = []\n",
    "    wrong_list = []\n",
    "    \n",
    "    for item in pun_data:\n",
    "        clean_tokens = model.to_tokens(item[\"clean\"], prepend_bos=True)\n",
    "        corrupted_tokens = model.to_tokens(item[\"corrupted\"], prepend_bos=True)\n",
    "        \n",
    "        # Pad to same length if needed\n",
    "        max_len = max(clean_tokens.shape[1], corrupted_tokens.shape[1])\n",
    "        if clean_tokens.shape[1] < max_len:\n",
    "            pad = torch.zeros(1, max_len - clean_tokens.shape[1], dtype=torch.long, device=clean_tokens.device)\n",
    "            clean_tokens = torch.cat([clean_tokens, pad], dim=1)\n",
    "        if corrupted_tokens.shape[1] < max_len:\n",
    "            pad = torch.zeros(1, max_len - corrupted_tokens.shape[1], dtype=torch.long, device=corrupted_tokens.device)\n",
    "            corrupted_tokens = torch.cat([corrupted_tokens, pad], dim=1)\n",
    "        \n",
    "        target_id = model.to_tokens(item[\"target\"], prepend_bos=False)[0, 0].item()\n",
    "        wrong_id = model.to_tokens(item[\"wrong\"], prepend_bos=False)[0, 0].item()\n",
    "        \n",
    "        clean_list.append(clean_tokens)\n",
    "        corrupted_list.append(corrupted_tokens)\n",
    "        target_list.append(target_id)\n",
    "        wrong_list.append(wrong_id)\n",
    "    \n",
    "    # Stack into batches\n",
    "    # Need to pad all to same length\n",
    "    max_len = max(t.shape[1] for t in clean_list)\n",
    "    \n",
    "    clean_padded = []\n",
    "    corrupted_padded = []\n",
    "    for c, co in zip(clean_list, corrupted_list):\n",
    "        if c.shape[1] < max_len:\n",
    "            pad = torch.zeros(1, max_len - c.shape[1], dtype=torch.long, device=c.device)\n",
    "            c = torch.cat([c, pad], dim=1)\n",
    "        if co.shape[1] < max_len:\n",
    "            pad = torch.zeros(1, max_len - co.shape[1], dtype=torch.long, device=co.device)\n",
    "            co = torch.cat([co, pad], dim=1)\n",
    "        clean_padded.append(c)\n",
    "        corrupted_padded.append(co)\n",
    "    \n",
    "    clean_batch = torch.cat(clean_padded, dim=0)\n",
    "    corrupted_batch = torch.cat(corrupted_padded, dim=0)\n",
    "    target_ids = torch.tensor(target_list)\n",
    "    wrong_ids = torch.tensor(wrong_list)\n",
    "    \n",
    "    return [(clean_batch, corrupted_batch, target_ids, wrong_ids)]\n",
    "\n",
    "# Create dataloader\n",
    "eap_dataloader = create_pun_dataloader(model, pun_data)\n",
    "print(f\"Created EAP dataloader with {len(eap_dataloader)} batch(es)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric function for EAP-IG\n",
    "def metric_fn(logits, batch):\n",
    "    \"\"\"Metric function for EAP-IG.\"\"\"\n",
    "    clean_tokens, corrupted_tokens, target_ids, wrong_ids = batch\n",
    "    return pun_logit_diff(\n",
    "        logits, \n",
    "        target_ids.to(logits.device), \n",
    "        wrong_ids.to(logits.device),\n",
    "        loss=True,  # EAP-IG expects a loss to minimize\n",
    "        mean=True\n",
    "    )\n",
    "\n",
    "# Run EAP-IG attribution\n",
    "print(\"Running EAP-IG attribution...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "attribute(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    dataloader=eap_dataloader,\n",
    "    metric=metric_fn,\n",
    "    method='EAP-IG-inputs',  # Use integrated gradients\n",
    "    ig_steps=10  # Number of integration steps\n",
    ")\n",
    "\n",
    "print(\"Attribution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyze Edge Importance\n",
    "\n",
    "Let's see which edges are most important for pun recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all edges with their scores\n",
    "edge_scores = []\n",
    "for edge in graph.edges.values():\n",
    "    score = edge.score if hasattr(edge, 'score') and edge.score is not None else 0.0\n",
    "    edge_scores.append((edge.name, abs(score), score))\n",
    "\n",
    "# Sort by absolute score\n",
    "edge_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most important edges for pun recognition:\")\n",
    "print(\"=\" * 60)\n",
    "for name, abs_score, score in edge_scores[:20]:\n",
    "    print(f\"{name:40} score: {score:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by component type\n",
    "attn_scores = []\n",
    "mlp_scores = []\n",
    "\n",
    "for edge in graph.edges.values():\n",
    "    score = abs(edge.score) if hasattr(edge, 'score') and edge.score is not None else 0.0\n",
    "    if 'attn' in edge.name.lower() or 'a' in edge.name.split('->')[0]:\n",
    "        attn_scores.append(score)\n",
    "    elif 'mlp' in edge.name.lower() or 'm' in edge.name.split('->')[0]:\n",
    "        mlp_scores.append(score)\n",
    "\n",
    "print(f\"Attention edges: {len(attn_scores)}, mean |score|: {np.mean(attn_scores):.4f}\")\n",
    "print(f\"MLP edges: {len(mlp_scores)}, mean |score|: {np.mean(mlp_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize edge importance by layer\n",
    "layer_scores = {}\n",
    "\n",
    "for edge in graph.edges.values():\n",
    "    score = abs(edge.score) if hasattr(edge, 'score') and edge.score is not None else 0.0\n",
    "    # Extract layer from edge name\n",
    "    parts = edge.name.split('->')\n",
    "    if len(parts) >= 1:\n",
    "        src = parts[0]\n",
    "        # Try to extract layer number\n",
    "        for part in src.split('.'):\n",
    "            if part.isdigit():\n",
    "                layer = int(part)\n",
    "                if layer not in layer_scores:\n",
    "                    layer_scores[layer] = []\n",
    "                layer_scores[layer].append(score)\n",
    "                break\n",
    "\n",
    "# Plot mean score by layer\n",
    "layers = sorted(layer_scores.keys())\n",
    "mean_scores = [np.mean(layer_scores[l]) for l in layers]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(layers, mean_scores, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Mean |Edge Score|')\n",
    "plt.title('Edge Importance by Layer for Pun Recognition')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Extract the Pun Circuit\n",
    "\n",
    "Now we extract the minimal circuit by keeping only the top-scoring edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different circuit sizes\n",
    "circuit_sizes = [50, 100, 200, 500, 1000]\n",
    "\n",
    "print(\"Testing different circuit sizes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_edges in circuit_sizes:\n",
    "    # Create a fresh graph\n",
    "    test_graph = Graph.from_model(model)\n",
    "    \n",
    "    # Copy scores from attributed graph\n",
    "    for edge_name, edge in graph.edges.items():\n",
    "        if edge_name in test_graph.edges:\n",
    "            test_graph.edges[edge_name].score = edge.score\n",
    "    \n",
    "    # Apply top-n pruning\n",
    "    test_graph.apply_topn(n_edges, absolute=True)\n",
    "    \n",
    "    # Count remaining edges\n",
    "    active_edges = sum(1 for e in test_graph.edges.values() if e.in_graph)\n",
    "    print(f\"Circuit with top {n_edges} edges: {active_edges} edges active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final circuit with top 200 edges\n",
    "final_graph = Graph.from_model(model)\n",
    "\n",
    "# Copy scores\n",
    "for edge_name, edge in graph.edges.items():\n",
    "    if edge_name in final_graph.edges:\n",
    "        final_graph.edges[edge_name].score = edge.score\n",
    "\n",
    "# Apply pruning\n",
    "final_graph.apply_topn(200, absolute=True)\n",
    "\n",
    "# Get the circuit components\n",
    "circuit_edges = [(e.name, e.score) for e in final_graph.edges.values() if e.in_graph]\n",
    "circuit_edges.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(f\"Pun Circuit: {len(circuit_edges)} edges\")\n",
    "print(\"\\nTop edges in circuit:\")\n",
    "for name, score in circuit_edges[:15]:\n",
    "    print(f\"  {name}: {score:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Evaluate Circuit Faithfulness\n",
    "\n",
    "A circuit is **faithful** if it preserves the model's behavior. We test by:\n",
    "1. Running the full model (baseline)\n",
    "2. Running only the circuit (ablating everything else)\n",
    "3. Comparing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline (full model)\n",
    "print(\"Evaluating full model baseline...\")\n",
    "baseline_score = evaluate_baseline(\n",
    "    model=model,\n",
    "    dataloader=eap_dataloader,\n",
    "    metric=metric_fn\n",
    ")\n",
    "print(f\"Full model score: {baseline_score:.4f}\")\n",
    "\n",
    "# Evaluate circuit\n",
    "print(\"\\nEvaluating pun circuit...\")\n",
    "circuit_score = evaluate_graph(\n",
    "    model=model,\n",
    "    graph=final_graph,\n",
    "    dataloader=eap_dataloader,\n",
    "    metric=metric_fn\n",
    ")\n",
    "print(f\"Circuit score: {circuit_score:.4f}\")\n",
    "\n",
    "# Compute faithfulness\n",
    "faithfulness = circuit_score / baseline_score if baseline_score != 0 else 0\n",
    "print(f\"\\nFaithfulness: {faithfulness:.2%}\")\n",
    "print(f\"(Higher = circuit better captures full model behavior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Identify Key Attention Heads\n",
    "\n",
    "Let's identify which specific attention heads are most important in the pun circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scores by attention head\n",
    "head_scores = {}  # (layer, head) -> total score\n",
    "\n",
    "for edge in graph.edges.values():\n",
    "    if edge.score is None:\n",
    "        continue\n",
    "    \n",
    "    # Parse edge name to find attention heads\n",
    "    # Format varies, but typically includes layer.head info\n",
    "    name = edge.name\n",
    "    \n",
    "    # Look for attention head patterns\n",
    "    import re\n",
    "    # Match patterns like \"a5.h3\" or \"blocks.5.attn.hook_result\" \n",
    "    attn_match = re.search(r'a(\\d+)\\.h(\\d+)', name) or re.search(r'blocks\\.(\\d+).*attn', name)\n",
    "    \n",
    "    if attn_match:\n",
    "        layer = int(attn_match.group(1))\n",
    "        head = int(attn_match.group(2)) if len(attn_match.groups()) > 1 else 0\n",
    "        key = (layer, head)\n",
    "        if key not in head_scores:\n",
    "            head_scores[key] = 0\n",
    "        head_scores[key] += abs(edge.score)\n",
    "\n",
    "# Sort by score\n",
    "sorted_heads = sorted(head_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top attention heads for pun recognition:\")\n",
    "print(\"=\" * 40)\n",
    "for (layer, head), score in sorted_heads[:10]:\n",
    "    print(f\"Layer {layer}, Head {head}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of head importance\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "\n",
    "head_matrix = np.zeros((n_layers, n_heads))\n",
    "for (layer, head), score in head_scores.items():\n",
    "    if layer < n_layers and head < n_heads:\n",
    "        head_matrix[layer, head] = score\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(head_matrix, aspect='auto', cmap='Reds')\n",
    "plt.colorbar(label='Importance Score')\n",
    "plt.xlabel('Head')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Attention Head Importance for Pun Recognition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compare Circuit Sizes\n",
    "\n",
    "How does faithfulness change with circuit size? Find the minimal circuit that achieves >80% faithfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test circuit sizes from 10 to 1000\n",
    "# Plot faithfulness vs circuit size\n",
    "# Find the \"elbow\" - minimal size with good faithfulness\n",
    "\n",
    "sizes_to_test = [10, 25, 50, 100, 150, 200, 300, 500, 750, 1000]\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Ablation Validation\n",
    "\n",
    "Validate the circuit by ablating the top heads and measuring the drop in pun recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use TransformerLens hooks to:\n",
    "# 1. Ablate top-5 most important attention heads\n",
    "# 2. Measure change in pun logit difference\n",
    "# 3. Compare to ablating random heads\n",
    "\n",
    "# Hint: Use model.run_with_hooks() with zero ablation hooks\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare Pun Types\n",
    "\n",
    "Do different types of puns (homophone, homograph, semantic) use different circuits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create separate datasets for different pun types\n",
    "# Run EAP-IG on each\n",
    "# Compare the resulting circuits\n",
    "# Which components are shared vs unique?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Path Patching Validation\n",
    "\n",
    "Use path patching to validate specific edges in the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For the top-5 edges in the circuit:\n",
    "# 1. Path patch: replace clean edge with corrupted\n",
    "# 2. Measure effect on pun logit difference\n",
    "# 3. Confirm edges with high EAP-IG scores have high causal effects\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **EAP-IG** efficiently discovers circuits by estimating edge importance via integrated gradients\n",
    "\n",
    "2. **Circuit extraction** uses top-n pruning to find minimal subgraphs\n",
    "\n",
    "3. **Faithfulness evaluation** tests whether the circuit captures the full model's behavior\n",
    "\n",
    "4. **For puns**, we can identify specific attention heads and MLPs that contribute to pun recognition\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "- Is the pun circuit similar to known circuits (induction, binding)?\n",
    "- Do different types of puns share the same circuit?\n",
    "- How does the pun circuit compare to circuits for literal language?\n",
    "\n",
    "### Connections to Previous Weeks\n",
    "\n",
    "- **Week 4 (Geometry)**: Which layers in the circuit showed best pun separation?\n",
    "- **Week 5 (CMA)**: Do causally important positions match circuit components?\n",
    "- **Week 6 (Probes)**: Can we train probes on circuit activations only?\n",
    "- **Week 7 (Attribution)**: Do high-attribution tokens flow through circuit edges?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
