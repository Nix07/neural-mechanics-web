{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pun Circuit Tracing with Anthropic's Circuit-Tracer\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week8/pun_circuits_tracer.ipynb)\n",
    "\n",
    "This notebook demonstrates **attribution graph circuit tracing** using Anthropic's [circuit-tracer](https://github.com/safety-research/circuit-tracer) library. We'll trace the computational steps a model uses to recognize puns, revealing which features and their connections form the \"pun circuit.\"\n",
    "\n",
    "**Key Idea:** Attribution graphs show how transcoder features (interpretable units similar to SAE features) influence each other and the output. Unlike coarse methods that find important attention heads, this traces feature-level computation.\n",
    "\n",
    "## What Makes This Different\n",
    "- **Feature-level**: Traces individual interpretable features, not just attention heads/MLPs\n",
    "- **Transcoders**: Cross-layer sparse autoencoders that decompose MLP computations\n",
    "- **Interactive visualization**: Explore graphs in browser or Neuronpedia\n",
    "- **Interventions**: Modify feature values and observe output changes\n",
    "\n",
    "## References\n",
    "- [Circuit Tracing Methods Paper](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)\n",
    "- [circuit-tracer GitHub](https://github.com/safety-research/circuit-tracer)\n",
    "- [Neuronpedia Graph Explorer](https://www.neuronpedia.org/gemma-2-2b/graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install circuit-tracer from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install circuit-tracer\n",
    "!pip install -q git+https://github.com/safety-research/circuit-tracer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Model and Transcoders\n",
    "\n",
    "Circuit-tracer uses **transcoders** - cross-layer sparse autoencoders that decompose MLP computations into interpretable features. We'll use Qwen-3 (1.7B) which has good transcoder support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer import load_model_and_transcoders\n",
    "\n",
    "# Load Qwen-3 1.7B with its transcoders\n",
    "# Other options: \"gemma\" (Gemma-2-2B), \"llama\" (Llama-3.2-1B)\n",
    "# For larger Qwen: \"qwen-4b\", \"qwen-8b\", \"qwen-14b\"\n",
    "model, transcoders = load_model_and_transcoders(\n",
    "    transcoder_set=\"qwen-1.7b\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"Number of transcoders: {len(transcoders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use Gemma-2-2B (well-tested, good transcoder coverage)\n",
    "# Uncomment to use instead:\n",
    "\n",
    "# model, transcoders = load_model_and_transcoders(\n",
    "#     transcoder_set=\"gemma\",\n",
    "#     dtype=torch.bfloat16,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Trace a Pun Circuit\n",
    "\n",
    "Let's trace the circuit for our classic electrician pun. The attribution graph will show which features activate and influence the prediction of \"current.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer import attribute\n",
    "\n",
    "# Our pun prompt\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "\n",
    "print(f\"Tracing circuit for: '{pun_prompt}'\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "# Generate attribution graph\n",
    "graph = attribute(\n",
    "    model=model,\n",
    "    transcoders=transcoders,\n",
    "    prompt=pun_prompt,\n",
    "    max_n_logits=10,  # Trace top 10 predicted tokens\n",
    "    node_threshold=0.8,  # Keep nodes explaining 80% of influence\n",
    "    edge_threshold=0.98,  # Keep edges explaining 98% of influence\n",
    ")\n",
    "\n",
    "print(f\"\\nGraph created!\")\n",
    "print(f\"Number of nodes: {len(graph.nodes)}\")\n",
    "print(f\"Number of edges: {len(graph.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the top predicted tokens\n",
    "print(\"Top predicted tokens:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for logit_node in graph.logit_nodes[:10]:\n",
    "    token = logit_node.token\n",
    "    prob = logit_node.probability\n",
    "    print(f\"  {repr(token):15} p={prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Explore the Attribution Graph\n",
    "\n",
    "Let's examine which features are most important for the pun prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodes sorted by total influence\n",
    "def get_node_influence(node):\n",
    "    \"\"\"Compute total outgoing influence of a node.\"\"\"\n",
    "    return sum(abs(edge.weight) for edge in node.outgoing_edges)\n",
    "\n",
    "# Sort feature nodes by influence\n",
    "feature_nodes = [n for n in graph.nodes if hasattr(n, 'feature_id')]\n",
    "sorted_nodes = sorted(feature_nodes, key=get_node_influence, reverse=True)\n",
    "\n",
    "print(\"Top 15 most influential features:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for node in sorted_nodes[:15]:\n",
    "    influence = get_node_influence(node)\n",
    "    layer = node.layer if hasattr(node, 'layer') else '?'\n",
    "    feat_id = node.feature_id if hasattr(node, 'feature_id') else '?'\n",
    "    label = node.label if hasattr(node, 'label') else 'unlabeled'\n",
    "    \n",
    "    print(f\"Layer {layer:2}, Feature {feat_id:6}: influence={influence:.4f}\")\n",
    "    if label != 'unlabeled':\n",
    "        print(f\"    Label: {label[:60]}...\" if len(label) > 60 else f\"    Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which input tokens have the most influence\n",
    "token_nodes = [n for n in graph.nodes if hasattr(n, 'token_position')]\n",
    "\n",
    "print(\"\\nInput token influences:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for node in sorted(token_nodes, key=get_node_influence, reverse=True):\n",
    "    influence = get_node_influence(node)\n",
    "    if influence > 0.01:  # Only show significant tokens\n",
    "        token = node.token if hasattr(node, 'token') else '?'\n",
    "        pos = node.token_position if hasattr(node, 'token_position') else '?'\n",
    "        print(f\"  Position {pos:2} {repr(token):15}: influence={influence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize the Circuit\n",
    "\n",
    "Circuit-tracer includes an interactive visualization server. Let's launch it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer import serve_graph\n",
    "\n",
    "# Save graph for visualization\n",
    "graph_path = \"pun_circuit_graph.pt\"\n",
    "torch.save(graph, graph_path)\n",
    "print(f\"Graph saved to {graph_path}\")\n",
    "\n",
    "# Launch visualization server\n",
    "# This will open an interactive graph explorer\n",
    "print(\"\\nLaunching visualization server...\")\n",
    "print(\"Open the URL below in your browser to explore the circuit.\")\n",
    "print(\"(In Colab, you may need to use the 'Open in new tab' option)\")\n",
    "\n",
    "serve_graph(graph, port=8041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Display inline (if running locally with proper setup)\n",
    "# from circuit_tracer import visualize_graph\n",
    "# visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare Pun vs Literal Circuits\n",
    "\n",
    "How does the circuit differ when \"current\" is predicted in a pun vs literal context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace circuit for literal context\n",
    "literal_prompt = \"The electrician measured the electrical\"\n",
    "\n",
    "print(f\"Tracing literal context: '{literal_prompt}'\")\n",
    "\n",
    "literal_graph = attribute(\n",
    "    model=model,\n",
    "    transcoders=transcoders,\n",
    "    prompt=literal_prompt,\n",
    "    max_n_logits=10,\n",
    "    node_threshold=0.8,\n",
    "    edge_threshold=0.98,\n",
    ")\n",
    "\n",
    "print(f\"\\nLiteral graph: {len(literal_graph.nodes)} nodes, {len(literal_graph.edges)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top features between pun and literal\n",
    "def get_top_features(graph, n=20):\n",
    "    \"\"\"Extract top n features by influence.\"\"\"\n",
    "    feature_nodes = [node for node in graph.nodes if hasattr(node, 'feature_id')]\n",
    "    sorted_nodes = sorted(feature_nodes, key=get_node_influence, reverse=True)\n",
    "    return [(n.layer, n.feature_id, get_node_influence(n)) for n in sorted_nodes[:n]]\n",
    "\n",
    "pun_features = get_top_features(graph)\n",
    "literal_features = get_top_features(literal_graph)\n",
    "\n",
    "# Find shared and unique features\n",
    "pun_set = set((l, f) for l, f, _ in pun_features)\n",
    "literal_set = set((l, f) for l, f, _ in literal_features)\n",
    "\n",
    "shared = pun_set & literal_set\n",
    "pun_unique = pun_set - literal_set\n",
    "literal_unique = literal_set - pun_set\n",
    "\n",
    "print(f\"Shared features: {len(shared)}\")\n",
    "print(f\"Pun-unique features: {len(pun_unique)}\")\n",
    "print(f\"Literal-unique features: {len(literal_unique)}\")\n",
    "\n",
    "print(\"\\nPun-unique features (may relate to humor/wordplay):\")\n",
    "for layer, feat_id in list(pun_unique)[:10]:\n",
    "    print(f\"  Layer {layer}, Feature {feat_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Feature Interventions\n",
    "\n",
    "We can modify feature activations and observe how the model's output changes. This lets us test causal hypotheses about the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer import intervene\n",
    "\n",
    "# Get the most influential feature for the pun\n",
    "top_feature = sorted_nodes[0] if sorted_nodes else None\n",
    "\n",
    "if top_feature:\n",
    "    layer = top_feature.layer\n",
    "    feat_id = top_feature.feature_id\n",
    "    \n",
    "    print(f\"Testing intervention on Layer {layer}, Feature {feat_id}\")\n",
    "    print(f\"Original prompt: '{pun_prompt}'\")\n",
    "    \n",
    "    # Run without intervention\n",
    "    original_output = model.generate(\n",
    "        model.tokenizer(pun_prompt, return_tensors=\"pt\").input_ids.to(device),\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False\n",
    "    )\n",
    "    original_text = model.tokenizer.decode(original_output[0])\n",
    "    print(f\"\\nOriginal output: {original_text}\")\n",
    "    \n",
    "    # Run with feature ablated (set to 0)\n",
    "    ablated_output = intervene(\n",
    "        model=model,\n",
    "        transcoders=transcoders,\n",
    "        prompt=pun_prompt,\n",
    "        interventions={(layer, feat_id): 0.0},  # Set feature to 0\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "    print(f\"With feature ablated: {ablated_output}\")\n",
    "    \n",
    "    # Run with feature amplified (2x)\n",
    "    amplified_output = intervene(\n",
    "        model=model,\n",
    "        transcoders=transcoders,\n",
    "        prompt=pun_prompt,\n",
    "        interventions={(layer, feat_id): 2.0},  # Double the feature\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "    print(f\"With feature amplified: {amplified_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Trace Multiple Puns\n",
    "\n",
    "Let's trace circuits for different puns and look for common features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_prompts = [\n",
    "    \"Why do electricians make good swimmers? Because they know the\",\n",
    "    \"Why did the banker break up with his girlfriend? He lost\",\n",
    "    \"Why can't a bicycle stand on its own? Because it's two\",\n",
    "    \"I used to work at a clock factory but got fired for taking a\",\n",
    "]\n",
    "\n",
    "pun_graphs = {}\n",
    "\n",
    "for prompt in pun_prompts:\n",
    "    print(f\"Tracing: '{prompt[:50]}...'\")\n",
    "    \n",
    "    g = attribute(\n",
    "        model=model,\n",
    "        transcoders=transcoders,\n",
    "        prompt=prompt,\n",
    "        max_n_logits=5,\n",
    "        node_threshold=0.8,\n",
    "        edge_threshold=0.98,\n",
    "    )\n",
    "    \n",
    "    pun_graphs[prompt] = g\n",
    "    print(f\"  -> {len(g.nodes)} nodes, {len(g.edges)} edges\")\n",
    "\n",
    "print(\"\\nAll puns traced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features that appear across multiple puns\n",
    "from collections import Counter\n",
    "\n",
    "all_features = Counter()\n",
    "\n",
    "for prompt, g in pun_graphs.items():\n",
    "    features = get_top_features(g, n=30)\n",
    "    for layer, feat_id, _ in features:\n",
    "        all_features[(layer, feat_id)] += 1\n",
    "\n",
    "# Features appearing in multiple puns\n",
    "common_features = [(f, count) for f, count in all_features.items() if count >= 2]\n",
    "common_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Features appearing in multiple puns ({len(common_features)} total):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for (layer, feat_id), count in common_features[:15]:\n",
    "    print(f\"Layer {layer:2}, Feature {feat_id:6}: appears in {count}/{len(pun_prompts)} puns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Analyze Circuit Structure\n",
    "\n",
    "Let's examine the structure of the pun circuit - which layers are most active, and how do features connect across layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze layer distribution of important features\n",
    "layer_importance = {}\n",
    "\n",
    "for node in graph.nodes:\n",
    "    if hasattr(node, 'layer') and hasattr(node, 'feature_id'):\n",
    "        layer = node.layer\n",
    "        influence = get_node_influence(node)\n",
    "        \n",
    "        if layer not in layer_importance:\n",
    "            layer_importance[layer] = []\n",
    "        layer_importance[layer].append(influence)\n",
    "\n",
    "# Plot layer importance\n",
    "layers = sorted(layer_importance.keys())\n",
    "mean_importance = [np.mean(layer_importance[l]) for l in layers]\n",
    "total_importance = [np.sum(layer_importance[l]) for l in layers]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(layers, mean_importance, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Mean Feature Influence')\n",
    "axes[0].set_title('Average Feature Importance by Layer')\n",
    "\n",
    "axes[1].bar(layers, total_importance, color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Total Feature Influence')\n",
    "axes[1].set_title('Total Feature Importance by Layer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-layer connections\n",
    "layer_connections = {}  # (src_layer, dst_layer) -> total weight\n",
    "\n",
    "for edge in graph.edges:\n",
    "    src = edge.source\n",
    "    dst = edge.target\n",
    "    \n",
    "    if hasattr(src, 'layer') and hasattr(dst, 'layer'):\n",
    "        key = (src.layer, dst.layer)\n",
    "        if key not in layer_connections:\n",
    "            layer_connections[key] = 0\n",
    "        layer_connections[key] += abs(edge.weight)\n",
    "\n",
    "# Find strongest cross-layer connections\n",
    "sorted_connections = sorted(layer_connections.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Strongest cross-layer connections:\")\n",
    "print(\"=\" * 40)\n",
    "for (src_l, dst_l), weight in sorted_connections[:10]:\n",
    "    print(f\"Layer {src_l:2} -> Layer {dst_l:2}: weight={weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Trace the \"current\" Prediction Path\n",
    "\n",
    "Follow the path from input tokens to the \"current\" logit node. Which features directly influence it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the logit node for \"current\"\n",
    "# Trace backwards through the graph to find:\n",
    "# 1. Which features directly connect to the \"current\" logit\n",
    "# 2. Which earlier features feed into those\n",
    "# 3. Which input tokens ultimately drive the prediction\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Feature Interpretation\n",
    "\n",
    "Look up the top features on Neuronpedia to understand what concepts they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For the top 5 features in the pun circuit:\n",
    "# 1. Get their layer and feature ID\n",
    "# 2. Look them up on Neuronpedia (if available for your model)\n",
    "# 3. Record what concepts they seem to encode\n",
    "# 4. Hypothesize why they matter for puns\n",
    "\n",
    "# Neuronpedia URL pattern:\n",
    "# https://www.neuronpedia.org/{model}/{layer}-{transcoder_type}/{feature_id}\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Intervention Experiments\n",
    "\n",
    "Design interventions to test causal hypotheses about pun processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test these hypotheses with interventions:\n",
    "#\n",
    "# 1. If we ablate \"electrician\"-related features, does the model\n",
    "#    still predict \"current\"?\n",
    "#\n",
    "# 2. If we ablate \"swimmer\"-related features, does the model\n",
    "#    lose the water meaning of \"current\"?\n",
    "#\n",
    "# 3. Can we inject pun-related features into a literal context\n",
    "#    and make it predict a pun word?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compare to EAP-IG\n",
    "\n",
    "How do feature-level circuits compare to component-level (attention head/MLP) circuits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. Group features by their layer\n",
    "# 2. Sum importance within each layer\n",
    "# 3. Compare to EAP-IG results from the other notebook\n",
    "# 4. Discussion: Are the same layers important? \n",
    "#    What additional insight does feature-level give us?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Attribution graphs** trace feature-to-feature influences, not just component importance\n",
    "\n",
    "2. **Transcoders** decompose MLP computations into interpretable features (like SAEs, but cross-layer)\n",
    "\n",
    "3. **Interactive visualization** lets us explore circuit structure in detail\n",
    "\n",
    "4. **Interventions** let us test causal hypotheses about features\n",
    "\n",
    "5. **For puns**, we can trace exactly which semantic features bridge the dual meanings\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "- Do pun circuits have unique features, or just unusual combinations of common features?\n",
    "- Which features encode the \"humor\" or \"wordplay\" aspect vs the literal meanings?\n",
    "- Can we use these features to steer models toward or away from puns?\n",
    "\n",
    "### Advantages Over EAP-IG\n",
    "\n",
    "| Aspect | EAP-IG | Circuit-Tracer |\n",
    "|--------|--------|----------------|\n",
    "| Granularity | Attention heads, MLPs | Individual features |\n",
    "| Interpretability | Component-level | Concept-level |\n",
    "| Visualization | Basic graphs | Interactive explorer |\n",
    "| Interventions | Hook-based | Feature-level steering |\n",
    "| Requirement | TransformerLens | Transcoders (pretrained) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
