{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Training Probes on NDIF: Remote Training Loops\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week6/probe_training_ndif.ipynb)\n",
    "\n",
    "This notebook demonstrates **remote probe training** using [nnsight](https://nnsight.net/) and [NDIF](https://ndif.us/). Unlike previous notebooks where we collected activations remotely and trained locally, here the **entire training loop runs on NDIF**.\n",
    "\n",
    "**Why Remote Training?**\n",
    "- No need to download large activation tensors\n",
    "- All computation happens on NDIF GPUs\n",
    "- Essential for training probes on very large models\n",
    "\n",
    "We'll train probes to detect **puns** from Llama 3 70B activations, entirely remotely.\n",
    "\n",
    "## References\n",
    "- [nnsight documentation](https://nnsight.net/)\n",
    "- [NDIF - National Deep Inference Fabric](https://ndif.us/)\n",
    "- [Probing Classifiers](https://arxiv.org/abs/1909.03368) - Hewitt & Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnsight import LanguageModel\n",
    "from collections import defaultdict\n",
    "\n",
    "# Use remote=True to run on NDIF\n",
    "REMOTE = True\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B\", device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 1: Prepare Dataset\n",
    "\n",
    "We need paired pun and non-pun examples for probe training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: puns and non-puns\n",
    "train_puns = [\n",
    "    \"Why do electricians make good swimmers? Because they know the current.\",\n",
    "    \"Why did the banker break up with his girlfriend? He lost interest.\",\n",
    "    \"What do you call a fish without eyes? A fsh.\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything.\",\n",
    "    \"What did the ocean say to the beach? Nothing, it just waved.\",\n",
    "    \"Why do cows wear bells? Because their horns don't work.\",\n",
    "    \"I used to hate facial hair, but then it grew on me.\",\n",
    "    \"Why did the scarecrow win an award? He was outstanding in his field.\",\n",
    "    \"What do you call a bear with no teeth? A gummy bear.\",\n",
    "    \"Why can't a bicycle stand on its own? It's two tired.\",\n",
    "    \"What do you call a fake noodle? An impasta.\",\n",
    "    \"Why did the math book look so sad? It had too many problems.\",\n",
    "    \"What do you call a sleeping dinosaur? A dino-snore.\",\n",
    "    \"Why did the golfer bring two pairs of pants? In case he got a hole in one.\",\n",
    "    \"What did the grape say when stepped on? Nothing, it let out a little wine.\",\n",
    "]\n",
    "\n",
    "train_nonpuns = [\n",
    "    \"Why do electricians wear rubber gloves? To protect from electrical shocks.\",\n",
    "    \"Why did the banker open a savings account? To manage his finances better.\",\n",
    "    \"What do you call a fish that lives in rivers? A freshwater fish.\",\n",
    "    \"Why don't scientists make assumptions? They need empirical evidence.\",\n",
    "    \"What did the ocean look like today? Calm and peaceful.\",\n",
    "    \"Why do cows produce milk? To nourish their calves.\",\n",
    "    \"I used to avoid exercise, but then I started a routine.\",\n",
    "    \"Why did the scarecrow need repairs? It was damaged by weather.\",\n",
    "    \"What do you call a bear in winter? A hibernating animal.\",\n",
    "    \"Why can't a bicycle go uphill easily? The gradient is steep.\",\n",
    "    \"What do you call a fresh noodle? Al dente pasta.\",\n",
    "    \"Why did the math book look worn? It had been used for years.\",\n",
    "    \"What do you call a prehistoric reptile? A dinosaur.\",\n",
    "    \"Why did the golfer check the weather? To plan his game.\",\n",
    "    \"What did the grape taste like? Sweet and refreshing.\",\n",
    "]\n",
    "\n",
    "# Test data (held out)\n",
    "test_puns = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs.\",\n",
    "    \"What do you call a lazy kangaroo? A pouch potato.\",\n",
    "    \"Why did the stadium get hot? All the fans left.\",\n",
    "    \"What do you call a pig that does karate? A pork chop.\",\n",
    "    \"Why did the coffee file a police report? It got mugged.\",\n",
    "]\n",
    "\n",
    "test_nonpuns = [\n",
    "    \"Why do programmers use version control? To track code changes.\",\n",
    "    \"What do you call a baby kangaroo? A joey.\",\n",
    "    \"Why did the stadium close early? For maintenance.\",\n",
    "    \"What do you call a pig on a farm? Livestock.\",\n",
    "    \"Why did the coffee taste bitter? It was over-extracted.\",\n",
    "]\n",
    "\n",
    "print(f\"Training: {len(train_puns)} puns, {len(train_nonpuns)} non-puns\")\n",
    "print(f\"Test: {len(test_puns)} puns, {len(test_nonpuns)} non-puns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 2: Remote Probe Training\n",
    "\n",
    "The key insight: we can create a probe (linear layer) and run the **entire training loop** inside nnsight's trace context. All computation happens on NDIF - no large tensors are transferred to our local machine.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Create a `torch.nn.Linear` probe\n",
    "2. Inside the trace context:\n",
    "   - Run the main model to get hidden states\n",
    "   - Pass hidden states through the probe\n",
    "   - Compute loss\n",
    "   - Backpropagate through the probe\n",
    "3. Update probe weights (gradient step)\n",
    "4. Repeat for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteProbeTrainer:\n",
    "    \"\"\"\n",
    "    Train a linear probe entirely on NDIF using nnsight.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_idx, hidden_size, lr=0.01):\n",
    "        self.model = model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Initialize probe weights (will be moved to device during training)\n",
    "        # Binary classification: pun (1) vs non-pun (0)\n",
    "        self.probe_weight = torch.randn(1, hidden_size) * 0.01\n",
    "        self.probe_bias = torch.zeros(1)\n",
    "    \n",
    "    def train_step(self, texts, labels, remote=True):\n",
    "        \"\"\"\n",
    "        Run one training step on a batch of texts.\n",
    "        Returns the loss value.\n",
    "        \"\"\"\n",
    "        batch_size = len(texts)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "        # Store gradients\n",
    "        weight_grad = None\n",
    "        bias_grad = None\n",
    "        loss_value = None\n",
    "        \n",
    "        with self.model.trace(texts, remote=remote) as tracer:\n",
    "            # Get hidden states at the target layer, last token position\n",
    "            # Shape: (batch_size, seq_len, hidden_size)\n",
    "            hidden_states = self.model.model.layers[self.layer_idx].output[0]\n",
    "            \n",
    "            # Extract last token for each example\n",
    "            # For batched inference, we need to handle variable sequence lengths\n",
    "            # nnsight handles this - we get (batch_size, hidden_size) for last positions\n",
    "            last_hidden = hidden_states[:, -1, :]  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Move probe weights to device (inside trace, they're on the remote device)\n",
    "            w = self.probe_weight.to(last_hidden.device)\n",
    "            b = self.probe_bias.to(last_hidden.device)\n",
    "            w.requires_grad_(True)\n",
    "            b.requires_grad_(True)\n",
    "            \n",
    "            # Forward through probe: logits = hidden @ W^T + b\n",
    "            logits = (last_hidden @ w.T).squeeze(-1) + b  # (batch_size,)\n",
    "            \n",
    "            # Binary cross-entropy loss\n",
    "            labels_dev = labels_tensor.to(logits.device)\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                logits, labels_dev\n",
    "            )\n",
    "            \n",
    "            # Backward pass through probe\n",
    "            loss.backward()\n",
    "            \n",
    "            # Save gradients and loss\n",
    "            weight_grad = w.grad.save()\n",
    "            bias_grad = b.grad.save()\n",
    "            loss_value = loss.save()\n",
    "        \n",
    "        # Update weights using saved gradients (this happens locally)\n",
    "        self.probe_weight -= self.lr * weight_grad.value.cpu()\n",
    "        self.probe_bias -= self.lr * bias_grad.value.cpu()\n",
    "        \n",
    "        return loss_value.value.item()\n",
    "    \n",
    "    def evaluate(self, texts, labels, remote=True):\n",
    "        \"\"\"\n",
    "        Evaluate probe accuracy on a batch.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        with self.model.trace(texts, remote=remote) as tracer:\n",
    "            hidden_states = self.model.model.layers[self.layer_idx].output[0]\n",
    "            last_hidden = hidden_states[:, -1, :]\n",
    "            \n",
    "            w = self.probe_weight.to(last_hidden.device)\n",
    "            b = self.probe_bias.to(last_hidden.device)\n",
    "            \n",
    "            logits = (last_hidden @ w.T).squeeze(-1) + b\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float().save()\n",
    "        \n",
    "        preds_np = preds.value.cpu().numpy()\n",
    "        labels_np = np.array(labels)\n",
    "        accuracy = np.mean(preds_np == labels_np)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Train a Probe at One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a layer to probe (middle layer often works well)\n",
    "target_layer = model.config.num_hidden_layers // 2\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "print(f\"Training probe at layer {target_layer}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = RemoteProbeTrainer(\n",
    "    model=model,\n",
    "    layer_idx=target_layer,\n",
    "    hidden_size=hidden_size,\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "train_texts = train_puns + train_nonpuns\n",
    "train_labels = [1.0] * len(train_puns) + [0.0] * len(train_nonpuns)\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(train_texts))\n",
    "train_texts = [train_texts[i] for i in indices]\n",
    "train_labels = [train_labels[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 10\n",
    "batch_size = 5  # Small batches for stability\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Prepare test data\n",
    "test_texts = test_puns + test_nonpuns\n",
    "test_labels = [1.0] * len(test_puns) + [0.0] * len(test_nonpuns)\n",
    "\n",
    "print(\"Training probe...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(train_texts), batch_size):\n",
    "        batch_texts = train_texts[i:i+batch_size]\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "        \n",
    "        if len(batch_texts) < 2:  # Skip very small batches\n",
    "            continue\n",
    "            \n",
    "        loss = trainer.train_step(batch_texts, batch_labels, remote=REMOTE)\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = trainer.evaluate(train_texts, train_labels, remote=REMOTE)\n",
    "    test_acc = trainer.evaluate(test_texts, test_labels, remote=REMOTE)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Train Acc={train_acc:.1%}, Test Acc={test_acc:.1%}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses, 'b-o')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, 'b-o', label='Train')\n",
    "ax2.plot(test_accs, 'r-o', label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 3: Layer-Wise Probe Analysis\n",
    "\n",
    "Train probes at multiple layers to see where pun information is most accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe_at_layer(model, layer_idx, train_texts, train_labels,\n",
    "                         test_texts, test_labels, n_epochs=10, lr=0.1,\n",
    "                         remote=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Train a probe at a specific layer and return final accuracies.\n",
    "    \"\"\"\n",
    "    hidden_size = model.config.hidden_size\n",
    "    trainer = RemoteProbeTrainer(model, layer_idx, hidden_size, lr=lr)\n",
    "    \n",
    "    batch_size = 5\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, len(train_texts), batch_size):\n",
    "            batch_texts = train_texts[i:i+batch_size]\n",
    "            batch_labels = train_labels[i:i+batch_size]\n",
    "            if len(batch_texts) < 2:\n",
    "                continue\n",
    "            trainer.train_step(batch_texts, batch_labels, remote=remote)\n",
    "        \n",
    "        if verbose:\n",
    "            acc = trainer.evaluate(test_texts, test_labels, remote=remote)\n",
    "            print(f\"  Epoch {epoch+1}: {acc:.1%}\")\n",
    "    \n",
    "    train_acc = trainer.evaluate(train_texts, train_labels, remote=remote)\n",
    "    test_acc = trainer.evaluate(test_texts, test_labels, remote=remote)\n",
    "    \n",
    "    return train_acc, test_acc, trainer.probe_weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at multiple layers\n",
    "n_layers = model.config.num_hidden_layers\n",
    "layers_to_probe = list(range(0, n_layers, n_layers // 8))  # Every 1/8th layer\n",
    "if n_layers - 1 not in layers_to_probe:\n",
    "    layers_to_probe.append(n_layers - 1)\n",
    "\n",
    "print(f\"Probing layers: {layers_to_probe}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in layers_to_probe:\n",
    "    print(f\"Training probe at layer {layer_idx}...\")\n",
    "    \n",
    "    train_acc, test_acc, weights = train_probe_at_layer(\n",
    "        model, layer_idx,\n",
    "        train_texts, train_labels,\n",
    "        test_texts, test_labels,\n",
    "        n_epochs=10,\n",
    "        remote=REMOTE\n",
    "    )\n",
    "    \n",
    "    layer_results[layer_idx] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'weights': weights\n",
    "    }\n",
    "    \n",
    "    print(f\"  Layer {layer_idx}: Train={train_acc:.1%}, Test={test_acc:.1%}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer-wise probe performance\n",
    "layers = sorted(layer_results.keys())\n",
    "train_accs = [layer_results[l]['train_acc'] for l in layers]\n",
    "test_accs = [layer_results[l]['test_acc'] for l in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layers, train_accs, 'b-o', label='Train', markersize=8)\n",
    "plt.plot(layers, test_accs, 'r-o', label='Test', markersize=8)\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Probe Accuracy Across Layers: Where is Pun Information?')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.4, 1.05)\n",
    "\n",
    "# Mark the best layer\n",
    "best_layer = max(layer_results.keys(), key=lambda l: layer_results[l]['test_acc'])\n",
    "best_acc = layer_results[best_layer]['test_acc']\n",
    "plt.scatter([best_layer], [best_acc], s=200, c='green', marker='*', \n",
    "            zorder=5, label=f'Best: L{best_layer}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best layer: {best_layer} with test accuracy {best_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 4: Control Tasks\n",
    "\n",
    "Validate that the probe detects puns, not spurious features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control 1: Random labels\n",
    "# If probe achieves high accuracy with random labels, it's overfitting\n",
    "\n",
    "print(\"Control Task 1: Random Labels\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "random_labels = np.random.randint(0, 2, len(train_labels)).astype(float).tolist()\n",
    "\n",
    "random_train_acc, random_test_acc, _ = train_probe_at_layer(\n",
    "    model, best_layer,\n",
    "    train_texts, random_labels,\n",
    "    test_texts, test_labels,  # Use real test labels for evaluation\n",
    "    n_epochs=10,\n",
    "    remote=REMOTE\n",
    ")\n",
    "\n",
    "print(f\"Random label probe - Train: {random_train_acc:.1%}, Test: {random_test_acc:.1%}\")\n",
    "print(f\"Real label probe - Test: {layer_results[best_layer]['test_acc']:.1%}\")\n",
    "\n",
    "if random_test_acc < 0.6:\n",
    "    print(\"PASS: Random labels give random performance (probe is not overfitting)\")\n",
    "else:\n",
    "    print(\"WARNING: Random labels give high accuracy (possible overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control 2: Edge cases\n",
    "# Does the probe respond to puns it hasn't seen?\n",
    "\n",
    "edge_cases = [\n",
    "    # Clear puns (should classify as pun)\n",
    "    (\"Why do bees have sticky hair? They use honeycombs.\", 1),\n",
    "    (\"What do you call a dinosaur that crashes cars? Tyrannosaurus wrecks.\", 1),\n",
    "    \n",
    "    # Questions that aren't puns (should NOT classify as pun)\n",
    "    (\"Why is the sky blue? Due to Rayleigh scattering.\", 0),\n",
    "    (\"What is the capital of France? Paris.\", 0),\n",
    "    \n",
    "    # Non-question puns (harder case)\n",
    "    (\"I'm reading a book about anti-gravity. It's impossible to put down.\", 1),\n",
    "    (\"I told my wife she was drawing her eyebrows too high. She looked surprised.\", 1),\n",
    "]\n",
    "\n",
    "print(\"\\nControl Task 2: Edge Cases\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the best layer's trainer\n",
    "hidden_size = model.config.hidden_size\n",
    "edge_trainer = RemoteProbeTrainer(model, best_layer, hidden_size)\n",
    "edge_trainer.probe_weight = layer_results[best_layer]['weights'].clone()\n",
    "\n",
    "edge_texts = [e[0] for e in edge_cases]\n",
    "edge_labels = [float(e[1]) for e in edge_cases]\n",
    "\n",
    "# Get predictions\n",
    "with model.trace(edge_texts, remote=REMOTE) as tracer:\n",
    "    hidden = model.model.layers[best_layer].output[0]\n",
    "    last_hidden = hidden[:, -1, :]\n",
    "    w = edge_trainer.probe_weight.to(last_hidden.device)\n",
    "    b = edge_trainer.probe_bias.to(last_hidden.device)\n",
    "    logits = (last_hidden @ w.T).squeeze(-1) + b\n",
    "    probs = torch.sigmoid(logits).save()\n",
    "\n",
    "probs_np = probs.value.cpu().numpy()\n",
    "\n",
    "for (text, expected), prob in zip(edge_cases, probs_np):\n",
    "    pred = \"PUN\" if prob > 0.5 else \"NOT PUN\"\n",
    "    correct = (prob > 0.5) == expected\n",
    "    status = \"correct\" if correct else \"WRONG\"\n",
    "    print(f\"P(pun)={prob:.2f} [{pred}] {status}\")\n",
    "    print(f\"  {text[:60]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Part 5: Compare with Causal Direction\n",
    "\n",
    "Does the probe's learned direction match the mass mean-difference direction from Week 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_difference_direction(model, layer_idx, pun_texts, nonpun_texts, remote=True):\n",
    "    \"\"\"\n",
    "    Compute the mass mean-difference direction (Week 4 style).\n",
    "    \"\"\"\n",
    "    # Get pun activations\n",
    "    with model.trace(pun_texts, remote=remote) as tracer:\n",
    "        pun_hidden = model.model.layers[layer_idx].output[0][:, -1, :].save()\n",
    "    pun_mean = pun_hidden.value.mean(dim=0).cpu()\n",
    "    \n",
    "    # Get non-pun activations\n",
    "    with model.trace(nonpun_texts, remote=remote) as tracer:\n",
    "        nonpun_hidden = model.model.layers[layer_idx].output[0][:, -1, :].save()\n",
    "    nonpun_mean = nonpun_hidden.value.mean(dim=0).cpu()\n",
    "    \n",
    "    # Mean difference direction\n",
    "    direction = pun_mean - nonpun_mean\n",
    "    direction_normalized = direction / direction.norm()\n",
    "    \n",
    "    return direction_normalized\n",
    "\n",
    "# Compute mean-difference direction\n",
    "mean_diff_direction = compute_mean_difference_direction(\n",
    "    model, best_layer, train_puns, train_nonpuns, remote=REMOTE\n",
    ")\n",
    "\n",
    "# Get probe direction (the weight vector)\n",
    "probe_direction = layer_results[best_layer]['weights'].squeeze()\n",
    "probe_direction = probe_direction / probe_direction.norm()\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = torch.dot(mean_diff_direction, probe_direction).item()\n",
    "\n",
    "print(f\"Cosine similarity between probe direction and mean-difference direction:\")\n",
    "print(f\"  {cosine_sim:.4f}\")\n",
    "print()\n",
    "\n",
    "if abs(cosine_sim) > 0.8:\n",
    "    print(\"HIGH similarity: Probe learned the same direction as mean-difference.\")\n",
    "elif abs(cosine_sim) > 0.5:\n",
    "    print(\"MODERATE similarity: Probe learned a related but distinct direction.\")\n",
    "else:\n",
    "    print(\"LOW similarity: Probe learned a different direction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Exercise 1: MLP Probe\n",
    "\n",
    "Implement a nonlinear (MLP) probe and compare to the linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify RemoteProbeTrainer to support MLP probes\n",
    "# Add a hidden layer with ReLU activation\n",
    "# Compare accuracy: does MLP do better than linear?\n",
    "# If yes, pun representation may be nonlinear\n",
    "\n",
    "class RemoteMLPProbeTrainer:\n",
    "    \"\"\"\n",
    "    Train an MLP probe entirely on NDIF.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_idx, hidden_size, mlp_hidden=64, lr=0.01):\n",
    "        self.model = model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Two-layer MLP: input -> hidden -> output\n",
    "        self.w1 = torch.randn(mlp_hidden, hidden_size) * 0.01\n",
    "        self.b1 = torch.zeros(mlp_hidden)\n",
    "        self.w2 = torch.randn(1, mlp_hidden) * 0.01\n",
    "        self.b2 = torch.zeros(1)\n",
    "    \n",
    "    # TODO: Implement train_step and evaluate methods\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Exercise 2: Position Analysis\n",
    "\n",
    "We trained on the last token. How does probe accuracy vary with position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify training to extract from different positions\n",
    "# - Last token (current approach)\n",
    "# - Middle of sequence\n",
    "# - Average over all positions\n",
    "# \n",
    "# Question: At which position is pun information most accessible?\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Exercise 3: Causal Validation\n",
    "\n",
    "Use the probe direction for steering. If steering along the probe direction makes non-puns more \"pun-like,\" the direction is causally meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement steering with probe direction\n",
    "# 1. Take a non-pun sentence\n",
    "# 2. Add the probe direction to activations at the probed layer\n",
    "# 3. See if the model's output changes in pun-like ways\n",
    "#\n",
    "# This validates that the probe captures causally relevant information\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Remote probe training** - the entire training loop runs on NDIF, no large tensor downloads\n",
    "\n",
    "2. **Layer-wise analysis** - probes reveal which layers contain pun information\n",
    "\n",
    "3. **Control tasks** - random labels and edge cases validate probe quality\n",
    "\n",
    "4. **Direction comparison** - probe weights align (or don't) with mean-difference direction\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Probe accuracy measures **linear accessibility** of information\n",
    "- High accuracy does NOT prove the model uses this information\n",
    "- Control tasks are essential for validation\n",
    "- Compare probes with causal methods (Week 5) for complete picture\n",
    "\n",
    "### For Your Research\n",
    "\n",
    "1. Apply remote training to your concept\n",
    "2. Find which layers encode your concept (layer-wise probing)\n",
    "3. Run control tasks to validate\n",
    "4. Compare probe direction with your Week 4 concept direction\n",
    "5. Use Week 5 causal methods to verify the probe captures causally relevant information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
