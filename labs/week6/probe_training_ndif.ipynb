{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Training Probes on NDIF: Remote Training Loops\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week6/probe_training_ndif.ipynb)\n\nThis notebook demonstrates **remote probe training** using [nnsight](https://nnsight.net/) and [NDIF](https://ndif.us/). The **entire training loop** - all epochs, all batches - runs on NDIF using session-based execution.\n\n**Why Remote Training?**\n- No need to download large activation tensors\n- All computation happens on NDIF GPUs\n- Essential for training probes on very large models\n\n**Session-Based Training:**\nUsing `model.session()` and `session.iter()`, we run the complete training loop remotely:\n- Probe weights are created and updated on NDIF\n- Optimizer runs on NDIF\n- Only the final trained weights are downloaded\n\nWe'll train probes to detect **puns** from Llama 3 70B activations, entirely remotely.\n\n## References\n- [nnsight documentation](https://nnsight.net/)\n- [NDIF - National Deep Inference Fabric](https://ndif.us/)\n- [Probing Classifiers](https://arxiv.org/abs/1909.03368) - Hewitt & Liang"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nnsight\nfrom nnsight import LanguageModel, CONFIG\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\n\n# Configure NDIF API key from Colab secrets\ntry:\n    from google.colab import userdata\n    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\nexcept:\n    pass  # Not in Colab or secret not set\n\n# Use remote=True to run on NDIF\nREMOTE = True\n\n# For reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-70B\", device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 1: Prepare Dataset\n",
    "\n",
    "We need paired pun and non-pun examples for probe training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: puns and non-puns\n",
    "train_puns = [\n",
    "    \"Why do electricians make good swimmers? Because they know the current.\",\n",
    "    \"Why did the banker break up with his girlfriend? He lost interest.\",\n",
    "    \"What do you call a fish without eyes? A fsh.\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything.\",\n",
    "    \"What did the ocean say to the beach? Nothing, it just waved.\",\n",
    "    \"Why do cows wear bells? Because their horns don't work.\",\n",
    "    \"I used to hate facial hair, but then it grew on me.\",\n",
    "    \"Why did the scarecrow win an award? He was outstanding in his field.\",\n",
    "    \"What do you call a bear with no teeth? A gummy bear.\",\n",
    "    \"Why can't a bicycle stand on its own? It's two tired.\",\n",
    "    \"What do you call a fake noodle? An impasta.\",\n",
    "    \"Why did the math book look so sad? It had too many problems.\",\n",
    "    \"What do you call a sleeping dinosaur? A dino-snore.\",\n",
    "    \"Why did the golfer bring two pairs of pants? In case he got a hole in one.\",\n",
    "    \"What did the grape say when stepped on? Nothing, it let out a little wine.\",\n",
    "]\n",
    "\n",
    "train_nonpuns = [\n",
    "    \"Why do electricians wear rubber gloves? To protect from electrical shocks.\",\n",
    "    \"Why did the banker open a savings account? To manage his finances better.\",\n",
    "    \"What do you call a fish that lives in rivers? A freshwater fish.\",\n",
    "    \"Why don't scientists make assumptions? They need empirical evidence.\",\n",
    "    \"What did the ocean look like today? Calm and peaceful.\",\n",
    "    \"Why do cows produce milk? To nourish their calves.\",\n",
    "    \"I used to avoid exercise, but then I started a routine.\",\n",
    "    \"Why did the scarecrow need repairs? It was damaged by weather.\",\n",
    "    \"What do you call a bear in winter? A hibernating animal.\",\n",
    "    \"Why can't a bicycle go uphill easily? The gradient is steep.\",\n",
    "    \"What do you call a fresh noodle? Al dente pasta.\",\n",
    "    \"Why did the math book look worn? It had been used for years.\",\n",
    "    \"What do you call a prehistoric reptile? A dinosaur.\",\n",
    "    \"Why did the golfer check the weather? To plan his game.\",\n",
    "    \"What did the grape taste like? Sweet and refreshing.\",\n",
    "]\n",
    "\n",
    "# Test data (held out)\n",
    "test_puns = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs.\",\n",
    "    \"What do you call a lazy kangaroo? A pouch potato.\",\n",
    "    \"Why did the stadium get hot? All the fans left.\",\n",
    "    \"What do you call a pig that does karate? A pork chop.\",\n",
    "    \"Why did the coffee file a police report? It got mugged.\",\n",
    "]\n",
    "\n",
    "test_nonpuns = [\n",
    "    \"Why do programmers use version control? To track code changes.\",\n",
    "    \"What do you call a baby kangaroo? A joey.\",\n",
    "    \"Why did the stadium close early? For maintenance.\",\n",
    "    \"What do you call a pig on a farm? Livestock.\",\n",
    "    \"Why did the coffee taste bitter? It was over-extracted.\",\n",
    "]\n",
    "\n",
    "print(f\"Training: {len(train_puns)} puns, {len(train_nonpuns)} non-puns\")\n",
    "print(f\"Test: {len(test_puns)} puns, {len(test_nonpuns)} non-puns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Part 2: Remote Probe Training with Sessions\n\nUsing nnsight's `session` and `iter` APIs, we run the **entire training loop** on NDIF:\n\n1. `model.session()` creates a persistent remote context\n2. Probe weights and optimizer are created on NDIF\n3. `session.iter()` iterates over epochs and batches remotely\n4. Only the final trained weights are downloaded\n\n### Data Loading Options\n\n**Option 1: In-memory dataset** (used in this notebook)\n- Data is serialized and sent to NDIF as part of the computation graph\n- Good for small datasets (< 1000 examples)\n\n**Option 2: Load from GitHub raw URL**\n- Reference CSV/JSON files directly from a public GitHub repo\n- NDIF fetches the data server-side\n\n```python\nfrom datasets import load_dataset\n\n# Load CSV directly from GitHub raw URL\ngithub_url = \"https://raw.githubusercontent.com/your-org/your-repo/main/data/puns.csv\"\ndataset = load_dataset(\"csv\", data_files={\"train\": github_url})\n\n# Or JSON\ndataset = load_dataset(\"json\", data_files={\n    \"train\": \"https://raw.githubusercontent.com/your-org/repo/main/train.json\",\n    \"test\": \"https://raw.githubusercontent.com/your-org/repo/main/test.json\"\n})\n```\n\n**Option 3: HuggingFace Hub dataset**\n- Upload your dataset to HuggingFace, NDIF loads it server-side\n\n```python\n# Upload local dataset to HuggingFace Hub\nfrom datasets import Dataset\ndataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\ndataset.push_to_hub(\"your-username/pun-dataset\", private=True)\n\n# Then NDIF loads it directly:\ndataset = load_dataset(\"your-username/pun-dataset\")\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "class RemoteProbeTrainer:\n    \"\"\"\n    Train a linear probe entirely on NDIF using nnsight sessions.\n    \n    The entire training loop (all epochs) runs remotely.\n    Only the final trained weights are downloaded.\n    \"\"\"\n    \n    def __init__(self, model, layer_idx, hidden_size, lr=0.01):\n        self.model = model\n        self.layer_idx = layer_idx\n        self.hidden_size = hidden_size\n        self.lr = lr\n        \n        # Probe weights (will be created remotely during training)\n        self.probe_weight = None\n        self.probe_bias = None\n    \n    def train(self, texts, labels, n_epochs=10, batch_size=5, remote=True):\n        \"\"\"\n        Train probe with ENTIRE loop on NDIF using session + iter.\n        \n        Args:\n            texts: List of input texts\n            labels: List of labels (0 or 1)\n            n_epochs: Number of training epochs\n            batch_size: Examples per batch\n            remote: Whether to run on NDIF\n            \n        Returns:\n            losses: List of per-epoch losses\n        \"\"\"\n        # Prepare dataset as list of (text, label) tuples\n        dataset = list(zip(texts, labels))\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        epoch_indices = list(range(n_epochs))\n        \n        print(f\"Training {n_epochs} epochs on {'NDIF' if remote else 'local'}...\")\n        \n        # Run ENTIRE training loop on NDIF\n        with self.model.session(remote=remote) as session:\n            \n            # Initialize probe weights remotely\n            probe_w = torch.nn.Parameter(torch.randn(1, self.hidden_size) * 0.01)\n            probe_b = torch.nn.Parameter(torch.zeros(1))\n            \n            # Optimizer runs on remote device\n            optimizer = torch.optim.AdamW([probe_w, probe_b], lr=self.lr)\n            \n            # Storage for losses\n            all_losses = []\n            \n            # Iterate over epochs using session.iter\n            with session.iter(epoch_indices) as epoch:\n                epoch_loss = torch.tensor(0.0)\n                n_batches = 0\n                \n                # Iterate over batches\n                with session.iter(dataloader) as batch:\n                    batch_texts = [item[0] for item in batch]\n                    batch_labels = torch.tensor([item[1] for item in batch], dtype=torch.float32)\n                    \n                    # Forward pass through model\n                    with self.model.trace(batch_texts):\n                        # Get hidden states at target layer, last token\n                        hidden_states = self.model.model.layers[self.layer_idx].output[0]\n                        last_hidden = hidden_states[:, -1, :]  # (batch, hidden)\n                        \n                        # Move probe to correct device\n                        w = probe_w.to(last_hidden.device)\n                        b = probe_b.to(last_hidden.device)\n                        \n                        # Probe forward: logits = hidden @ W^T + b\n                        logits = (last_hidden @ w.T).squeeze(-1) + b\n                        \n                        # Binary cross-entropy loss\n                        labels_dev = batch_labels.to(logits.device)\n                        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n                            logits, labels_dev\n                        )\n                        loss.backward()\n                        \n                        epoch_loss = epoch_loss.to(loss.device) + loss\n                        n_batches += 1\n                    \n                    # Update parameters\n                    optimizer.step()\n                    optimizer.zero_grad()\n                \n                # Log epoch loss\n                avg_loss = epoch_loss / max(n_batches, 1)\n                nnsight.log(\"Epoch loss:\", avg_loss)\n                all_losses.append(avg_loss)\n            \n            # Save final weights - THIS is what we download\n            final_w = probe_w.detach().clone().save()\n            final_b = probe_b.detach().clone().save()\n            final_losses = [l.save() for l in all_losses]\n        \n        # Store trained weights locally\n        self.probe_weight = final_w.value.cpu()\n        self.probe_bias = final_b.value.cpu()\n        losses = [l.value.item() for l in final_losses]\n        \n        print(f\"Training complete!\")\n        return losses\n    \n    def train_step(self, texts, labels, remote=True):\n        \"\"\"\n        Single training step (for compatibility).\n        Downloads gradients and updates locally.\n        \"\"\"\n        if self.probe_weight is None:\n            self.probe_weight = torch.randn(1, self.hidden_size) * 0.01\n            self.probe_bias = torch.zeros(1)\n        \n        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n        \n        with self.model.trace(texts, remote=remote) as tracer:\n            hidden_states = self.model.model.layers[self.layer_idx].output[0]\n            last_hidden = hidden_states[:, -1, :]\n            \n            w = self.probe_weight.to(last_hidden.device)\n            b = self.probe_bias.to(last_hidden.device)\n            w.requires_grad_(True)\n            b.requires_grad_(True)\n            \n            logits = (last_hidden @ w.T).squeeze(-1) + b\n            labels_dev = labels_tensor.to(logits.device)\n            loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, labels_dev)\n            loss.backward()\n            \n            weight_grad = w.grad.save()\n            bias_grad = b.grad.save()\n            loss_value = loss.save()\n        \n        self.probe_weight -= self.lr * weight_grad.value.cpu()\n        self.probe_bias -= self.lr * bias_grad.value.cpu()\n        \n        return loss_value.value.item()\n    \n    def evaluate(self, texts, labels, remote=True):\n        \"\"\"\n        Evaluate probe accuracy on a batch.\n        \"\"\"\n        if self.probe_weight is None:\n            raise ValueError(\"No trained weights. Call train() first.\")\n        \n        with self.model.trace(texts, remote=remote) as tracer:\n            hidden_states = self.model.model.layers[self.layer_idx].output[0]\n            last_hidden = hidden_states[:, -1, :]\n            \n            w = self.probe_weight.to(last_hidden.device)\n            b = self.probe_bias.to(last_hidden.device)\n            \n            logits = (last_hidden @ w.T).squeeze(-1) + b\n            probs = torch.sigmoid(logits)\n            preds = (probs > 0.5).float().save()\n        \n        preds_np = preds.value.cpu().numpy()\n        labels_np = np.array(labels)\n        accuracy = np.mean(preds_np == labels_np)\n        \n        return accuracy"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Train a Probe at One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a layer to probe (middle layer often works well)\n",
    "target_layer = model.config.num_hidden_layers // 2\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "print(f\"Training probe at layer {target_layer}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = RemoteProbeTrainer(\n",
    "    model=model,\n",
    "    layer_idx=target_layer,\n",
    "    hidden_size=hidden_size,\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "train_texts = train_puns + train_nonpuns\n",
    "train_labels = [1.0] * len(train_puns) + [0.0] * len(train_nonpuns)\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(train_texts))\n",
    "train_texts = [train_texts[i] for i in indices]\n",
    "train_labels = [train_labels[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Train probe - ENTIRE loop runs on NDIF!\nlosses = trainer.train(\n    train_texts, \n    train_labels, \n    n_epochs=10, \n    batch_size=5, \n    remote=REMOTE\n)\n\n# Evaluate\ntest_texts = test_puns + test_nonpuns\ntest_labels = [1.0] * len(test_puns) + [0.0] * len(test_nonpuns)\n\ntrain_acc = trainer.evaluate(train_texts, train_labels, remote=REMOTE)\ntest_acc = trainer.evaluate(test_texts, test_labels, remote=REMOTE)\n\nprint(f\"\\nFinal Train Accuracy: {train_acc:.1%}\")\nprint(f\"Final Test Accuracy: {test_acc:.1%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses, 'b-o')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, 'b-o', label='Train')\n",
    "ax2.plot(test_accs, 'r-o', label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 3: Layer-Wise Probe Analysis\n",
    "\n",
    "Train probes at multiple layers to see where pun information is most accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "def train_probe_at_layer(model, layer_idx, train_texts, train_labels,\n                         test_texts, test_labels, n_epochs=10, lr=0.1,\n                         batch_size=5, remote=True, verbose=False):\n    \"\"\"\n    Train a probe at a specific layer and return final accuracies.\n    Uses session-based training (entire loop on NDIF).\n    \"\"\"\n    hidden_size = model.config.hidden_size\n    trainer = RemoteProbeTrainer(model, layer_idx, hidden_size, lr=lr)\n    \n    # Train with session-based approach\n    losses = trainer.train(\n        train_texts, train_labels,\n        n_epochs=n_epochs,\n        batch_size=batch_size,\n        remote=remote\n    )\n    \n    if verbose:\n        for i, loss in enumerate(losses):\n            print(f\"  Epoch {i+1}: loss={loss:.4f}\")\n    \n    train_acc = trainer.evaluate(train_texts, train_labels, remote=remote)\n    test_acc = trainer.evaluate(test_texts, test_labels, remote=remote)\n    \n    return train_acc, test_acc, trainer.probe_weight.clone()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probes at multiple layers\n",
    "n_layers = model.config.num_hidden_layers\n",
    "layers_to_probe = list(range(0, n_layers, n_layers // 8))  # Every 1/8th layer\n",
    "if n_layers - 1 not in layers_to_probe:\n",
    "    layers_to_probe.append(n_layers - 1)\n",
    "\n",
    "print(f\"Probing layers: {layers_to_probe}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in layers_to_probe:\n",
    "    print(f\"Training probe at layer {layer_idx}...\")\n",
    "    \n",
    "    train_acc, test_acc, weights = train_probe_at_layer(\n",
    "        model, layer_idx,\n",
    "        train_texts, train_labels,\n",
    "        test_texts, test_labels,\n",
    "        n_epochs=10,\n",
    "        remote=REMOTE\n",
    "    )\n",
    "    \n",
    "    layer_results[layer_idx] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'weights': weights\n",
    "    }\n",
    "    \n",
    "    print(f\"  Layer {layer_idx}: Train={train_acc:.1%}, Test={test_acc:.1%}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer-wise probe performance\n",
    "layers = sorted(layer_results.keys())\n",
    "train_accs = [layer_results[l]['train_acc'] for l in layers]\n",
    "test_accs = [layer_results[l]['test_acc'] for l in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(layers, train_accs, 'b-o', label='Train', markersize=8)\n",
    "plt.plot(layers, test_accs, 'r-o', label='Test', markersize=8)\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Probe Accuracy Across Layers: Where is Pun Information?')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.4, 1.05)\n",
    "\n",
    "# Mark the best layer\n",
    "best_layer = max(layer_results.keys(), key=lambda l: layer_results[l]['test_acc'])\n",
    "best_acc = layer_results[best_layer]['test_acc']\n",
    "plt.scatter([best_layer], [best_acc], s=200, c='green', marker='*', \n",
    "            zorder=5, label=f'Best: L{best_layer}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best layer: {best_layer} with test accuracy {best_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 4: Control Tasks\n",
    "\n",
    "Validate that the probe detects puns, not spurious features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control 1: Random labels\n",
    "# If probe achieves high accuracy with random labels, it's overfitting\n",
    "\n",
    "print(\"Control Task 1: Random Labels\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "random_labels = np.random.randint(0, 2, len(train_labels)).astype(float).tolist()\n",
    "\n",
    "random_train_acc, random_test_acc, _ = train_probe_at_layer(\n",
    "    model, best_layer,\n",
    "    train_texts, random_labels,\n",
    "    test_texts, test_labels,  # Use real test labels for evaluation\n",
    "    n_epochs=10,\n",
    "    remote=REMOTE\n",
    ")\n",
    "\n",
    "print(f\"Random label probe - Train: {random_train_acc:.1%}, Test: {random_test_acc:.1%}\")\n",
    "print(f\"Real label probe - Test: {layer_results[best_layer]['test_acc']:.1%}\")\n",
    "\n",
    "if random_test_acc < 0.6:\n",
    "    print(\"PASS: Random labels give random performance (probe is not overfitting)\")\n",
    "else:\n",
    "    print(\"WARNING: Random labels give high accuracy (possible overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control 2: Edge cases\n",
    "# Does the probe respond to puns it hasn't seen?\n",
    "\n",
    "edge_cases = [\n",
    "    # Clear puns (should classify as pun)\n",
    "    (\"Why do bees have sticky hair? They use honeycombs.\", 1),\n",
    "    (\"What do you call a dinosaur that crashes cars? Tyrannosaurus wrecks.\", 1),\n",
    "    \n",
    "    # Questions that aren't puns (should NOT classify as pun)\n",
    "    (\"Why is the sky blue? Due to Rayleigh scattering.\", 0),\n",
    "    (\"What is the capital of France? Paris.\", 0),\n",
    "    \n",
    "    # Non-question puns (harder case)\n",
    "    (\"I'm reading a book about anti-gravity. It's impossible to put down.\", 1),\n",
    "    (\"I told my wife she was drawing her eyebrows too high. She looked surprised.\", 1),\n",
    "]\n",
    "\n",
    "print(\"\\nControl Task 2: Edge Cases\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the best layer's trainer\n",
    "hidden_size = model.config.hidden_size\n",
    "edge_trainer = RemoteProbeTrainer(model, best_layer, hidden_size)\n",
    "edge_trainer.probe_weight = layer_results[best_layer]['weights'].clone()\n",
    "\n",
    "edge_texts = [e[0] for e in edge_cases]\n",
    "edge_labels = [float(e[1]) for e in edge_cases]\n",
    "\n",
    "# Get predictions\n",
    "with model.trace(edge_texts, remote=REMOTE) as tracer:\n",
    "    hidden = model.model.layers[best_layer].output[0]\n",
    "    last_hidden = hidden[:, -1, :]\n",
    "    w = edge_trainer.probe_weight.to(last_hidden.device)\n",
    "    b = edge_trainer.probe_bias.to(last_hidden.device)\n",
    "    logits = (last_hidden @ w.T).squeeze(-1) + b\n",
    "    probs = torch.sigmoid(logits).save()\n",
    "\n",
    "probs_np = probs.value.cpu().numpy()\n",
    "\n",
    "for (text, expected), prob in zip(edge_cases, probs_np):\n",
    "    pred = \"PUN\" if prob > 0.5 else \"NOT PUN\"\n",
    "    correct = (prob > 0.5) == expected\n",
    "    status = \"correct\" if correct else \"WRONG\"\n",
    "    print(f\"P(pun)={prob:.2f} [{pred}] {status}\")\n",
    "    print(f\"  {text[:60]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Part 5: Compare with Causal Direction\n",
    "\n",
    "Does the probe's learned direction match the mass mean-difference direction from Week 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_difference_direction(model, layer_idx, pun_texts, nonpun_texts, remote=True):\n",
    "    \"\"\"\n",
    "    Compute the mass mean-difference direction (Week 4 style).\n",
    "    \"\"\"\n",
    "    # Get pun activations\n",
    "    with model.trace(pun_texts, remote=remote) as tracer:\n",
    "        pun_hidden = model.model.layers[layer_idx].output[0][:, -1, :].save()\n",
    "    pun_mean = pun_hidden.value.mean(dim=0).cpu()\n",
    "    \n",
    "    # Get non-pun activations\n",
    "    with model.trace(nonpun_texts, remote=remote) as tracer:\n",
    "        nonpun_hidden = model.model.layers[layer_idx].output[0][:, -1, :].save()\n",
    "    nonpun_mean = nonpun_hidden.value.mean(dim=0).cpu()\n",
    "    \n",
    "    # Mean difference direction\n",
    "    direction = pun_mean - nonpun_mean\n",
    "    direction_normalized = direction / direction.norm()\n",
    "    \n",
    "    return direction_normalized\n",
    "\n",
    "# Compute mean-difference direction\n",
    "mean_diff_direction = compute_mean_difference_direction(\n",
    "    model, best_layer, train_puns, train_nonpuns, remote=REMOTE\n",
    ")\n",
    "\n",
    "# Get probe direction (the weight vector)\n",
    "probe_direction = layer_results[best_layer]['weights'].squeeze()\n",
    "probe_direction = probe_direction / probe_direction.norm()\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = torch.dot(mean_diff_direction, probe_direction).item()\n",
    "\n",
    "print(f\"Cosine similarity between probe direction and mean-difference direction:\")\n",
    "print(f\"  {cosine_sim:.4f}\")\n",
    "print()\n",
    "\n",
    "if abs(cosine_sim) > 0.8:\n",
    "    print(\"HIGH similarity: Probe learned the same direction as mean-difference.\")\n",
    "elif abs(cosine_sim) > 0.5:\n",
    "    print(\"MODERATE similarity: Probe learned a related but distinct direction.\")\n",
    "else:\n",
    "    print(\"LOW similarity: Probe learned a different direction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Exercise 1: MLP Probe\n",
    "\n",
    "Implement a nonlinear (MLP) probe and compare to the linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify RemoteProbeTrainer to support MLP probes\n",
    "# Add a hidden layer with ReLU activation\n",
    "# Compare accuracy: does MLP do better than linear?\n",
    "# If yes, pun representation may be nonlinear\n",
    "\n",
    "class RemoteMLPProbeTrainer:\n",
    "    \"\"\"\n",
    "    Train an MLP probe entirely on NDIF.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_idx, hidden_size, mlp_hidden=64, lr=0.01):\n",
    "        self.model = model\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Two-layer MLP: input -> hidden -> output\n",
    "        self.w1 = torch.randn(mlp_hidden, hidden_size) * 0.01\n",
    "        self.b1 = torch.zeros(mlp_hidden)\n",
    "        self.w2 = torch.randn(1, mlp_hidden) * 0.01\n",
    "        self.b2 = torch.zeros(1)\n",
    "    \n",
    "    # TODO: Implement train_step and evaluate methods\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Exercise 2: Position Analysis\n",
    "\n",
    "We trained on the last token. How does probe accuracy vary with position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify training to extract from different positions\n",
    "# - Last token (current approach)\n",
    "# - Middle of sequence\n",
    "# - Average over all positions\n",
    "# \n",
    "# Question: At which position is pun information most accessible?\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Exercise 3: Causal Validation\n",
    "\n",
    "Use the probe direction for steering. If steering along the probe direction makes non-puns more \"pun-like,\" the direction is causally meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement steering with probe direction\n",
    "# 1. Take a non-pun sentence\n",
    "# 2. Add the probe direction to activations at the probed layer\n",
    "# 3. See if the model's output changes in pun-like ways\n",
    "#\n",
    "# This validates that the probe captures causally relevant information\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we demonstrated:\n\n1. **Session-based remote training** - the entire training loop runs on NDIF:\n   - `model.session()` creates a persistent remote context\n   - `session.iter()` iterates over epochs and batches remotely\n   - Only final trained weights are downloaded\n\n2. **Layer-wise analysis** - probes reveal which layers contain pun information\n\n3. **Control tasks** - random labels and edge cases validate probe quality\n\n4. **Direction comparison** - probe weights align (or don't) with mean-difference direction\n\n### Data Transfer Patterns\n\n| Pattern | Data Transfer | Best For |\n|---------|---------------|----------|\n| In-memory + session.iter() | Dataset sent once at session start | Small datasets (< 1000) |\n| HuggingFace Hub | NDIF loads server-side | Large datasets |\n\n### Key Insights\n\n- Probe accuracy measures **linear accessibility** of information\n- High accuracy does NOT prove the model uses this information\n- Control tasks are essential for validation\n- Compare probes with causal methods (Week 5) for complete picture\n\n### For Your Research\n\n1. Apply remote training to your concept\n2. Find which layers encode your concept (layer-wise probing)\n3. Run control tasks to validate\n4. Compare probe direction with your Week 4 concept direction\n5. Use Week 5 causal methods to verify the probe captures causally relevant information"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}