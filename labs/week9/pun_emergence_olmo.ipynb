{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Pun Emergence During OLMo Training\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week9/pun_emergence_olmo.ipynb)\n",
    "\n",
    "This notebook studies **when pun understanding emerges during training** by analyzing OLMo checkpoints at different training steps. We'll apply interpretability methods from previous weeks to track the formation of pun-related mechanisms.\n",
    "\n",
    "**Key Question:** Does pun understanding emerge gradually, or is there a phase transition where the model suddenly \"gets\" puns?\n",
    "\n",
    "## What We'll Track\n",
    "- Pun completion accuracy across training\n",
    "- Probe accuracy (linear separability of pun vs non-pun)\n",
    "- Logit lens evolution (when does the punchline emerge?)\n",
    "- Layer-wise representation quality\n",
    "\n",
    "## References\n",
    "- [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)\n",
    "- [Progress Measures for Grokking](https://arxiv.org/abs/2301.05217) (Nanda et al.)\n",
    "- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install nnsight and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pin nnsight version for NDIF compatibility
!pip install -q nnsight==0.5.11 transformers huggingface_hub scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom huggingface_hub import list_repo_refs\nfrom nnsight import LanguageModel, CONFIG\n\n# Configure NDIF API key from Colab secrets\ntry:\n    from google.colab import userdata\n    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\nexcept:\n    pass  # Not in Colab or secret not set\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Discover Available OLMo Checkpoints\n",
    "\n",
    "OLMo releases checkpoints every 1000 training steps. Let's see what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available checkpoints for OLMo-1B\n",
    "MODEL_ID = \"allenai/OLMo-1B-hf\"\n",
    "\n",
    "out = list_repo_refs(MODEL_ID)\n",
    "branches = [b.name for b in out.branches]\n",
    "\n",
    "# Filter to get training checkpoints (format: stepXXX-tokensYYYB)\n",
    "checkpoints = [b for b in branches if b.startswith('step') and 'tokens' in b]\n",
    "checkpoints.sort(key=lambda x: int(x.split('-')[0].replace('step', '')))\n",
    "\n",
    "print(f\"Found {len(checkpoints)} training checkpoints\")\n",
    "print(f\"\\nFirst 10: {checkpoints[:10]}\")\n",
    "print(f\"Last 10: {checkpoints[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_checkpoint(name):\n",
    "    \"\"\"Parse checkpoint name to get step and token count.\"\"\"\n",
    "    parts = name.split('-')\n",
    "    step = int(parts[0].replace('step', ''))\n",
    "    tokens = parts[1].replace('tokens', '').replace('B', '')\n",
    "    tokens_b = float(tokens)\n",
    "    return step, tokens_b\n",
    "\n",
    "# Select checkpoints spanning training (log-spaced)\n",
    "# Early, middle, and late training\n",
    "selected_checkpoints = [\n",
    "    checkpoints[0],   # Very early\n",
    "    checkpoints[4],   # Early\n",
    "    checkpoints[9],   # ~10k steps\n",
    "    checkpoints[24],  # ~25k steps\n",
    "    checkpoints[49],  # ~50k steps\n",
    "    checkpoints[99] if len(checkpoints) > 99 else checkpoints[-3],  # ~100k steps\n",
    "    checkpoints[199] if len(checkpoints) > 199 else checkpoints[-2],  # ~200k steps\n",
    "    checkpoints[-1],  # Final\n",
    "]\n",
    "\n",
    "# Remove duplicates and sort\n",
    "selected_checkpoints = sorted(set(selected_checkpoints), \n",
    "                               key=lambda x: parse_checkpoint(x)[0])\n",
    "\n",
    "print(\"Selected checkpoints for analysis:\")\n",
    "for cp in selected_checkpoints:\n",
    "    step, tokens = parse_checkpoint(cp)\n",
    "    print(f\"  {cp}: step {step:,}, {tokens}B tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prepare Pun Test Data\n",
    "\n",
    "We'll use puns with clear punchlines to test completion accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pun test set with expected completions\n",
    "pun_tests = [\n",
    "    {\n",
    "        \"prompt\": \"Why do electricians make good swimmers? Because they know the\",\n",
    "        \"target\": \" current\",\n",
    "        \"category\": \"homograph\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Why did the banker break up with his girlfriend? He lost\",\n",
    "        \"target\": \" interest\",\n",
    "        \"category\": \"homograph\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"I used to be a banker, but I lost\",\n",
    "        \"target\": \" interest\",\n",
    "        \"category\": \"homograph\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Why can't a bicycle stand on its own? Because it's two\",\n",
    "        \"target\": \" tired\",\n",
    "        \"category\": \"homophone\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Time flies like an arrow; fruit flies like a\",\n",
    "        \"target\": \" banana\",\n",
    "        \"category\": \"structural\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"I used to work at a calendar factory but got fired for taking a\",\n",
    "        \"target\": \" day\",\n",
    "        \"category\": \"homograph\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"The math teacher called in sick because she had\",\n",
    "        \"target\": \" problems\",\n",
    "        \"category\": \"homograph\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"I'm reading a book about anti-gravity. It's impossible to put\",\n",
    "        \"target\": \" down\",\n",
    "        \"category\": \"idiom\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Non-pun control sentences for probe training\n",
    "non_pun_tests = [\n",
    "    {\"prompt\": \"The electrician fixed the wiring in the\", \"target\": \" house\"},\n",
    "    {\"prompt\": \"The banker reviewed the financial\", \"target\": \" report\"},\n",
    "    {\"prompt\": \"She rode her bicycle to the\", \"target\": \" store\"},\n",
    "    {\"prompt\": \"Time passes quickly when you're having\", \"target\": \" fun\"},\n",
    "    {\"prompt\": \"The calendar showed the date of the\", \"target\": \" meeting\"},\n",
    "    {\"prompt\": \"The math teacher explained the\", \"target\": \" concept\"},\n",
    "    {\"prompt\": \"I'm reading a book about\", \"target\": \" history\"},\n",
    "    {\"prompt\": \"The swimmer dove into the\", \"target\": \" pool\"},\n",
    "]\n",
    "\n",
    "print(f\"Pun tests: {len(pun_tests)}\")\n",
    "print(f\"Non-pun controls: {len(non_pun_tests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define Evaluation Metrics\n",
    "\n",
    "We'll track multiple metrics across checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model_id, revision, device=\"cuda\"):\n",
    "    \"\"\"Load a specific OLMo checkpoint with nnsight.\"\"\"\n",
    "    model = LanguageModel(\n",
    "        model_id,\n",
    "        revision=revision,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_target_probability(model, prompt, target):\n",
    "    \"\"\"Get probability of target token given prompt.\"\"\"\n",
    "    # Tokenize\n",
    "    target_ids = model.tokenizer.encode(target, add_special_tokens=False)\n",
    "    if len(target_ids) == 0:\n",
    "        return 0.0\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    with model.trace(prompt) as tracer:\n",
    "        logits = model.output.logits.save()\n",
    "    \n",
    "    # Get probability at last position\n",
    "    last_logits = logits.value[0, -1, :]\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    return probs[target_id].item()\n",
    "\n",
    "def get_target_rank(model, prompt, target):\n",
    "    \"\"\"Get rank of target token in model's predictions.\"\"\"\n",
    "    target_ids = model.tokenizer.encode(target, add_special_tokens=False)\n",
    "    if len(target_ids) == 0:\n",
    "        return float('inf')\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    with model.trace(prompt) as tracer:\n",
    "        logits = model.output.logits.save()\n",
    "    \n",
    "    last_logits = logits.value[0, -1, :]\n",
    "    sorted_indices = torch.argsort(last_logits, descending=True)\n",
    "    rank = (sorted_indices == target_id).nonzero(as_tuple=True)[0].item()\n",
    "    \n",
    "    return rank + 1  # 1-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, prompt, layer_idx):\n",
    "    \"\"\"Extract hidden states from a specific layer.\"\"\"\n",
    "    with model.trace(prompt) as tracer:\n",
    "        hidden = model.model.layers[layer_idx].output[0].save()\n",
    "    \n",
    "    # Return last token representation\n",
    "    return hidden.value[0, -1, :].cpu().numpy()\n",
    "\n",
    "def train_pun_probe(model, pun_data, non_pun_data, layer_idx):\n",
    "    \"\"\"Train a linear probe to classify pun vs non-pun at a given layer.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Get pun representations\n",
    "    for item in pun_data:\n",
    "        h = get_hidden_states(model, item[\"prompt\"], layer_idx)\n",
    "        X.append(h)\n",
    "        y.append(1)  # Pun\n",
    "    \n",
    "    # Get non-pun representations\n",
    "    for item in non_pun_data:\n",
    "        h = get_hidden_states(model, item[\"prompt\"], layer_idx)\n",
    "        X.append(h)\n",
    "        y.append(0)  # Non-pun\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Train logistic regression with cross-validation\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=min(3, len(y)//2))\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate Across Checkpoints\n",
    "\n",
    "Now we'll load each checkpoint and measure pun understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoint(model_id, revision, pun_tests, non_pun_tests, probe_layer=None):\n",
    "    \"\"\"\n",
    "    Evaluate a single checkpoint on pun understanding metrics.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - mean_pun_prob: Average probability of pun target\n",
    "    - mean_pun_rank: Average rank of pun target\n",
    "    - top1_accuracy: % of puns where target is top prediction\n",
    "    - top5_accuracy: % of puns where target is in top 5\n",
    "    - probe_accuracy: Linear probe accuracy (if probe_layer specified)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {revision}...\")\n",
    "    model = load_checkpoint(model_id, revision, device=device)\n",
    "    \n",
    "    # Determine probe layer (middle layer if not specified)\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    if probe_layer is None:\n",
    "        probe_layer = n_layers // 2\n",
    "    \n",
    "    results = {\n",
    "        \"revision\": revision,\n",
    "        \"pun_probs\": [],\n",
    "        \"pun_ranks\": [],\n",
    "    }\n",
    "    \n",
    "    # Evaluate pun completions\n",
    "    for item in pun_tests:\n",
    "        prob = get_target_probability(model, item[\"prompt\"], item[\"target\"])\n",
    "        rank = get_target_rank(model, item[\"prompt\"], item[\"target\"])\n",
    "        results[\"pun_probs\"].append(prob)\n",
    "        results[\"pun_ranks\"].append(rank)\n",
    "    \n",
    "    # Compute summary stats\n",
    "    results[\"mean_pun_prob\"] = np.mean(results[\"pun_probs\"])\n",
    "    results[\"mean_pun_rank\"] = np.mean(results[\"pun_ranks\"])\n",
    "    results[\"top1_accuracy\"] = np.mean([r == 1 for r in results[\"pun_ranks\"]])\n",
    "    results[\"top5_accuracy\"] = np.mean([r <= 5 for r in results[\"pun_ranks\"]])\n",
    "    \n",
    "    # Train probe\n",
    "    try:\n",
    "        results[\"probe_accuracy\"] = train_pun_probe(\n",
    "            model, pun_tests, non_pun_tests, probe_layer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  Probe training failed: {e}\")\n",
    "        results[\"probe_accuracy\"] = 0.5\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"  Prob: {results['mean_pun_prob']:.4f}, Rank: {results['mean_pun_rank']:.1f}, \"\n",
    "          f\"Top1: {results['top1_accuracy']:.2%}, Probe: {results['probe_accuracy']:.2%}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all selected checkpoints\n",
    "all_results = []\n",
    "\n",
    "print(f\"Evaluating {len(selected_checkpoints)} checkpoints...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for checkpoint in tqdm(selected_checkpoints):\n",
    "    results = evaluate_checkpoint(\n",
    "        MODEL_ID, \n",
    "        checkpoint, \n",
    "        pun_tests, \n",
    "        non_pun_tests\n",
    "    )\n",
    "    all_results.append(results)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Emergence Patterns\n",
    "\n",
    "Let's plot how pun understanding evolves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "steps = [parse_checkpoint(r[\"revision\"])[0] for r in all_results]\n",
    "tokens = [parse_checkpoint(r[\"revision\"])[1] for r in all_results]\n",
    "\n",
    "mean_probs = [r[\"mean_pun_prob\"] for r in all_results]\n",
    "mean_ranks = [r[\"mean_pun_rank\"] for r in all_results]\n",
    "top1_accs = [r[\"top1_accuracy\"] for r in all_results]\n",
    "top5_accs = [r[\"top5_accuracy\"] for r in all_results]\n",
    "probe_accs = [r[\"probe_accuracy\"] for r in all_results]\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Target probability\n",
    "axes[0, 0].plot(tokens, mean_probs, 'b-o', markersize=8)\n",
    "axes[0, 0].set_xlabel('Training Tokens (B)')\n",
    "axes[0, 0].set_ylabel('Mean P(pun target)')\n",
    "axes[0, 0].set_title('Pun Completion Probability')\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Target rank\n",
    "axes[0, 1].plot(tokens, mean_ranks, 'r-o', markersize=8)\n",
    "axes[0, 1].set_xlabel('Training Tokens (B)')\n",
    "axes[0, 1].set_ylabel('Mean Rank of Pun Target')\n",
    "axes[0, 1].set_title('Pun Target Rank (lower = better)')\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Top-k accuracy\n",
    "axes[1, 0].plot(tokens, top1_accs, 'g-o', markersize=8, label='Top-1')\n",
    "axes[1, 0].plot(tokens, top5_accs, 'g--s', markersize=8, label='Top-5')\n",
    "axes[1, 0].set_xlabel('Training Tokens (B)')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Pun Completion Accuracy')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Plot 4: Probe accuracy\n",
    "axes[1, 1].plot(tokens, probe_accs, 'm-o', markersize=8)\n",
    "axes[1, 1].axhline(y=0.5, color='gray', linestyle='--', label='Chance')\n",
    "axes[1, 1].set_xlabel('Training Tokens (B)')\n",
    "axes[1, 1].set_ylabel('Probe Accuracy')\n",
    "axes[1, 1].set_title('Linear Probe: Pun vs Non-Pun')\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.suptitle('Pun Understanding Emergence During OLMo Training', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Detect Phase Transitions\n",
    "\n",
    "Look for sudden changes in pun capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_transitions(values, tokens, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect sharp transitions in a metric.\n",
    "    Returns list of (token_count, delta) for significant changes.\n",
    "    \"\"\"\n",
    "    transitions = []\n",
    "    \n",
    "    for i in range(1, len(values)):\n",
    "        delta = values[i] - values[i-1]\n",
    "        if abs(delta) > threshold:\n",
    "            transitions.append({\n",
    "                \"from_tokens\": tokens[i-1],\n",
    "                \"to_tokens\": tokens[i],\n",
    "                \"delta\": delta,\n",
    "                \"from_value\": values[i-1],\n",
    "                \"to_value\": values[i]\n",
    "            })\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "# Detect transitions in each metric\n",
    "print(\"Phase transitions detected:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prob_transitions = detect_transitions(mean_probs, tokens, threshold=0.05)\n",
    "print(f\"\\nPun probability transitions:\")\n",
    "for t in prob_transitions:\n",
    "    print(f\"  {t['from_tokens']}B -> {t['to_tokens']}B: \"\n",
    "          f\"{t['from_value']:.3f} -> {t['to_value']:.3f} (delta: {t['delta']:+.3f})\")\n",
    "\n",
    "probe_transitions = detect_transitions(probe_accs, tokens, threshold=0.1)\n",
    "print(f\"\\nProbe accuracy transitions:\")\n",
    "for t in probe_transitions:\n",
    "    print(f\"  {t['from_tokens']}B -> {t['to_tokens']}B: \"\n",
    "          f\"{t['from_value']:.2%} -> {t['to_value']:.2%} (delta: {t['delta']:+.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Layer-wise Analysis Across Training\n",
    "\n",
    "How does pun representation quality change at different layers during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_layers(model_id, revision, pun_tests, non_pun_tests):\n",
    "    \"\"\"Evaluate probe accuracy at each layer.\"\"\"\n",
    "    model = load_checkpoint(model_id, revision, device=device)\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    \n",
    "    layer_accs = []\n",
    "    for layer in range(n_layers):\n",
    "        try:\n",
    "            acc = train_pun_probe(model, pun_tests, non_pun_tests, layer)\n",
    "        except:\n",
    "            acc = 0.5\n",
    "        layer_accs.append(acc)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return layer_accs\n",
    "\n",
    "# Evaluate layers at early, middle, and late checkpoints\n",
    "checkpoints_for_layers = [\n",
    "    selected_checkpoints[0],   # Early\n",
    "    selected_checkpoints[len(selected_checkpoints)//2],  # Middle\n",
    "    selected_checkpoints[-1],  # Late\n",
    "]\n",
    "\n",
    "layer_results = {}\n",
    "for cp in checkpoints_for_layers:\n",
    "    print(f\"Evaluating layers for {cp}...\")\n",
    "    layer_results[cp] = evaluate_all_layers(MODEL_ID, cp, pun_tests, non_pun_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layer-wise accuracy at different training stages\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, (cp, accs) in enumerate(layer_results.items()):\n",
    "    step, tok = parse_checkpoint(cp)\n",
    "    plt.plot(range(len(accs)), accs, f'{colors[i]}-o', \n",
    "             label=f'{tok}B tokens', markersize=4)\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Probe Accuracy')\n",
    "plt.title('Layer-wise Pun Probe Accuracy at Different Training Stages')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Logit Lens Across Training\n",
    "\n",
    "How does the model's internal prediction of the punchline evolve during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens_target_prob(model, prompt, target, n_layers=None):\n",
    "    \"\"\"Get probability of target at each layer using logit lens.\"\"\"\n",
    "    if n_layers is None:\n",
    "        n_layers = model.config.num_hidden_layers\n",
    "    \n",
    "    target_ids = model.tokenizer.encode(target, add_special_tokens=False)\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    layer_probs = []\n",
    "    \n",
    "    with model.trace(prompt) as tracer:\n",
    "        for layer_idx in range(n_layers):\n",
    "            hidden = model.model.layers[layer_idx].output[0]\n",
    "            # Project to vocabulary\n",
    "            logits = model.lm_head(model.model.norm(hidden)).save()\n",
    "            layer_probs.append(logits)\n",
    "    \n",
    "    probs = []\n",
    "    for logits in layer_probs:\n",
    "        p = torch.softmax(logits.value[0, -1], dim=-1)[target_id].item()\n",
    "        probs.append(p)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "# Compare logit lens at different training stages\n",
    "test_pun = pun_tests[0]  # Electrician pun\n",
    "\n",
    "logit_lens_results = {}\n",
    "for cp in checkpoints_for_layers:\n",
    "    print(f\"Running logit lens for {cp}...\")\n",
    "    model = load_checkpoint(MODEL_ID, cp, device=device)\n",
    "    probs = logit_lens_target_prob(model, test_pun[\"prompt\"], test_pun[\"target\"])\n",
    "    logit_lens_results[cp] = probs\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logit lens evolution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, (cp, probs) in enumerate(logit_lens_results.items()):\n",
    "    step, tok = parse_checkpoint(cp)\n",
    "    plt.plot(range(len(probs)), probs, f'{colors[i]}-o',\n",
    "             label=f'{tok}B tokens', markersize=4)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f'P(\"{test_pun[\"target\"]}\")')\n",
    "plt.title(f'Logit Lens: When Does \"{test_pun[\"target\"].strip()}\" Emerge?\\n'\n",
    "          f'Prompt: \"{test_pun[\"prompt\"]}\"')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compare Pun Types\n",
    "\n",
    "Do different types of puns (homograph, homophone, structural) emerge at different times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Group puns by category and track emergence separately\n",
    "# Do homograph puns emerge before or after structural puns?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Correlation with General Capabilities\n",
    "\n",
    "Does pun understanding correlate with general language ability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Track perplexity on general text alongside pun metrics\n",
    "# Does pun capability track general capability, or emerge independently?\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: In-Context Learning for Puns\n",
    "\n",
    "Does providing pun examples in context help at early checkpoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare pun completion with and without examples\n",
    "# At early checkpoints: does ICL help?\n",
    "# At late checkpoints: is ICL still beneficial?\n",
    "\n",
    "icl_prompt = \"\"\"\n",
    "Q: Why did the scarecrow win an award? A: Because he was outstanding in his field.\n",
    "Q: Why do electricians make good swimmers? A: Because they know the\n",
    "\"\"\".strip()\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **OLMo provides training checkpoints** every 1000 steps, enabling training dynamics studies\n",
    "\n",
    "2. **nnsight can load specific revisions** using the `revision` parameter\n",
    "\n",
    "3. **Pun understanding emergence** can be tracked via completion probability, rank, and probe accuracy\n",
    "\n",
    "4. **Phase transitions** may indicate when specific mechanisms form\n",
    "\n",
    "5. **Layer-wise analysis** shows how representations develop at different depths\n",
    "\n",
    "### Key Questions Answered\n",
    "\n",
    "- When does pun understanding emerge during training?\n",
    "- Is emergence gradual or sudden (phase transition)?\n",
    "- Do different layers develop pun representations at different rates?\n",
    "- How does logit lens prediction evolve during training?\n",
    "\n",
    "### Connections to Previous Weeks\n",
    "\n",
    "| Week | Method | Training Dynamics Application |\n",
    "|------|--------|------------------------------|\n",
    "| 4 | Geometry/PCA | Track separation in representation space |\n",
    "| 5 | CMA/Patching | When do causal components become important? |\n",
    "| 6 | Probes | Probe accuracy trajectory |\n",
    "| 7 | Attribution | How do attribution patterns change? |\n",
    "| 8 | Circuits | When does the circuit form? |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}