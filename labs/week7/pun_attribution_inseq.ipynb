{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pun Attribution with Inseq\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week7/pun_attribution_inseq.ipynb)\n",
    "\n",
    "This notebook demonstrates **attribution methods** for understanding which input tokens contribute to a model's predictions, using the [inseq](https://inseq.org/) library. We'll apply these techniques to **puns** to discover which words make the model recognize (or generate) the punchline.\n",
    "\n",
    "**Key Idea:** Attribution methods assign importance scores to input tokens, helping us understand *why* a model made a particular prediction. For puns, we want to know: which words in the setup make the model predict the pun word?\n",
    "\n",
    "## Methods Covered\n",
    "- **Integrated Gradients**: Path-based attribution that satisfies theoretical axioms\n",
    "- **Input x Gradient**: Simple gradient-based attribution\n",
    "- **Attention-based attribution**: Using attention weights as proxies for importance\n",
    "\n",
    "## References\n",
    "- [Inseq Documentation](https://inseq.org/)\n",
    "- [Integrated Gradients paper](https://arxiv.org/abs/1703.01365) (Sundararajan et al., 2017)\n",
    "- [Attention is not Explanation](https://arxiv.org/abs/1902.10186) (Jain & Wallace, 2019)\n",
    "- [Attention is not not Explanation](https://arxiv.org/abs/1908.04626) (Wiegreffe & Pinter, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install inseq and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q inseq transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inseq\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Check available attribution methods\n",
    "print(\"Available attribution methods:\")\n",
    "print(inseq.list_feature_attribution_methods())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading a Model with Inseq\n",
    "\n",
    "Inseq wraps HuggingFace models to enable attribution. We'll use GPT-2 for this tutorial since it runs efficiently on CPU/Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 with Integrated Gradients attribution\n",
    "model = inseq.load_model(\"gpt2\", \"integrated_gradients\")\n",
    "\n",
    "print(f\"Model: {model.model_name}\")\n",
    "print(f\"Attribution method: {model.attribution_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Attribution on a Pun\n",
    "\n",
    "Let's see which words in a pun setup contribute to predicting the punchline word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classic electrician pun\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "pun_completion = \" current\"\n",
    "\n",
    "# Run attribution\n",
    "# We attribute the generation of the completion to the input tokens\n",
    "out = model.attribute(\n",
    "    input_texts=pun_prompt,\n",
    "    generated_texts=pun_prompt + pun_completion,\n",
    "    n_steps=50,  # Number of integration steps\n",
    "    internal_batch_size=10\n",
    ")\n",
    "\n",
    "# Visualize the attribution\n",
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attribution scores as a numpy array\n",
    "attr_scores = out.sequence_attributions[0].source_attributions\n",
    "tokens = out.sequence_attributions[0].source\n",
    "\n",
    "print(\"Attribution scores for predicting 'current':\")\n",
    "print(\"=\" * 50)\n",
    "for token, score in zip(tokens, attr_scores[-1]):  # -1 for last generated token\n",
    "    print(f\"{repr(token):15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Attribution Methods\n",
    "\n",
    "Different attribution methods can give different results. Let's compare Integrated Gradients, Input x Gradient, and Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributions(prompt, completion, method=\"integrated_gradients\"):\n",
    "    \"\"\"Get attribution scores for a prompt-completion pair.\"\"\"\n",
    "    model = inseq.load_model(\"gpt2\", method)\n",
    "    \n",
    "    kwargs = {}\n",
    "    if method == \"integrated_gradients\":\n",
    "        kwargs = {\"n_steps\": 50, \"internal_batch_size\": 10}\n",
    "    \n",
    "    out = model.attribute(\n",
    "        input_texts=prompt,\n",
    "        generated_texts=prompt + completion,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    seq_attr = out.sequence_attributions[0]\n",
    "    tokens = seq_attr.source\n",
    "    # Get attribution for the completion token(s)\n",
    "    scores = seq_attr.source_attributions[-1]  # For last generated token\n",
    "    \n",
    "    return tokens, scores\n",
    "\n",
    "# Compare methods on our pun\n",
    "methods = [\"integrated_gradients\", \"input_x_gradient\", \"attention\"]\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"Running {method}...\")\n",
    "    tokens, scores = get_attributions(pun_prompt, pun_completion, method)\n",
    "    results[method] = (tokens, scores)\n",
    "    print(f\"  Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(14, 3*len(methods)))\n",
    "\n",
    "for ax, method in zip(axes, methods):\n",
    "    tokens, scores = results[method]\n",
    "    scores_np = np.array([s.item() if hasattr(s, 'item') else float(s) for s in scores])\n",
    "    \n",
    "    colors = ['red' if s < 0 else 'blue' for s in scores_np]\n",
    "    ax.bar(range(len(tokens)), np.abs(scores_np), color=colors, alpha=0.7)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('|Attribution|')\n",
    "    ax.set_title(f'{method}')\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle(f'Attribution Comparison: \"{pun_prompt}{pun_completion}\"', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Which Words Make Puns Punny?\n",
    "\n",
    "Let's analyze multiple puns to find patterns in what words drive pun recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up with his girlfriend? He lost\", \" interest\"),\n",
    "    (\"Why do cows wear bells? Because their horns don't\", \" work\"),\n",
    "    (\"What do you call a fish without eyes? A\", \" f\"),\n",
    "    (\"I used to hate facial hair, but then it grew on\", \" me\"),\n",
    "]\n",
    "\n",
    "# Load model once\n",
    "ig_model = inseq.load_model(\"gpt2\", \"integrated_gradients\")\n",
    "\n",
    "def analyze_pun(model, prompt, completion):\n",
    "    \"\"\"Analyze which tokens contribute most to the pun completion.\"\"\"\n",
    "    out = model.attribute(\n",
    "        input_texts=prompt,\n",
    "        generated_texts=prompt + completion,\n",
    "        n_steps=30,\n",
    "        internal_batch_size=10\n",
    "    )\n",
    "    \n",
    "    seq_attr = out.sequence_attributions[0]\n",
    "    tokens = seq_attr.source\n",
    "    scores = seq_attr.source_attributions[-1]\n",
    "    scores_np = np.array([s.item() if hasattr(s, 'item') else float(s) for s in scores])\n",
    "    \n",
    "    # Find top contributing tokens\n",
    "    top_indices = np.argsort(np.abs(scores_np))[-5:][::-1]\n",
    "    \n",
    "    return tokens, scores_np, top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top contributing tokens for each pun:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt, completion in puns:\n",
    "    tokens, scores, top_idx = analyze_pun(ig_model, prompt, completion)\n",
    "    \n",
    "    print(f\"\\nPun: '{prompt}{completion}'\")\n",
    "    print(f\"Punchline: '{completion}'\")\n",
    "    print(\"Top contributing tokens:\")\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        print(f\"  {i+1}. {repr(tokens[idx]):15} (score: {scores[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Pun vs Literal Context Attribution\n",
    "\n",
    "How does attribution differ when the same word appears in a pun vs literal context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same completion word, different contexts\n",
    "pun_context = \"Why do electricians make good swimmers? Because they know the\"\n",
    "literal_context = \"The electrician measured the electrical\"\n",
    "target_word = \" current\"\n",
    "\n",
    "# Get attributions for both\n",
    "pun_tokens, pun_scores, _ = analyze_pun(ig_model, pun_context, target_word)\n",
    "lit_tokens, lit_scores, _ = analyze_pun(ig_model, literal_context, target_word)\n",
    "\n",
    "# Visualize side by side\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Pun context\n",
    "colors = ['red' if s < 0 else 'blue' for s in pun_scores]\n",
    "axes[0].bar(range(len(pun_tokens)), np.abs(pun_scores), color=colors, alpha=0.7)\n",
    "axes[0].set_xticks(range(len(pun_tokens)))\n",
    "axes[0].set_xticklabels(pun_tokens, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_ylabel('|Attribution|')\n",
    "axes[0].set_title(f'PUN: \"{pun_context}{target_word}\"')\n",
    "\n",
    "# Literal context\n",
    "colors = ['red' if s < 0 else 'blue' for s in lit_scores]\n",
    "axes[1].bar(range(len(lit_tokens)), np.abs(lit_scores), color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(lit_tokens)))\n",
    "axes[1].set_xticklabels(lit_tokens, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_ylabel('|Attribution|')\n",
    "axes[1].set_title(f'LITERAL: \"{literal_context}{target_word}\"')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the difference\n",
    "print(\"Comparison of top contributors:\")\n",
    "print(\"\\nPUN context - Top 5 tokens:\")\n",
    "top_pun = np.argsort(np.abs(pun_scores))[-5:][::-1]\n",
    "for idx in top_pun:\n",
    "    print(f\"  {repr(pun_tokens[idx]):15} {pun_scores[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nLITERAL context - Top 5 tokens:\")\n",
    "top_lit = np.argsort(np.abs(lit_scores))[-5:][::-1]\n",
    "for idx in top_lit:\n",
    "    print(f\"  {repr(lit_tokens[idx]):15} {lit_scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Validating Attribution with Ablation\n",
    "\n",
    "Attribution scores claim certain tokens are important. Let's validate by removing high-attribution tokens and measuring the change in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load raw model for ablation experiments\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "raw_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "raw_model.eval()\n",
    "\n",
    "def get_token_probability(model, tokenizer, prompt, target_token):\n",
    "    \"\"\"Get the probability of target_token given prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    target_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1]  # Last position\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return probs[target_ids[0]].item()\n",
    "\n",
    "# Original probability\n",
    "orig_prob = get_token_probability(raw_model, tokenizer, pun_prompt, pun_completion)\n",
    "print(f\"Original P('{pun_completion}'): {orig_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_token(prompt, token_idx, replacement=\"\"):\n",
    "    \"\"\"Replace a token at the given position.\"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    # Decode each token, replace the one at token_idx\n",
    "    token_strs = [tokenizer.decode([t]) for t in tokens]\n",
    "    token_strs[token_idx] = replacement\n",
    "    return \"\".join(token_strs)\n",
    "\n",
    "# Get attribution scores\n",
    "tokens, scores, top_indices = analyze_pun(ig_model, pun_prompt, pun_completion)\n",
    "\n",
    "print(\"Ablation study: removing high-attribution tokens\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original prompt: '{pun_prompt}'\")\n",
    "print(f\"Original P('{pun_completion}'): {orig_prob:.4f}\")\n",
    "print()\n",
    "\n",
    "# Ablate top contributing tokens\n",
    "for idx in top_indices[:5]:\n",
    "    token = tokens[idx]\n",
    "    ablated_prompt = ablate_token(pun_prompt, idx, \"[MASK]\")\n",
    "    \n",
    "    # We can't easily get probability with masked token in GPT-2\n",
    "    # Instead, let's just remove the token\n",
    "    tokens_list = tokenizer.encode(pun_prompt)\n",
    "    tokens_removed = tokens_list[:idx] + tokens_list[idx+1:]\n",
    "    reduced_prompt = tokenizer.decode(tokens_removed)\n",
    "    \n",
    "    new_prob = get_token_probability(raw_model, tokenizer, reduced_prompt, pun_completion)\n",
    "    delta = new_prob - orig_prob\n",
    "    \n",
    "    print(f\"Remove {repr(token):12} (attr: {scores[idx]:+.4f}): P = {new_prob:.4f} (delta: {delta:+.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: ablate LOW attribution tokens\n",
    "print(\"\\nAblating LOW-attribution tokens (should have less effect):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "low_indices = np.argsort(np.abs(scores))[:5]  # Lowest attribution\n",
    "\n",
    "for idx in low_indices:\n",
    "    token = tokens[idx]\n",
    "    tokens_list = tokenizer.encode(pun_prompt)\n",
    "    tokens_removed = tokens_list[:idx] + tokens_list[idx+1:]\n",
    "    reduced_prompt = tokenizer.decode(tokens_removed)\n",
    "    \n",
    "    new_prob = get_token_probability(raw_model, tokenizer, reduced_prompt, pun_completion)\n",
    "    delta = new_prob - orig_prob\n",
    "    \n",
    "    print(f\"Remove {repr(token):12} (attr: {scores[idx]:+.4f}): P = {new_prob:.4f} (delta: {delta:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Attribution Across Layers\n",
    "\n",
    "Inseq can compute layer-wise attributions. Do different layers attribute importance to different tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use inseq's layer attribution capabilities to:\n",
    "# 1. Get attribution scores at each layer\n",
    "# 2. Create a heatmap showing attribution by layer and token\n",
    "# 3. Identify which layers focus on semantically relevant tokens\n",
    "\n",
    "# Hint: Check inseq.list_aggregators() for layer aggregation options\n",
    "print(\"Available aggregators:\")\n",
    "print(inseq.list_aggregators())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Attention Pattern Analysis\n",
    "\n",
    "Compare attention-based attribution with gradient-based methods. Are they correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. Get attention-based attribution scores\n",
    "# 2. Get integrated gradients scores\n",
    "# 3. Compute correlation between them\n",
    "# 4. Discuss: When do they agree? When do they disagree?\n",
    "\n",
    "# Load attention model\n",
    "attention_model = inseq.load_model(\"gpt2\", \"attention\")\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Contrastive Attribution\n",
    "\n",
    "What makes \"current\" the right answer vs a wrong answer? Use contrastive attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare attribution for correct vs incorrect completions\n",
    "# \n",
    "# For the prompt \"Why do electricians make good swimmers? Because they know the\"\n",
    "# Compare:\n",
    "# - Correct: \" current\" (pun answer)\n",
    "# - Wrong: \" water\" (literal but wrong)\n",
    "# - Wrong: \" answer\" (generic)\n",
    "#\n",
    "# Which tokens distinguish the correct pun answer from incorrect ones?\n",
    "\n",
    "completions_to_compare = [\" current\", \" water\", \" answer\", \" secret\"]\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Critical Evaluation of Attribution\n",
    "\n",
    "Attribution methods have limitations. Design experiments to test their reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test attribution reliability\n",
    "#\n",
    "# 1. Sanity check: Do random baselines give different results?\n",
    "# 2. Consistency: Run attribution multiple times - are results stable?\n",
    "# 3. Sensitivity: Small prompt changes shouldn't drastically change attribution\n",
    "#\n",
    "# Refer to Adebayo et al., \"Sanity Checks for Saliency Maps\" for inspiration\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Attribution methods** assign importance scores to input tokens, explaining model predictions\n",
    "\n",
    "2. **Integrated Gradients** provides theoretically grounded attribution that satisfies key axioms\n",
    "\n",
    "3. **Different methods can disagree** - attention, gradient, and path-based methods may highlight different tokens\n",
    "\n",
    "4. **Validation is crucial** - ablation studies help verify that high-attribution tokens actually affect predictions\n",
    "\n",
    "5. **For puns**, attribution reveals which setup words prime the model for the punchline\n",
    "\n",
    "### Key Questions\n",
    "\n",
    "- Do attribution methods capture *why* the model predicts a pun, or just *what* it attends to?\n",
    "- How reliable are these explanations? (See Jain & Wallace, 2019)\n",
    "- Can we use attribution to find where pun understanding happens in the model?\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- Attribution shows correlation, not causation\n",
    "- Results depend on implementation details (baseline, integration steps)\n",
    "- May not capture distributed representations well\n",
    "- Should be validated with causal methods (activation patching, ablation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
