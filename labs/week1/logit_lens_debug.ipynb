{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens Debug Version (Local GPT-2)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week1/logit_lens_debug.ipynb)\n",
    "\n",
    "**This is a debug version using GPT-2 locally.** Use this to test nnsight patterns before running on large models via NDIF.\n",
    "\n",
    "**Colab Setup:** Go to Runtime > Change runtime type > Select **T4 GPU**\n",
    "\n",
    "This notebook demonstrates the **logit lens** technique using [nnsight](https://nnsight.net/). The logit lens lets us peek inside a transformer to see what the model is \"thinking\" at each layer.\n",
    "\n",
    "**Key Idea:** At each layer, we project the hidden states into vocabulary space using the model's unembedding matrix. This reveals how the model's predictions evolve as information flows through the network.\n",
    "\n",
    "## References\n",
    "- [nostalgebraist's Logit Lens post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n",
    "- [nnsight Logit Lens tutorial](https://nnsight.net/notebooks/tutorials/probing/logit_lens/)\n",
    "- [nnsight documentation](https://nnsight.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install nnsight and check GPU availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Local execution - no NDIF needed\n",
    "REMOTE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2\n",
    "\n",
    "GPT-2 is small enough to run locally on Colab. This lets us debug nnsight patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 locally\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prompt\n",
    "\n",
    "Let's start with a simple prompt to test the logit lens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic logit lens example\n",
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Basic Logit Lens: Watching Predictions Develop\n\nLet's see how the model's predictions evolve across layers. This uses the same pattern as the NDIF notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_value(saved):\n    \"\"\"Helper to get value from saved tensor (handles local vs remote).\"\"\"\n    try:\n        return saved.value\n    except AttributeError:\n        return saved\n\ndef get_top_predictions(probs, indices, tokenizer):\n    \"\"\"Convert saved top-k probs and indices to token strings.\"\"\"\n    return [(tokenizer.decode([idx]), prob.item()) for idx, prob in zip(indices, probs)]\n\ndef logit_lens_layers(prompt, model, layers_to_check=None, remote=False, top_k=10):\n    \"\"\"\n    Run logit lens on specified layers.\n    Returns top-k predictions per layer (much less data than full vocab).\n    GPT-2 version (uses transformer.h, transformer.ln_f).\n    \"\"\"\n    n_layers = model.config.n_layer\n    if layers_to_check is None:\n        # Check every 2nd layer plus first and last for GPT-2 (12 layers)\n        layers_to_check = [0, 2, 4, 6, 8, 10, n_layers-1]\n        layers_to_check = [l for l in layers_to_check if l < n_layers]\n    \n    layer_results = {}\n    \n    # Single trace call - compute top-k on server to reduce data transfer\n    with model.trace(prompt, remote=remote):\n        for layer_idx in layers_to_check:\n            hidden = model.transformer.h[layer_idx].output[0]\n            logits = model.lm_head(model.transformer.ln_f(hidden))\n            # Compute softmax and top-k, only save small tensors\n            probs = torch.softmax(logits[0, -1], dim=-1)\n            top_probs, top_indices = probs.topk(top_k)\n            layer_results[layer_idx] = (top_probs.save(), top_indices.save())\n    \n    # Extract values after trace completes\n    return {k: (get_value(v[0]), get_value(v[1])) for k, v in layer_results.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run logit lens on the classic example\nlayer_results = logit_lens_layers(prompt, model, remote=REMOTE)\n\nprint(f\"Prompt: '{prompt}'\\n\")\nprint(\"Layer-by-layer predictions:\")\nprint(\"=\" * 60)\n\nfor layer_idx, (probs, indices) in sorted(layer_results.items()):\n    preds = get_top_predictions(probs, indices, model.tokenizer)\n    print(f\"\\nLayer {layer_idx:2d}:\")\n    for token, prob in preds[:5]:  # Show top 5\n        print(f\"  {repr(token):15} {prob:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Specific Token Probability\n",
    "\n",
    "Now let's track a specific target token across layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def track_token_probability(prompt, target_token, model, remote=False):\n    \"\"\"\n    Track the probability of a specific token across all layers.\n    Only saves single probability value per layer (minimal data transfer).\n    GPT-2 version (uses transformer.h, transformer.ln_f).\n    \"\"\"\n    n_layers = model.config.n_layer\n    \n    # Get target token ID\n    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n    if len(target_ids) != 1:\n        print(f\"Warning: '{target_token}' tokenizes to {len(target_ids)} tokens\")\n    target_id = target_ids[0]\n    \n    layer_probs = []\n    \n    # Single trace call - only save the single probability we need\n    with model.trace(prompt, remote=remote):\n        for layer_idx in range(n_layers):\n            hidden = model.transformer.h[layer_idx].output[0]\n            logits = model.lm_head(model.transformer.ln_f(hidden))\n            probs = torch.softmax(logits[0, -1], dim=-1)\n            # Only save the single probability for target token\n            target_prob = probs[target_id].save()\n            layer_probs.append(target_prob)\n    \n    # Extract probabilities after trace completes\n    return [get_value(p).item() for p in layer_probs]\n\n# Track \" Paris\" probability\ntarget = \" Paris\"\nprobs = track_token_probability(prompt, target, model, remote=REMOTE)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(probs)), probs, 'b-o', markersize=5)\nplt.xlabel('Layer')\nplt.ylabel(f'P(\"{target}\")')\nplt.title(f'Logit Lens: Tracking \"{target}\" probability\\nPrompt: \"{prompt}\"')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Pun (GPT-2 may not get it)\n",
    "\n",
    "GPT-2 is smaller so it might not understand puns as well, but we can still test the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\ntarget = \" current\"\n\nprobs = track_token_probability(pun_prompt, target, model, remote=REMOTE)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(probs)), probs, 'b-o', markersize=5)\nplt.xlabel('Layer')\nplt.ylabel(f'P(\"{target}\")')\nplt.title(f'Logit Lens: Tracking \"{target}\" probability\\nPrompt: \"{pun_prompt}\"')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal layer probability: {probs[-1]:.4f}\")\nprint(\"(Note: GPT-2 may not understand puns as well as larger models)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens with Top-K (Optimized Pattern)\n",
    "\n",
    "This is the pattern we want to use for NDIF - compute top-k on server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with all layers\nresults = logit_lens_layers(prompt, model, layers_to_check=list(range(model.config.n_layer)), remote=REMOTE)\n\nprint(f\"Prompt: '{prompt}'\\n\")\nprint(\"Top-5 predictions per layer:\")\nprint(\"=\" * 50)\n\nfor layer_idx, (probs, indices) in sorted(results.items()):\n    preds = get_top_predictions(probs, indices, model.tokenizer)\n    print(f\"\\nLayer {layer_idx:2d}:\")\n    for token, prob in preds[:5]:\n        print(f\"  {repr(token):15} {prob:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Target Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def logit_lens_heatmap(prompt, target_token, model, remote=False):\n    \"\"\"\n    Create a logit lens heatmap showing the probability of a target token\n    at each layer and position.\n    Only saves target token probabilities (minimal data transfer).\n    GPT-2 version (uses transformer.h, transformer.ln_f).\n    \"\"\"\n    n_layers = model.config.n_layer\n    \n    # Get target token ID\n    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n    target_id = target_ids[0]\n    \n    # Get token strings\n    tokens = model.tokenizer.encode(prompt)\n    token_strs = [model.tokenizer.decode([t]) for t in tokens]\n    \n    layer_probs = []\n    \n    # Single trace call - only save target token probability at each position\n    with model.trace(prompt, remote=remote):\n        for layer_idx in range(n_layers):\n            hidden = model.transformer.h[layer_idx].output[0]\n            logits = model.lm_head(model.transformer.ln_f(hidden))\n            probs = torch.softmax(logits[0], dim=-1)\n            # Only save probabilities for target token at all positions\n            target_probs = probs[:, target_id].save()\n            layer_probs.append(target_probs)\n    \n    # Stack after trace completes\n    all_probs = torch.stack([get_value(p) for p in layer_probs])\n    target_probs = all_probs.cpu().numpy()\n    \n    return target_probs, token_strs\n\n# Visualize\nprobs, tokens = logit_lens_heatmap(prompt, \" Paris\", model, remote=REMOTE)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(probs, aspect='auto', cmap='Blues', vmin=0)\nplt.colorbar(label='P(\" Paris\")')\nplt.xlabel('Token Position')\nplt.ylabel('Layer')\nplt.title(f'Logit Lens Heatmap\\nPrompt: \"{prompt}\"')\nplt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis debug notebook uses the **same nnsight pattern** as the NDIF notebook:\n\n1. **Single trace call**: `with model.trace(prompt, remote=remote):`\n2. **Loop over layers inside trace**: `for layer_idx in range(n_layers):`\n3. **Access layer output**: `model.transformer.h[layer_idx].output[0]`\n4. **Apply logit lens**: `model.lm_head(model.transformer.ln_f(hidden))`\n5. **Compute top-k on \"server\"**: `probs.topk(top_k)` - reduces data transfer\n6. **Save results**: `.save()` on tensors you want to keep\n7. **Access after trace**: Use `get_value()` helper (handles local vs remote)\n\n### Differences for Llama on NDIF:\n- Use `model.model.layers[layer_idx]` instead of `model.transformer.h[layer_idx]`\n- Use `model.model.norm` instead of `model.transformer.ln_f`\n- Add `remote=True` to `model.trace()`\n- Use `model.config.num_hidden_layers` instead of `model.config.n_layer`"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}