{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens Debug Version (Local GPT-2)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week1/logit_lens_debug.ipynb)\n",
    "\n",
    "**This is a debug version using GPT-2 locally.** Use this to test nnsight patterns before running on large models via NDIF.\n",
    "\n",
    "**Colab Setup:** Go to Runtime > Change runtime type > Select **T4 GPU**\n",
    "\n",
    "This notebook demonstrates the **logit lens** technique using [nnsight](https://nnsight.net/). The logit lens lets us peek inside a transformer to see what the model is \"thinking\" at each layer.\n",
    "\n",
    "**Key Idea:** At each layer, we project the hidden states into vocabulary space using the model's unembedding matrix. This reveals how the model's predictions evolve as information flows through the network.\n",
    "\n",
    "## References\n",
    "- [nostalgebraist's Logit Lens post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n",
    "- [nnsight Logit Lens tutorial](https://nnsight.net/notebooks/tutorials/probing/logit_lens/)\n",
    "- [nnsight documentation](https://nnsight.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install nnsight and check GPU availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Local execution - no NDIF needed\n",
    "REMOTE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2\n",
    "\n",
    "GPT-2 is small enough to run locally on Colab. This lets us debug nnsight patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 locally\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.n_layer}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prompt\n",
    "\n",
    "Let's start with a simple prompt to test the logit lens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic logit lens example\n",
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "print(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logit Lens (Following nnsight Tutorial)\n",
    "\n",
    "This follows the exact pattern from the [nnsight logit lens tutorial](https://nnsight.net/notebooks/tutorials/probing/logit_lens/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all layers (GPT-2 uses transformer.h)\n",
    "layers = model.transformer.h\n",
    "probs_layers = []\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        input_tokens = invoker.inputs.save()\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            # GPT-2: apply ln_f (final layer norm) then lm_head\n",
    "            layer_output = model.lm_head(model.transformer.ln_f(layer.output[0]))\n",
    "            probs = torch.nn.functional.softmax(layer_output, dim=-1).save()\n",
    "            probs_layers.append(probs)\n",
    "\n",
    "print(f\"Collected probabilities from {len(probs_layers)} layers\")\n",
    "print(f\"Input tokens shape: {input_tokens[1]['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack probabilities and get top predictions\n",
    "probs = torch.cat([p.value for p in probs_layers])\n",
    "max_probs, tokens = probs.max(dim=-1)\n",
    "\n",
    "# Decode tokens\n",
    "words = [[model.tokenizer.decode(t.cpu()) for t in layer_tokens] for layer_tokens in tokens]\n",
    "input_words = [model.tokenizer.decode(t) for t in input_tokens[1]['input_ids'][0]]\n",
    "\n",
    "print(f\"Input words: {input_words}\")\n",
    "print(f\"\\nTop prediction at each layer for last token:\")\n",
    "for i, w in enumerate(words):\n",
    "    print(f\"  Layer {i:2d}: {repr(w[-1]):15} (prob: {max_probs[i, -1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with Plotly (Like nnsight Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set renderer for Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    pio.renderers.default = \"colab\"\n",
    "except ImportError:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "fig = px.imshow(\n",
    "    max_probs.detach().cpu().numpy(),\n",
    "    x=input_words,\n",
    "    y=list(range(len(words))),\n",
    "    color_continuous_scale=px.colors.diverging.RdYlBu_r,\n",
    "    color_continuous_midpoint=0.50,\n",
    "    text_auto=True,\n",
    "    labels=dict(x=\"Input Tokens\", y=\"Layers\", color=\"Probability\")\n",
    ")\n",
    "\n",
    "fig.update_layout(title='Logit Lens: Top Token Probability by Layer', xaxis_tickangle=0)\n",
    "fig.update_traces(text=words, texttemplate=\"%{text}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Specific Token Probability\n",
    "\n",
    "Now let's track a specific target token across layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_token_probability_gpt2(prompt, target_token, model):\n",
    "    \"\"\"\n",
    "    Track the probability of a specific token across all layers.\n",
    "    GPT-2 version (uses transformer.h, transformer.ln_f).\n",
    "    \"\"\"\n",
    "    n_layers = model.config.n_layer\n",
    "    \n",
    "    # Get target token ID\n",
    "    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    if len(target_ids) != 1:\n",
    "        print(f\"Warning: '{target_token}' tokenizes to {len(target_ids)} tokens\")\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    layers = model.transformer.h\n",
    "    layer_probs = []\n",
    "    \n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt):\n",
    "            for layer in layers:\n",
    "                logits = model.lm_head(model.transformer.ln_f(layer.output[0]))\n",
    "                probs = torch.softmax(logits[0, -1], dim=-1)\n",
    "                target_prob = probs[target_id].save()\n",
    "                layer_probs.append(target_prob)\n",
    "    \n",
    "    return [p.value.item() for p in layer_probs]\n",
    "\n",
    "# Test with \"Paris\"\n",
    "target = \" Paris\"\n",
    "probs = track_token_probability_gpt2(prompt, target, model)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(probs)), probs, 'b-o', markersize=5)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f'P(\"{target}\")')\n",
    "plt.title(f'Logit Lens: Tracking \"{target}\" probability\\nPrompt: \"{prompt}\"')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Pun (GPT-2 may not get it)\n",
    "\n",
    "GPT-2 is smaller so it might not understand puns as well, but we can still test the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "target = \" current\"\n",
    "\n",
    "probs = track_token_probability_gpt2(pun_prompt, target, model)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(probs)), probs, 'b-o', markersize=5)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f'P(\"{target}\")')\n",
    "plt.title(f'Logit Lens: Tracking \"{target}\" probability\\nPrompt: \"{pun_prompt}\"')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal layer probability: {probs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens with Top-K (Optimized Pattern)\n",
    "\n",
    "This is the pattern we want to use for NDIF - compute top-k on server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens_topk_gpt2(prompt, model, layers_to_check=None, top_k=10):\n",
    "    \"\"\"\n",
    "    Run logit lens returning top-k predictions per layer.\n",
    "    GPT-2 version.\n",
    "    \"\"\"\n",
    "    n_layers = model.config.n_layer\n",
    "    if layers_to_check is None:\n",
    "        layers_to_check = list(range(n_layers))\n",
    "    \n",
    "    all_layers = model.transformer.h\n",
    "    layer_results = {}\n",
    "    \n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt):\n",
    "            for layer_idx in layers_to_check:\n",
    "                layer = all_layers[layer_idx]\n",
    "                logits = model.lm_head(model.transformer.ln_f(layer.output[0]))\n",
    "                probs = torch.softmax(logits[0, -1], dim=-1)\n",
    "                top_probs, top_indices = probs.topk(top_k)\n",
    "                layer_results[layer_idx] = (top_probs.save(), top_indices.save())\n",
    "    \n",
    "    return {k: (v[0].value, v[1].value) for k, v in layer_results.items()}\n",
    "\n",
    "# Test\n",
    "results = logit_lens_topk_gpt2(prompt, model)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Top-5 predictions per layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for layer_idx, (probs, indices) in sorted(results.items()):\n",
    "    print(f\"\\nLayer {layer_idx:2d}:\")\n",
    "    for p, idx in zip(probs[:5], indices[:5]):\n",
    "        token = model.tokenizer.decode([idx])\n",
    "        print(f\"  {repr(token):15} {p.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Target Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens_heatmap_gpt2(prompt, target_token, model):\n",
    "    \"\"\"\n",
    "    Create heatmap of target token probability at each layer and position.\n",
    "    GPT-2 version.\n",
    "    \"\"\"\n",
    "    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    tokens = model.tokenizer.encode(prompt)\n",
    "    token_strs = [model.tokenizer.decode([t]) for t in tokens]\n",
    "    \n",
    "    layers = model.transformer.h\n",
    "    layer_probs = []\n",
    "    \n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt):\n",
    "            for layer in layers:\n",
    "                logits = model.lm_head(model.transformer.ln_f(layer.output[0]))\n",
    "                probs = torch.softmax(logits[0], dim=-1)\n",
    "                target_probs = probs[:, target_id].save()\n",
    "                layer_probs.append(target_probs)\n",
    "    \n",
    "    all_probs = torch.stack([p.value for p in layer_probs])\n",
    "    return all_probs.cpu().numpy(), token_strs\n",
    "\n",
    "# Visualize\n",
    "probs, tokens = logit_lens_heatmap_gpt2(prompt, \" Paris\", model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(probs, aspect='auto', cmap='Blues', vmin=0)\n",
    "plt.colorbar(label='P(\" Paris\")')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Layer')\n",
    "plt.title(f'Logit Lens Heatmap\\nPrompt: \"{prompt}\"')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This debug notebook demonstrates the nnsight patterns that work locally:\n",
    "\n",
    "1. **Basic pattern**: `model.trace()` + `tracer.invoke(prompt)`\n",
    "2. **Iterate over layers**: `for layer in model.transformer.h`\n",
    "3. **GPT-2 logit lens**: `model.lm_head(model.transformer.ln_f(layer.output[0]))`\n",
    "4. **Save results**: `.save()` on tensors you want to keep\n",
    "5. **Access after trace**: `saved_tensor.value`\n",
    "\n",
    "### Differences for Llama on NDIF:\n",
    "- Use `model.model.layers` instead of `model.transformer.h`\n",
    "- Use `model.model.norm` instead of `model.transformer.ln_f`\n",
    "- Add `remote=True` to `model.trace()`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
