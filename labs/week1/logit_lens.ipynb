{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Logit Lens with nnsight and NDIF\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week1/logit_lens.ipynb)\n\nThis notebook demonstrates the **logit lens** technique using [nnsight](https://nnsight.net/) and the [NDIF](https://ndif.us/) remote inference API. The logit lens lets us peek inside a transformer to see what the model is \"thinking\" at each layer.\n\n**Key Idea:** At each layer, we project the hidden states into vocabulary space using the model's unembedding matrix. This reveals how the model's predictions evolve as information flows through the network.\n\nWe'll use **Llama 3.1 70B** via NDIF to explore how large language models process **puns**—a fascinating case where the model must hold multiple meanings in mind simultaneously.\n\n## References\n- [nostalgebraist's Logit Lens post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n- [nnsight documentation](https://nnsight.net/)\n- [NDIF - National Deep Inference Fabric](https://ndif.us/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install nnsight if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nnsight import LanguageModel, CONFIG\n\n# Configure NDIF API key from Colab secrets\ntry:\n    from google.colab import userdata\n    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\nexcept:\n    pass  # Not in Colab or secret not set\n\n# We use remote=True to run on NDIF's shared GPU resources\n# This lets us use Llama 3 70B without needing massive local compute!\nREMOTE = True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Llama 3.1 70B\n\nThanks to NDIF, we can run a 70 billion parameter model from a Colab notebook!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Llama 3.1 70B via NDIF\nmodel = LanguageModel(\"meta-llama/Llama-3.1-70B\", device_map=\"auto\")\n\nprint(f\"Model: {model.config._name_or_path}\")\nprint(f\"Layers: {model.config.num_hidden_layers}\")\nprint(f\"Hidden size: {model.config.hidden_size}\")\nprint(f\"Vocabulary size: {model.config.vocab_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puns: A Window into Dual Meanings\n",
    "\n",
    "Puns are interesting for interpretability because they require the model to process words with multiple meanings. When does the model \"get\" the joke? At which layer does the pun's alternative meaning emerge?\n",
    "\n",
    "Let's start with a classic pun setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pun that plays on \"current\" (electrical vs water)\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "# Expected punchline involves \"current\"\n",
    "\n",
    "print(f\"Prompt: {pun_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logit Lens: Watching the Pun Develop\n",
    "\n",
    "Let's see how the model's predictions evolve across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_value(saved):\n    \"\"\"Helper to get value from saved tensor (handles local vs remote).\"\"\"\n    try:\n        return saved.value\n    except AttributeError:\n        return saved\n\ndef get_top_predictions(probs, indices, tokenizer):\n    \"\"\"Convert saved top-k probs and indices to token strings.\"\"\"\n    return [(tokenizer.decode([idx]), prob.item()) for idx, prob in zip(indices, probs)]\n\ndef logit_lens_layers(prompt, model, layers_to_check=None, remote=True, top_k=10):\n    \"\"\"\n    Run logit lens on specified layers.\n    Returns top-k predictions per layer (much less data than full vocab).\n    \"\"\"\n    n_layers = model.config.num_hidden_layers\n    if layers_to_check is None:\n        # Check every 10th layer plus first and last\n        layers_to_check = [0, 10, 20, 30, 40, 50, 60, 70, n_layers-1]\n        layers_to_check = [l for l in layers_to_check if l < n_layers]\n    \n    layer_results = {}\n    \n    # Single trace call - compute top-k on server to reduce data transfer\n    with model.trace(prompt, remote=remote):\n        for layer_idx in layers_to_check:\n            hidden = model.model.layers[layer_idx].output[0]\n            logits = model.lm_head(model.model.norm(hidden))\n            # Compute softmax and top-k on server, only save small tensors\n            probs = torch.softmax(logits[0, -1], dim=-1)\n            top_probs, top_indices = probs.topk(top_k)\n            layer_results[layer_idx] = (top_probs.save(), top_indices.save())\n    \n    # Extract values after trace completes\n    return {k: (get_value(v[0]), get_value(v[1])) for k, v in layer_results.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run logit lens on the pun\nlayer_results = logit_lens_layers(pun_prompt, model, remote=REMOTE)\n\nprint(f\"Prompt: '{pun_prompt}'\\n\")\nprint(\"Layer-by-layer predictions:\")\nprint(\"=\" * 60)\n\nfor layer_idx, (probs, indices) in sorted(layer_results.items()):\n    preds = get_top_predictions(probs, indices, model.tokenizer)\n    print(f\"\\nLayer {layer_idx:2d}:\")\n    for token, prob in preds[:5]:  # Show top 5\n        print(f\"  {repr(token):15} {prob:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking the Pun Word\n",
    "\n",
    "Let's specifically track how the probability of \"current\" (the pun word) develops across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def track_token_probability(prompt, target_token, model, remote=True):\n    \"\"\"\n    Track the probability of a specific token across all layers.\n    Only saves single probability value per layer (minimal data transfer).\n    \"\"\"\n    n_layers = model.config.num_hidden_layers\n    \n    # Get target token ID\n    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n    if len(target_ids) != 1:\n        print(f\"Warning: '{target_token}' tokenizes to {len(target_ids)} tokens\")\n    target_id = target_ids[0]\n    \n    layer_probs = []\n    \n    # Single trace call - only save the single probability we need\n    with model.trace(prompt, remote=remote):\n        for layer_idx in range(n_layers):\n            hidden = model.model.layers[layer_idx].output[0]\n            logits = model.lm_head(model.model.norm(hidden))\n            probs = torch.softmax(logits[0, -1], dim=-1)\n            # Only save the single probability for target token\n            target_prob = probs[target_id].save()\n            layer_probs.append(target_prob)\n    \n    # Extract probabilities after trace completes\n    return [get_value(p).item() for p in layer_probs]\n\n# Track \"current\" probability\ncurrent_probs = track_token_probability(pun_prompt, \" current\", model, remote=REMOTE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(current_probs)), current_probs, 'b-o', markersize=3)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('P(\" current\")')\n",
    "plt.title(f'Logit Lens: When does the model \"get\" the pun?\\n\"{pun_prompt}\"')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the layer where probability first exceeds 0.1\n",
    "threshold = 0.1\n",
    "for i, p in enumerate(current_probs):\n",
    "    if p > threshold:\n",
    "        print(f\"'current' first exceeds {threshold} probability at layer {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compare Multiple Puns\n",
    "\n",
    "Do different types of puns show similar patterns? Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up with his girlfriend? He lost\", \" interest\"),\n",
    "    (\"Why do cows wear bells? Because their horns don't\", \" work\"),\n",
    "    (\"What do you call a fish without eyes? A\", \" f\"),  # \"fsh\" pun\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for prompt, target in puns:\n",
    "    probs = track_token_probability(prompt, target, model, remote=REMOTE)\n",
    "    label = f'\"{target.strip()}\" ({prompt[:30]}...)'\n",
    "    plt.plot(range(len(probs)), probs, '-o', markersize=2, label=label)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logit Lens: Comparing Pun Recognition Across Layers')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Pun vs Literal Completion\n",
    "\n",
    "How does the model decide between a pun completion and a literal one? Let's compare prompts that could go either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pun setup vs literal setup\n",
    "pun_setup = \"Why do electricians make good swimmers? Because they know the\"\n",
    "literal_setup = \"Electricians work with wires that carry electrical\"\n",
    "\n",
    "target = \" current\"\n",
    "\n",
    "pun_probs = track_token_probability(pun_setup, target, model, remote=REMOTE)\n",
    "literal_probs = track_token_probability(literal_setup, target, model, remote=REMOTE)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(pun_probs)), pun_probs, 'b-o', markersize=3, label='Pun context')\n",
    "plt.plot(range(len(literal_probs)), literal_probs, 'r-o', markersize=3, label='Literal context')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f'P(\"{target}\")')\n",
    "plt.title('Logit Lens: Same Word, Different Contexts')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Visualize Full Token Heatmap\n",
    "\n",
    "Create a heatmap showing how the pun word's probability develops at each token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def logit_lens_heatmap(prompt, target_token, model, remote=True):\n    \"\"\"\n    Create a logit lens heatmap showing the probability of a target token\n    at each layer and position.\n    Only saves target token probabilities (minimal data transfer).\n    \"\"\"\n    n_layers = model.config.num_hidden_layers\n    \n    # Get target token ID\n    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n    target_id = target_ids[0]\n    \n    # Get token strings\n    tokens = model.tokenizer.encode(prompt)\n    token_strs = [model.tokenizer.decode([t]) for t in tokens]\n    \n    layer_probs = []\n    \n    # Single trace call - only save target token probability at each position\n    with model.trace(prompt, remote=remote):\n        for layer_idx in range(n_layers):\n            hidden = model.model.layers[layer_idx].output[0]\n            logits = model.lm_head(model.model.norm(hidden))\n            probs = torch.softmax(logits[0], dim=-1)\n            # Only save probabilities for target token at all positions\n            target_probs = probs[:, target_id].save()\n            layer_probs.append(target_probs)\n    \n    # Stack after trace completes\n    all_probs = torch.stack([get_value(p) for p in layer_probs])\n    target_probs = all_probs.cpu().numpy()\n    \n    return target_probs, token_strs\n\n# Visualize\nprompt = \"Why do electricians make good swimmers? Because they know the\"\ntarget = \" current\"\n\nprobs, tokens = logit_lens_heatmap(prompt, target, model, remote=REMOTE)\n\nplt.figure(figsize=(16, 10))\nplt.imshow(probs, aspect='auto', cmap='Blues', vmin=0)\nplt.colorbar(label=f'P(\"{target}\")')\nplt.xlabel('Token Position')\nplt.ylabel('Layer')\nplt.title(f'Logit Lens Heatmap: \"{target}\"\\nPrompt: \"{prompt}\"')\nplt.xticks(range(len(tokens)), tokens, rotation=45, ha='right', fontsize=8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Alternative Scaling Methods\n",
    "\n",
    "The standard logit lens uses the final RMSNorm. Try different scaling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and compare:\n",
    "# 1. Standard logit lens (with RMSNorm)\n",
    "# 2. Raw projection (no normalization)\n",
    "# 3. Tuned lens (learned affine transform per layer)\n",
    "#\n",
    "# See: https://arxiv.org/abs/2303.08112 (Tuned Lens paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **The Logit Lens** projects intermediate hidden states to vocabulary space to see what the model \"thinks\" at each layer\n",
    "\n",
    "2. **nnsight + NDIF** lets us run Llama 3 70B from a notebook without local GPU resources\n",
    "\n",
    "3. **Puns are interesting** because they require dual meanings—we can watch when the pun \"clicks\" in the model\n",
    "\n",
    "4. **Layer patterns vary** by context, suggesting different processing for literal vs humorous completions\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "- At which layer does the pun word first become the top prediction?\n",
    "- Do puns \"develop\" differently than literal factual knowledge?\n",
    "- How does the pattern change for puns that require more cultural knowledge?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}