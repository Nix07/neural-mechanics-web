{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Logit Lens with nnsight and NDIF\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davidbau/neural-mechanics-web/blob/main/labs/week1/logit_lens.ipynb)\n",
    "\n",
    "This notebook demonstrates the **logit lens** technique using [nnsight](https://nnsight.net/) and the [NDIF](https://ndif.us/) remote inference API. The logit lens lets us peek inside a transformer to see what the model is \"thinking\" at each layer.\n",
    "\n",
    "**Key Idea:** At each layer, we project the hidden states into vocabulary space using the model's unembedding matrix. This reveals how the model's predictions evolve as information flows through the network.\n",
    "\n",
    "We'll use **Llama 3.1 70B** via NDIF to explore:\n",
    "1. **Puns** - where the model must hold multiple meanings\n",
    "2. **Multilingual concepts** - where we can see English emerge as an internal \"concept language\"\n",
    "\n",
    "## References\n",
    "- [nostalgebraist's Logit Lens post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n",
    "- [Do Llamas Work in English? (Wendler et al., ACL 2024)](https://aclanthology.org/2024.acl-long.820/) - Key paper on multilingual concept representations\n",
    "- [nnsight documentation](https://nnsight.net/)\n",
    "- [NDIF - National Deep Inference Fabric](https://ndif.us/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Install nnsight for model access\n!pip install -q nnsight\n\n# Install nnterp for standardized model access (also needed on NDIF server)\n!pip install -q nnterp\n\n# Install logitlenskit for visualization\n!pip install -q git+https://github.com/davidbau/logitlenskit.git#subdirectory=python"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnsight import LanguageModel, CONFIG\n",
    "\n",
    "# Configure NDIF API key from Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\n",
    "except:\n",
    "    pass  # Not in Colab or secret not set\n",
    "\n",
    "# We use remote=True to run on NDIF's shared GPU resources\n",
    "# This lets us use Llama 3 70B without needing massive local compute!\n",
    "REMOTE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Llama 3.1 70B\n",
    "\n",
    "Thanks to NDIF, we can run a 70 billion parameter model from a Colab notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 3.1 70B via NDIF\n",
    "model = LanguageModel(\"meta-llama/Llama-3.1-70B\", device_map=\"auto\")\n",
    "\n",
    "print(f\"Model: {model.config._name_or_path}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Quick Way - LogitLensKit\n",
    "\n",
    "Before diving into the details, let's see logit lens in action with just **two lines of code**!\n",
    "\n",
    "The `logitlenskit` library provides a high-level API that handles all the complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from logitlenskit import collect_logit_lens, show_logit_lens\n",
    "\n",
    "# Use nnterp's StandardizedTransformer for consistent access across model architectures\n",
    "st_model = StandardizedTransformer(\"meta-llama/Llama-3.1-70B\")\n",
    "\n",
    "# Two lines to visualize logit lens!\n",
    "data = collect_logit_lens(\"The capital of France is\", st_model, remote=REMOTE)\n",
    "show_logit_lens(data, title=\"Logit Lens: Capital of France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Understanding the Widget\n",
    "\n",
    "The interactive widget shows:\n",
    "- **Rows**: Input token positions (top to bottom)\n",
    "- **Columns**: Layers (left to right, from layer 0 to final layer)\n",
    "- **Cell text**: Top-1 predicted next token at that layer\n",
    "- **Cell color**: Probability of the top prediction (darker = higher)\n",
    "\n",
    "**Interactions:**\n",
    "- **Hover** over cells to see the probability trajectory in the chart below\n",
    "- **Click** cells to see top-k predictions with probabilities\n",
    "- **Shift+click** to pin trajectories for comparison\n",
    "- **Drag** the column borders to resize and see more/fewer layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Understanding the Details - Building Logit Lens by Hand\n",
    "\n",
    "Now let's understand what's happening under the hood. The logit lens works by:\n",
    "\n",
    "1. **Intercepting hidden states** at each layer\n",
    "2. **Applying the final layer normalization** (RMSNorm for Llama)\n",
    "3. **Projecting to vocabulary space** using the language model head (unembedding matrix)\n",
    "4. **Converting to probabilities** via softmax\n",
    "\n",
    "Let's implement this step by step using nnsight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(saved):\n",
    "    \"\"\"Helper to get value from saved tensor (handles local vs remote).\"\"\"\n",
    "    try:\n",
    "        return saved.value\n",
    "    except AttributeError:\n",
    "        return saved\n",
    "\n",
    "\n",
    "def logit_lens_manual(prompt, model, layers_to_check=None, remote=True, top_k=10):\n",
    "    \"\"\"\n",
    "    Implement logit lens from scratch using nnsight.\n",
    "    \n",
    "    This shows exactly what happens at each step:\n",
    "    1. Get hidden state from layer output\n",
    "    2. Apply final layer norm (model.model.norm)\n",
    "    3. Project to vocabulary (model.lm_head)\n",
    "    4. Softmax to get probabilities\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    if layers_to_check is None:\n",
    "        # Sample every 10 layers plus first and last\n",
    "        layers_to_check = list(range(0, n_layers, 10)) + [n_layers - 1]\n",
    "        layers_to_check = sorted(set(layers_to_check))\n",
    "    \n",
    "    # Use nnsight's trace context to intercept model internals\n",
    "    saved_logits = None\n",
    "    with model.trace(prompt, remote=remote):\n",
    "        logits_list = []\n",
    "        for layer_idx in layers_to_check:\n",
    "            # Step 1: Get hidden state from this layer's output\n",
    "            # model.model.layers[i].output is a tuple; [0] is the hidden state\n",
    "            hidden = model.model.layers[layer_idx].output[0]\n",
    "            \n",
    "            # Step 2: Apply final layer normalization\n",
    "            # For Llama, this is RMSNorm stored at model.model.norm\n",
    "            normed = model.model.norm(hidden)\n",
    "            \n",
    "            # Step 3: Project to vocabulary space\n",
    "            # The lm_head maps hidden_size -> vocab_size\n",
    "            logits = model.lm_head(normed)\n",
    "            \n",
    "            # Get last position only (the \"next token\" prediction)\n",
    "            last_logits = logits[0, -1] if len(logits.shape) == 3 else logits[-1]\n",
    "            logits_list.append(last_logits)\n",
    "        \n",
    "        # Save all logits to retrieve after trace\n",
    "        saved_logits = logits_list.save()\n",
    "    \n",
    "    # Process results after trace completes\n",
    "    layer_results = {}\n",
    "    for i, layer_idx in enumerate(layers_to_check):\n",
    "        logits = get_value(saved_logits[i])\n",
    "        # Step 4: Convert to probabilities\n",
    "        probs = torch.softmax(logits.float(), dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        layer_results[layer_idx] = (top_probs, top_indices)\n",
    "    \n",
    "    return layer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our manual implementation\n",
    "prompt = \"The capital of France is\"\n",
    "layer_results = logit_lens_manual(prompt, model, remote=REMOTE)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Layer-by-layer predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for layer_idx, (probs, indices) in sorted(layer_results.items()):\n",
    "    top_tokens = [(model.tokenizer.decode([idx]), prob.item()) \n",
    "                  for idx, prob in zip(indices, probs)]\n",
    "    print(f\"\\nLayer {layer_idx:2d}:\")\n",
    "    for token, prob in top_tokens[:5]:  # Show top 5\n",
    "        print(f\"  {repr(token):15} {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "Notice how \" Paris\" emerges as the top prediction around the middle layers and becomes increasingly confident toward the final layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Multilingual Concepts - \"Espanol: amor, Francais: amour\"\n",
    "\n",
    "One of the most fascinating findings about multilingual LLMs comes from [Wendler et al. (2024)](https://aclanthology.org/2024.acl-long.820/): **\"Do Llamas Work in English?\"**\n",
    "\n",
    "Their key insight: When processing non-English text, the model's internal representations pass through three phases:\n",
    "1. **Input space**: Early layers encode the input language\n",
    "2. **Concept space**: Middle layers represent meaning in a language-neutral (but English-biased) space\n",
    "3. **Output space**: Final layers translate back to the target language\n",
    "\n",
    "Let's test this! We'll prompt the model with a French pattern and see if English \"love\" appears in the middle layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual concept prompt\n",
    "multilingual_prompt = \"Espanol: amor, Francais:\"\n",
    "\n",
    "# Track both English \"love\" and French \"amour\" across layers\n",
    "data = collect_logit_lens(multilingual_prompt, st_model, remote=REMOTE)\n",
    "show_logit_lens(data, title='Multilingual Concepts: \"amor\" → \"amour\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Tracking Specific Tokens\n",
    "\n",
    "Let's explicitly track how the probabilities of \" love\" (English), \" amour\" (French), and \" amor\" (Spanish) evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_token_probability(prompt, target_token, model, remote=True):\n",
    "    \"\"\"\n",
    "    Track the probability of a specific token across all layers.\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    \n",
    "    # Get target token ID\n",
    "    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    if len(target_ids) != 1:\n",
    "        print(f\"Warning: '{target_token}' tokenizes to {len(target_ids)} tokens: {target_ids}\")\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    # Collect logits at all layers\n",
    "    saved_logits = None\n",
    "    with model.trace(prompt, remote=remote):\n",
    "        logits_list = []\n",
    "        for layer_idx in range(n_layers):\n",
    "            hidden = model.model.layers[layer_idx].output[0]\n",
    "            logits = model.lm_head(model.model.norm(hidden))\n",
    "            last_logits = logits[0, -1] if len(logits.shape) == 3 else logits[-1]\n",
    "            logits_list.append(last_logits)\n",
    "        saved_logits = logits_list.save()\n",
    "    \n",
    "    # Extract probabilities\n",
    "    layer_probs = []\n",
    "    for i in range(n_layers):\n",
    "        logits = get_value(saved_logits[i])\n",
    "        probs = torch.softmax(logits.float(), dim=-1)\n",
    "        layer_probs.append(probs[target_id].item())\n",
    "    \n",
    "    return layer_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track English, French, and Spanish words for \"love\"\n",
    "multilingual_prompt = \"Espanol: amor, Francais:\"\n",
    "\n",
    "love_en = track_token_probability(multilingual_prompt, \" love\", model, remote=REMOTE)\n",
    "love_fr = track_token_probability(multilingual_prompt, \" amour\", model, remote=REMOTE)  \n",
    "love_es = track_token_probability(multilingual_prompt, \" amor\", model, remote=REMOTE)\n",
    "\n",
    "# Plot the trajectories\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "layers = range(len(love_en))\n",
    "plt.plot(layers, love_en, 'b-o', markersize=3, label='\" love\" (English)', linewidth=2)\n",
    "plt.plot(layers, love_fr, 'r-o', markersize=3, label='\" amour\" (French)', linewidth=2)\n",
    "plt.plot(layers, love_es, 'g-o', markersize=3, label='\" amor\" (Spanish)', linewidth=2)\n",
    "\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title(f'Multilingual Concept Representation\\nPrompt: \"{multilingual_prompt}\"', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for the three phases\n",
    "plt.axvspan(0, 20, alpha=0.1, color='gray', label='Input Space')\n",
    "plt.axvspan(20, 60, alpha=0.1, color='blue', label='Concept Space')\n",
    "plt.axvspan(60, 80, alpha=0.1, color='green', label='Output Space')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find peak probabilities\n",
    "print(f\"\\nPeak probability layers:\")\n",
    "print(f\"  'love' (English): layer {np.argmax(love_en)} ({max(love_en):.3f})\")\n",
    "print(f\"  'amour' (French): layer {np.argmax(love_fr)} ({max(love_fr):.3f})\")\n",
    "print(f\"  'amor' (Spanish): layer {np.argmax(love_es)} ({max(love_es):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "If the Wendler et al. hypothesis is correct, you should see:\n",
    "- **Early layers**: Low probability for all translations\n",
    "- **Middle layers**: English \"love\" peaks higher than French \"amour\" (English as concept space)\n",
    "- **Final layers**: French \"amour\" overtakes as the model prepares the output\n",
    "\n",
    "This suggests the model internally \"thinks\" in English before translating to the output language!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Puns - A Window into Dual Meanings\n",
    "\n",
    "Puns are interesting for interpretability because they require the model to process words with multiple meanings. When does the model \"get\" the joke? At which layer does the pun's alternative meaning emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pun that plays on \"current\" (electrical vs water)\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "\n",
    "data = collect_logit_lens(pun_prompt, st_model, remote=REMOTE)\n",
    "show_logit_lens(data, title=\"Pun: Electricians & Swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track \"current\" probability across layers\n",
    "current_probs = track_token_probability(pun_prompt, \" current\", model, remote=REMOTE)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(current_probs)), current_probs, 'b-o', markersize=3)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('P(\" current\")')\n",
    "plt.title(f'When does the model \"get\" the pun?\\n\"{pun_prompt}\"')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find when probability first exceeds 0.1\n",
    "threshold = 0.1\n",
    "for i, p in enumerate(current_probs):\n",
    "    if p > threshold:\n",
    "        print(f\"'current' first exceeds {threshold} probability at layer {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Exercise: Compare Multiple Puns\n",
    "\n",
    "Do different types of puns show similar patterns? Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "puns = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up with his girlfriend? He lost\", \" interest\"),\n",
    "    (\"Why do cows wear bells? Because their horns don't\", \" work\"),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for prompt, target in puns:\n",
    "    probs = track_token_probability(prompt, target, model, remote=REMOTE)\n",
    "    label = f'\"{target.strip()}\" ({prompt[:25]}...)'\n",
    "    plt.plot(range(len(probs)), probs, '-o', markersize=2, label=label)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logit Lens: Comparing Pun Recognition Across Layers')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Context Changes Interpretation\n",
    "\n",
    "The same sentence can be interpreted literally or as a pun depending on context. Does preceding context prime the model toward pun interpretations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same sentence in different contexts\n",
    "neutral = \"I used to be a banker, but I lost my\"\n",
    "after_pun = \"I used to be a tailor, but the job didn't suit me. I used to be a banker, but I lost my\"\n",
    "\n",
    "# Track both \"job\" (literal) and \"interest\" (pun)\n",
    "targets = [\" job\", \" interest\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, context_name, prompt in zip(axes, [\"Neutral context\", \"After another pun\"], [neutral, after_pun]):\n",
    "    for target in targets:\n",
    "        probs = track_token_probability(prompt, target, model, remote=REMOTE)\n",
    "        ax.plot(range(len(probs)), probs, '-o', markersize=2, label=f'P(\"{target.strip()}\")')\n",
    "    \n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'{context_name}\\n\"...banker, but I lost my ___\"')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Advanced Visualization with LogitLensKit\n",
    "\n",
    "The LogitLensKit widget provides rich interactivity for exploring logit lens data. Here are some advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the multilingual example with the full widget\n",
    "data = collect_logit_lens(\n",
    "    \"German: Liebe, Italian: amore, English:\",\n",
    "    st_model,\n",
    "    remote=REMOTE\n",
    ")\n",
    "show_logit_lens(data, title=\"Multilingual Love Across Languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More examples to explore\n",
    "examples = [\n",
    "    \"The Eiffel Tower is located in\",\n",
    "    \"To be or not to be, that is the\",\n",
    "    \"In 1969, Neil Armstrong became the first person to walk on the\",\n",
    "    \"The quick brown fox jumps over the lazy\",\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    data = collect_logit_lens(ex, st_model, remote=REMOTE)\n",
    "    show_logit_lens(data, title=f'\"{ex[:40]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **The Logit Lens** projects intermediate hidden states to vocabulary space to see what the model \"thinks\" at each layer\n",
    "\n",
    "2. **Two ways to use it:**\n",
    "   - **Quick way**: `collect_logit_lens()` + `show_logit_lens()` from logitlenskit\n",
    "   - **Manual way**: Intercept with nnsight, apply norm and lm_head, softmax\n",
    "\n",
    "3. **Multilingual concepts**: Models may use English as an internal \"concept language\" (Wendler et al., 2024)\n",
    "\n",
    "4. **Puns** are interesting because they require dual meanings—we can watch when the pun \"clicks\"\n",
    "\n",
    "5. **nnsight + NDIF** lets us run Llama 3 70B from a notebook without local GPU resources\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "- At which layer does the correct answer first become the top prediction?\n",
    "- Do factual vs. creative completions show different layer patterns?\n",
    "- How does the pattern change for different languages?\n",
    "- Can you find examples where the middle-layer prediction is \"more correct\" than the final prediction?\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Wendler et al. (2024): Do Llamas Work in English?](https://aclanthology.org/2024.acl-long.820/)\n",
    "- [Tuned Lens (Belrose et al., 2023)](https://arxiv.org/abs/2303.08112)\n",
    "- [LogitLensKit Documentation](https://davidbau.github.io/logitlenskit/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}