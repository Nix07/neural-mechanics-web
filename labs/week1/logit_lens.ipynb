{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Logit Lens with nnsight and NDIF\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nix07/neural-mechanics-web/blob/main/labs/week1/logit_lens.ipynb)\n\nThis notebook demonstrates the **logit lens** technique using [nnsight](https://nnsight.net/) and the [NDIF](https://ndif.us/) remote inference API. The logit lens lets us peek inside a transformer to see what the model is \"thinking\" at each layer.\n\n**Key Idea:** At each layer, we project the hidden states into vocabulary space using the model's unembedding matrix. This reveals how the model's predictions evolve as information flows through the network.\n\nWe'll use **Llama 3.1 70B** via NDIF to explore how large language models process **puns**—a fascinating case where the model must hold multiple meanings in mind simultaneously.\n\n## References\n- [nostalgebraist's Logit Lens post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)\n- [nnsight documentation](https://nnsight.net/)\n- [NDIF - National Deep Inference Fabric](https://ndif.us/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install nnsight if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pin nnsight version for NDIF compatibility\n!pip install -q nnsight==0.5.11\n\n# Download visualization utilities from course repo\n!curl -sO https://raw.githubusercontent.com/Nix07/neural-mechanics-web/main/labs/week1/logit_lens_viz.py\n!curl -sO https://raw.githubusercontent.com/Nix07/neural-mechanics-web/main/labs/week1/logit_lens_data.py"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nnsight import LanguageModel, CONFIG\n\n# Configure NDIF API key from Colab secrets\ntry:\n    from google.colab import userdata\n    CONFIG.set_default_api_key(userdata.get('NDIF_API'))\nexcept:\n    pass  # Not in Colab or secret not set\n\n# We use remote=True to run on NDIF's shared GPU resources\n# This lets us use Llama 3 70B without needing massive local compute!\nREMOTE = True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Llama 3.1 70B\n\nThanks to NDIF, we can run a 70 billion parameter model from a Colab notebook!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Llama 3.1 70B via NDIF\nmodel = LanguageModel(\"meta-llama/Llama-3.1-70B\", device_map=\"auto\")\n\nprint(f\"Model: {model.config._name_or_path}\")\nprint(f\"Layers: {model.config.num_hidden_layers}\")\nprint(f\"Hidden size: {model.config.hidden_size}\")\nprint(f\"Vocabulary size: {model.config.vocab_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puns: A Window into Dual Meanings\n",
    "\n",
    "Puns are interesting for interpretability because they require the model to process words with multiple meanings. When does the model \"get\" the joke? At which layer does the pun's alternative meaning emerge?\n",
    "\n",
    "Let's start with a classic pun setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pun that plays on \"current\" (electrical vs water)\n",
    "pun_prompt = \"Why do electricians make good swimmers? Because they know the\"\n",
    "# Expected punchline involves \"current\"\n",
    "\n",
    "print(f\"Prompt: {pun_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logit Lens: Watching the Pun Develop\n",
    "\n",
    "Let's see how the model's predictions evolve across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_value(saved):\n    \"\"\"Helper to get value from saved tensor (handles local vs remote).\"\"\"\n    try:\n        return saved.value\n    except AttributeError:\n        return saved\n\ndef get_top_predictions(probs, indices, tokenizer):\n    \"\"\"Convert saved top-k probs and indices to token strings.\"\"\"\n    return [(tokenizer.decode([idx]), prob.item()) for idx, prob in zip(indices, probs)]\n\ndef logit_lens_layers(prompt, model, layers_to_check=None, remote=True, top_k=10):\n    \"\"\"\n    Run logit lens on specified layers.\n    Returns top-k predictions per layer.\n    \n    Note: For NDIF remote execution, we save the list of tensors directly\n    using list.save() which returns a list of actual tensors after the trace.\n    \"\"\"\n    n_layers = model.config.num_hidden_layers\n    if layers_to_check is None:\n        # Check every 10th layer plus first and last\n        layers_to_check = [0, 10, 20, 30, 40, 50, 60, 70, n_layers-1]\n        layers_to_check = [l for l in layers_to_check if l < n_layers]\n    \n    # Collect logits for each layer, save the list\n    saved_logits = None\n    with model.trace(prompt, remote=remote):\n        logits_list = []\n        for layer_idx in layers_to_check:\n            hidden = model.model.layers[layer_idx].output[0]\n            logits = model.lm_head(model.model.norm(hidden))\n            # Get last position only to reduce data transfer\n            last_logits = logits[0, -1] if len(logits.shape) == 3 else logits[-1]\n            logits_list.append(last_logits)\n        # Save the list directly - returns list of tensors after trace\n        saved_logits = logits_list.save()\n    \n    # Process results after trace\n    layer_results = {}\n    for i, layer_idx in enumerate(layers_to_check):\n        logits = get_value(saved_logits[i])\n        probs = torch.softmax(logits.float(), dim=-1)\n        top_probs, top_indices = probs.topk(top_k)\n        layer_results[layer_idx] = (top_probs, top_indices)\n    \n    return layer_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run logit lens on the pun\nlayer_results = logit_lens_layers(pun_prompt, model, remote=REMOTE)\n\nprint(f\"Prompt: '{pun_prompt}'\\n\")\nprint(\"Layer-by-layer predictions:\")\nprint(\"=\" * 60)\n\nfor layer_idx, (probs, indices) in sorted(layer_results.items()):\n    preds = get_top_predictions(probs, indices, model.tokenizer)\n    print(f\"\\nLayer {layer_idx:2d}:\")\n    for token, prob in preds[:5]:  # Show top 5\n        print(f\"  {repr(token):15} {prob:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking the Pun Word\n",
    "\n",
    "Let's specifically track how the probability of \"current\" (the pun word) develops across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def track_token_probability(prompt, target_token, model, remote=True):\n    \"\"\"\n    Track the probability of a specific token across all layers.\n    \n    Note: For NDIF remote execution, we save the list of logits directly.\n    \"\"\"\n    n_layers = model.config.num_hidden_layers\n    \n    # Get target token ID\n    target_ids = model.tokenizer.encode(target_token, add_special_tokens=False)\n    if len(target_ids) != 1:\n        print(f\"Warning: '{target_token}' tokenizes to {len(target_ids)} tokens\")\n    target_id = target_ids[0]\n    \n    # Collect logits for each layer, save the list\n    saved_logits = None\n    with model.trace(prompt, remote=remote):\n        logits_list = []\n        for layer_idx in range(n_layers):\n            hidden = model.model.layers[layer_idx].output[0]\n            logits = model.lm_head(model.model.norm(hidden))\n            # Get last position only\n            last_logits = logits[0, -1] if len(logits.shape) == 3 else logits[-1]\n            logits_list.append(last_logits)\n        # Save the list directly\n        saved_logits = logits_list.save()\n    \n    # Process results after trace\n    layer_probs = []\n    for i in range(n_layers):\n        logits = get_value(saved_logits[i])\n        probs = torch.softmax(logits.float(), dim=-1)\n        layer_probs.append(probs[target_id].item())\n    \n    return layer_probs\n\n# Track \"current\" probability\ncurrent_probs = track_token_probability(pun_prompt, \" current\", model, remote=REMOTE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(current_probs)), current_probs, 'b-o', markersize=3)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('P(\" current\")')\n",
    "plt.title(f'Logit Lens: When does the model \"get\" the pun?\\n\"{pun_prompt}\"')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the layer where probability first exceeds 0.1\n",
    "threshold = 0.1\n",
    "for i, p in enumerate(current_probs):\n",
    "    if p > threshold:\n",
    "        print(f\"'current' first exceeds {threshold} probability at layer {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Compare Multiple Puns\n",
    "\n",
    "Do different types of puns show similar patterns? Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns = [\n",
    "    (\"Why do electricians make good swimmers? Because they know the\", \" current\"),\n",
    "    (\"Why did the banker break up with his girlfriend? He lost\", \" interest\"),\n",
    "    (\"Why do cows wear bells? Because their horns don't\", \" work\"),\n",
    "    (\"What do you call a fish without eyes? A\", \" f\"),  # \"fsh\" pun\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for prompt, target in puns:\n",
    "    probs = track_token_probability(prompt, target, model, remote=REMOTE)\n",
    "    label = f'\"{target.strip()}\" ({prompt[:30]}...)'\n",
    "    plt.plot(range(len(probs)), probs, '-o', markersize=2, label=label)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Logit Lens: Comparing Pun Recognition Across Layers')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Context Changes Interpretation\n\nThe same sentence can be interpreted literally or as a pun depending on context. Consider: \"I used to be a banker, but I lost my...\" — this could end with \"job\" (literal) or \"interest\" (pun).\n\nDoes preceding context prime the model toward pun interpretations?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The same sentence in different contexts\nneutral = \"I used to be a banker, but I lost my\"\nafter_pun = \"I used to be a tailor, but the job didn't suit me. I used to be a banker, but I lost my\"\n\n# Track both \"job\" (literal) and \"interest\" (pun)\ntargets = [\" job\", \" interest\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, context_name, prompt in zip(axes, [\"Neutral context\", \"After another pun\"], [neutral, after_pun]):\n    for target in targets:\n        probs = track_token_probability(prompt, target, model, remote=REMOTE)\n        ax.plot(range(len(probs)), probs, '-o', markersize=2, label=f'P(\"{target.strip()}\")')\n    \n    ax.set_xlabel('Layer')\n    ax.set_ylabel('Probability')\n    ax.set_title(f'{context_name}\\n\"...banker, but I lost my ___\"')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Show final predictions for both contexts\nprint(\"Final layer predictions:\")\nfor context_name, prompt in [(\"Neutral\", neutral), (\"After pun\", after_pun)]:\n    results = logit_lens_layers(prompt, model, layers_to_check=[79], remote=REMOTE)\n    preds = get_top_predictions(*results[79], model.tokenizer)\n    print(f\"\\n{context_name}: '{prompt[-40:]}...'\")\n    for token, prob in preds[:5]:\n        print(f\"  {repr(token):15} {prob:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Visualize Full Token Heatmap\n",
    "\n",
    "Create a heatmap showing how the pun word's probability develops at each token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the HTML visualization module\nfrom logit_lens_viz import logit_lens_heatmap as html_heatmap, token_trajectory\n\n# Visualize with interactive HTML heatmap\n# Hover over cells to see top-5 predictions at each layer/position\nprompt = \"Why do electricians make good swimmers? Because they know the\"\ntarget = \" current\"\n\nhtml_heatmap(prompt, model, target_token=target, remote=REMOTE, top_k=5)"
  },
  {
   "cell_type": "code",
   "source": "# New interactive visualization with trajectory tracking\n# This shows ALL positions and layers with hover/click interactions:\n# - Hover: see probability trajectory across layers in chart below\n# - Click: popup with full token and top-k predictions\n# - Shift+click: pin trajectories for comparison\n# - Drag resize handle: adjust column width (auto-strides layers)\n\nfrom logit_lens_data import collect_logit_lens_topk\nfrom logit_lens_viz import render_all_positions_heatmap\n\n# Collect data with track_across_layers=True for trajectory visualization\ndata = collect_logit_lens_topk(prompt, model, top_k=5, track_across_layers=True, remote=REMOTE)\n\n# Render interactive heatmap\nrender_all_positions_heatmap(data, model.tokenizer)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare how different tokens compete across layers\n# This shows the trajectory of multiple candidate completions\ntoken_trajectory(prompt, [\" current\", \" water\", \" rules\", \" best\"], model, remote=REMOTE)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Alternative Scaling Methods\n",
    "\n",
    "The standard logit lens uses the final RMSNorm. Try different scaling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and compare:\n",
    "# 1. Standard logit lens (with RMSNorm)\n",
    "# 2. Raw projection (no normalization)\n",
    "# 3. Tuned lens (learned affine transform per layer)\n",
    "#\n",
    "# See: https://arxiv.org/abs/2303.08112 (Tuned Lens paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **The Logit Lens** projects intermediate hidden states to vocabulary space to see what the model \"thinks\" at each layer\n",
    "\n",
    "2. **nnsight + NDIF** lets us run Llama 3 70B from a notebook without local GPU resources\n",
    "\n",
    "3. **Puns are interesting** because they require dual meanings—we can watch when the pun \"clicks\" in the model\n",
    "\n",
    "4. **Layer patterns vary** by context, suggesting different processing for literal vs humorous completions\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "- At which layer does the pun word first become the top prediction?\n",
    "- Do puns \"develop\" differently than literal factual knowledge?\n",
    "- How does the pattern change for puns that require more cultural knowledge?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}